[34mAuto commit by fitlog[0m
(vocab_size,class_num,seq_len):(27658,2,1070)
No pretrained model with be used.
vocabsize:27658
Using LSTM Model.
LSTMText(
  (embeds): Embedding(
    27658, 128
    (dropout): Dropout(p=0.0)
  )
  (lstm): LSTM(128, 128, num_layers=2, dropout=0.2, bidirectional=True)
  (fc): Linear(in_features=256, out_features=2, bias=True)
  (dropout): Dropout(p=0.2)
)
train_size:285354 ; val_size:128282 ; test_size:43444
Using Adam as optimizer.
input fields after batch(if batch size is 2):
	words: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2, 1070]) 
	seq_len: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2]) 
target fields after batch(if batch size is 2):
	target: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2]) 

training epochs started 2019-06-14-04-01-48
loss:0.45017
loss:0.30900
loss:0.25691
loss:0.29263
loss:0.27732
loss:0.27072
loss:0.27427
loss:0.25040
loss:0.28247
loss:0.27069
loss:0.22683
loss:0.27592
loss:0.29017
loss:0.28370
loss:0.26433
loss:0.28407
loss:0.29606
loss:0.22789
loss:0.26128
loss:0.27516
loss:0.27335
loss:0.26272
loss:0.30647
loss:0.25299
loss:0.27035
loss:0.29453
loss:0.25280
loss:0.27921
loss:0.25689
loss:0.26739
loss:0.27067
loss:0.27165
loss:0.28462
loss:0.27621
loss:0.26069
loss:0.25758
loss:0.26183
loss:0.25526
loss:0.26834
loss:0.23416
loss:0.28381
loss:0.28046
loss:0.26834
loss:0.25662
loss:0.26922
loss:0.27965
loss:0.27376
loss:0.29353
loss:0.30442
loss:0.27179
loss:0.24971
loss:0.26827
loss:0.27250
loss:0.30196
loss:0.30070
loss:0.28175
loss:0.30114
loss:0.25329
loss:0.25832
loss:0.27329
loss:0.25621
loss:0.27301
loss:0.28945
loss:0.28286
loss:0.26281
loss:0.29178
loss:0.26006
loss:0.29074
loss:0.26654
loss:0.28010
loss:0.26402
loss:0.29833
loss:0.29561
loss:0.27888
loss:0.29488
loss:0.28346
loss:0.29282
loss:0.26962
loss:0.26029
loss:0.25839
loss:0.27917
loss:0.24574
loss:0.27596
loss:0.26011
loss:0.28334
loss:0.26265
loss:0.27881
loss:0.26479
loss:0.28387
loss:0.29772
loss:0.28126
loss:0.27607
loss:0.27427
loss:0.28796
loss:0.25656
loss:0.28459
loss:0.26250
loss:0.29758
loss:0.26394
loss:0.29283
loss:0.26673
loss:0.25026
loss:0.28656
loss:0.26954
loss:0.25461
loss:0.25908
loss:0.27533
loss:0.26104
loss:0.25989
loss:0.28833
loss:0.29525
loss:0.28640
loss:0.27111
loss:0.25121
loss:0.29314
loss:0.24974
loss:0.27985
loss:0.30154
loss:0.27530
loss:0.28893
loss:0.26845
loss:0.28951
loss:0.26163
loss:0.22701
loss:0.29474
loss:0.29674
loss:0.29922
loss:0.28731
loss:0.28096
loss:0.28141
loss:0.27473
loss:0.26610
loss:0.30482
loss:0.28823
loss:0.27280
loss:0.28570
loss:0.30476
loss:0.26323
loss:0.23726
loss:0.28284
loss:0.28294
loss:0.28945
loss:0.26487
loss:0.28920
loss:0.27346
loss:0.26653
loss:0.29785
loss:0.27403
loss:0.26208
loss:0.26655
loss:0.26425
loss:0.28579
loss:0.28663
loss:0.28364
loss:0.27022
loss:0.28010
loss:0.28007
loss:0.26308
loss:0.25227
loss:0.26442
loss:0.28276
loss:0.25491
loss:0.29679
loss:0.25643
loss:0.29706
loss:0.26196
loss:0.27928
loss:0.25765
loss:0.25912
loss:0.26956
loss:0.26940
loss:0.24906
loss:0.29905
loss:0.28596
loss:0.24520
loss:0.29063
loss:0.25440
loss:0.24881
Evaluation at Epoch 1/10. Step:892/8920. AccuracyMetric: acc=0.89071

loss:0.29404
loss:0.25341
loss:0.26456
loss:0.27484
loss:0.30346
loss:0.28496
loss:0.26527
loss:0.23560
loss:0.27948
loss:0.26927
loss:0.26694
loss:0.29982
loss:0.26846
loss:0.26003
loss:0.27079
loss:0.27067
loss:0.27212
loss:0.25574
loss:0.25828
loss:0.28832
loss:0.30344
loss:0.26442
loss:0.27624
loss:0.27725
loss:0.26714
loss:0.26798
loss:0.29267
loss:0.29341
loss:0.28962
loss:0.24737
loss:0.25629
loss:0.28529
loss:0.24504
loss:0.26359
loss:0.26513
loss:0.25735
loss:0.29944
loss:0.28888
loss:0.28229
loss:0.25190
loss:0.28311
loss:0.27191
loss:0.29896
loss:0.28885
loss:0.27551
loss:0.26350
loss:0.30404
loss:0.26416
loss:0.31066
loss:0.27370
loss:0.27705
loss:0.27854
loss:0.26984
loss:0.27779
loss:0.27724
loss:0.27860
loss:0.27620
loss:0.28346
loss:0.27617
loss:0.26074
loss:0.25605
loss:0.25493
loss:0.26684
loss:0.27387
loss:0.28136
loss:0.24148
loss:0.27967
loss:0.26395
loss:0.27160
loss:0.27586
loss:0.26533
loss:0.28449
loss:0.27377
loss:0.25323
loss:0.21882
loss:0.24529
loss:0.26022
loss:0.24977
loss:0.27533
loss:0.27121
loss:0.28813
loss:0.25891
loss:0.25218
loss:0.25629
loss:0.27212
loss:0.27261
loss:0.29778
loss:0.26856
loss:0.25934
loss:0.27698
loss:0.25716
loss:0.28319
loss:0.26310
loss:0.27709
loss:0.28038
loss:0.27763
loss:0.28898
loss:0.28622
loss:0.28055
loss:0.27815
loss:0.25798
loss:0.27148
loss:0.25500
loss:0.26305
loss:0.27988
loss:0.27588
loss:0.25972
loss:0.27562
loss:0.30048
loss:0.29010
loss:0.27173
loss:0.25299
loss:0.27043
loss:0.25386
loss:0.29822
loss:0.26315
loss:0.29868
loss:0.27333
loss:0.25852
loss:0.26499
loss:0.24338
loss:0.30050
loss:0.23463
loss:0.26131
loss:0.24771
loss:0.27297
loss:0.28325
loss:0.26472
loss:0.26136
loss:0.26633
loss:0.26452
loss:0.25832
loss:0.24473
loss:0.27895
loss:0.27732
loss:0.28348
loss:0.29259
loss:0.26980
loss:0.25762
loss:0.26452
loss:0.28961
loss:0.26344
loss:0.26888
loss:0.28686
loss:0.27064
loss:0.28924
loss:0.28027
loss:0.30951
loss:0.26400
loss:0.26548
loss:0.29827
loss:0.25610
loss:0.27946
loss:0.28935
loss:0.29222
loss:0.27576
loss:0.27048
loss:0.27788
loss:0.31845
loss:0.28605
loss:0.27272
loss:0.26382
loss:0.27680
loss:0.27622
loss:0.25665
loss:0.27185
loss:0.29087
loss:0.28144
loss:0.25975
loss:0.26543
loss:0.24635
loss:0.27298
loss:0.28090
loss:0.27088
loss:0.28078
loss:0.27194
loss:0.27395
loss:0.30097
Evaluation at Epoch 2/10. Step:1784/8920. AccuracyMetric: acc=0.89071

loss:0.26612
loss:0.28655
loss:0.27739
loss:0.28031
loss:0.24437
loss:0.27754
loss:0.26736
loss:0.26193
loss:0.29184
loss:0.26890
loss:0.31183
loss:0.25217
loss:0.26233
loss:0.27856
loss:0.27506
loss:0.28474
loss:0.27512
loss:0.23705
loss:0.27010
loss:0.24270
loss:0.25072
loss:0.26045
loss:0.29918
loss:0.27652
loss:0.27316
loss:0.28440
loss:0.26406
loss:0.27033
loss:0.26247
loss:0.24156
loss:0.25857
loss:0.27705
loss:0.27844
loss:0.27138
loss:0.27949
loss:0.26131
loss:0.26192
loss:0.26265
loss:0.26491
loss:0.29271
loss:0.28540
loss:0.27151
loss:0.24657
loss:0.25602
loss:0.29271
loss:0.28191
loss:0.28890
loss:0.26548
loss:0.28436
loss:0.27822
loss:0.25172
loss:0.28245
loss:0.25905
loss:0.24154
loss:0.25690
loss:0.28832
loss:0.27427
loss:0.27830
loss:0.28045
loss:0.29313
loss:0.26290
loss:0.25562
loss:0.26362
loss:0.29702
loss:0.25935
loss:0.25630
loss:0.24051
loss:0.28537
loss:0.28910
loss:0.28449
loss:0.28647
loss:0.27853
loss:0.27284
loss:0.25289
loss:0.28492
loss:0.27021
loss:0.24122
loss:0.24192
loss:0.26291
loss:0.26468
loss:0.30211
loss:0.26932
loss:0.28567
loss:0.28192
loss:0.28211
loss:0.28277
loss:0.25813
loss:0.25568
loss:0.26124
loss:0.26547
loss:0.27039
loss:0.31358
loss:0.27425
loss:0.27937
loss:0.25908
loss:0.25870
loss:0.26795
loss:0.26011
loss:0.29509
loss:0.26339
loss:0.25444
loss:0.27297
loss:0.26798
loss:0.28123
loss:0.27797
loss:0.30138
loss:0.30738
loss:0.27921
loss:0.27460
loss:0.26710
loss:0.29234
loss:0.28278
loss:0.23732
loss:0.28648
loss:0.25059
loss:0.24958
loss:0.25838
loss:0.29075
loss:0.27756
loss:0.27827
loss:0.28689
loss:0.29102
loss:0.23503
loss:0.27633
loss:0.28785
loss:0.25109
loss:0.26770
loss:0.29148
loss:0.26543
loss:0.26163
loss:0.26849
loss:0.26053
loss:0.27591
loss:0.25659
loss:0.26435
loss:0.27426
loss:0.26251
loss:0.24512
loss:0.27142
loss:0.26666
loss:0.24822
loss:0.28317
loss:0.25111
loss:0.27013
loss:0.27185
loss:0.23665
loss:0.25496
loss:0.30598
loss:0.23963
loss:0.26675
loss:0.27557
loss:0.27810
loss:0.28551
loss:0.25040
loss:0.27082
loss:0.27733
loss:0.25913
loss:0.27530
loss:0.24828
loss:0.25407
loss:0.25973
loss:0.29506
loss:0.24115
loss:0.23267
loss:0.25011
loss:0.23825
loss:0.29259
loss:0.25319
loss:0.26053
loss:0.28808
loss:0.27194
loss:0.25946
loss:0.28343
loss:0.26685
loss:0.26705
loss:0.26557
loss:0.26317
loss:0.26738
loss:0.29582
Evaluation at Epoch 3/10. Step:2676/8920. AccuracyMetric: acc=0.89071

loss:0.27693
loss:0.24544
loss:0.24530
loss:0.24992
loss:0.23740
loss:0.28367
loss:0.24914
loss:0.26331
loss:0.26165
loss:0.26062
loss:0.25392
loss:0.24345
loss:0.26025
loss:0.27887
loss:0.25135
loss:0.26703
loss:0.24766
loss:0.25825
loss:0.26087
loss:0.28696
loss:0.26372
loss:0.25444
loss:0.27531
loss:0.27283
loss:0.29727
loss:0.28513
loss:0.27507
loss:0.28297
loss:0.26028
loss:0.27826
loss:0.24227
loss:0.25461
loss:0.27277
loss:0.29829
loss:0.29349
loss:0.26477
loss:0.24227
loss:0.30258
loss:0.25079
loss:0.23493
loss:0.29419
loss:0.26851
loss:0.26069
loss:0.26938
loss:0.25409
loss:0.27189
loss:0.28186
loss:0.28381
loss:0.28591
loss:0.26542
loss:0.27854
loss:0.26758
loss:0.25617
loss:0.26726
loss:0.27074
loss:0.26345
loss:0.24791
loss:0.26922
loss:0.24969
loss:0.24949
loss:0.26800
loss:0.30064
loss:0.27137
loss:0.26666
loss:0.24601
loss:0.25430
loss:0.23453
loss:0.28300
loss:0.25476
loss:0.28365
loss:0.28068
loss:0.27302
loss:0.26384
loss:0.27945
loss:0.29087
loss:0.25189
loss:0.27349
loss:0.29209
loss:0.26479
loss:0.24462
loss:0.24718
loss:0.26481
loss:0.27081
loss:0.25502
loss:0.28421
loss:0.26767
loss:0.25222
loss:0.27242
loss:0.26935
loss:0.26870
loss:0.26492
loss:0.27746
loss:0.28807
loss:0.27364
loss:0.25145
loss:0.27203
loss:0.26713
loss:0.26331
loss:0.24522
loss:0.27411
loss:0.23308
loss:0.27222
loss:0.24495
loss:0.26142
loss:0.29015
loss:0.27000
loss:0.25060
loss:0.27522
loss:0.27543
loss:0.26354
loss:0.25961
loss:0.24603
loss:0.28622
loss:0.23507
loss:0.24877
loss:0.26493
loss:0.27327
loss:0.26943
loss:0.25204
loss:0.26781
loss:0.26072
loss:0.27894
loss:0.24119
loss:0.27561
loss:0.25248
loss:0.26734
loss:0.28925
loss:0.24454
loss:0.28815
loss:0.27587
loss:0.25680
loss:0.28420
loss:0.26662
loss:0.27742
loss:0.26594
loss:0.24039
loss:0.27202
loss:0.24895
loss:0.24541
loss:0.25411
loss:0.26954
loss:0.26716
loss:0.28349
loss:0.29131
loss:0.28806
loss:0.27587
loss:0.25651
loss:0.25843
loss:0.30225
loss:0.25248
loss:0.26777
loss:0.24668
loss:0.26200
loss:0.24515
loss:0.25322
loss:0.26921
loss:0.25410
loss:0.28040
loss:0.28126
loss:0.27144
loss:0.24411
loss:0.24100
loss:0.26843
loss:0.26350
loss:0.28488
loss:0.27855
loss:0.27389
loss:0.24820
loss:0.26554
loss:0.28691
loss:0.30056
loss:0.28522
loss:0.25013
loss:0.27349
loss:0.27429
loss:0.27848
loss:0.25997
loss:0.27530
Evaluation at Epoch 4/10. Step:3568/8920. AccuracyMetric: acc=0.89071

loss:0.26046
loss:0.25389
loss:0.26524
loss:0.29224
loss:0.26777
loss:0.26175
loss:0.26627
loss:0.22959
loss:0.24803
loss:0.24369
loss:0.23795
loss:0.26553
loss:0.24289
loss:0.24344
loss:0.25607
loss:0.25446
loss:0.25805
loss:0.26111
loss:0.25572
loss:0.23262
loss:0.27396
loss:0.26545
loss:0.27002
loss:0.30510
loss:0.26788
loss:0.25857
loss:0.23939
loss:0.25557
loss:0.23687
loss:0.26084
loss:0.28676
loss:0.27951
loss:0.25806
loss:0.27586
loss:0.25517
loss:0.28174
loss:0.27136
loss:0.23464
loss:0.25684
loss:0.24031
loss:0.24844
loss:0.26754
loss:0.27691
loss:0.26355
loss:0.24634
loss:0.26217
loss:0.25255
loss:0.28054
loss:0.27180
loss:0.27607
loss:0.27066
loss:0.26079
loss:0.26839
loss:0.27544
loss:0.27050
loss:0.29148
loss:0.24201
loss:0.26174
loss:0.26246
loss:0.25770
loss:0.24382
loss:0.25033
loss:0.25484
loss:0.25448
loss:0.24847
loss:0.26109
loss:0.26814
loss:0.23354
loss:0.24118
loss:0.27407
loss:0.25040
loss:0.28335
loss:0.28828
loss:0.26381
loss:0.28067
loss:0.23833
loss:0.25228
loss:0.24527
loss:0.27107
loss:0.24224
loss:0.27112
loss:0.24248
loss:0.22822
loss:0.27714
loss:0.27739
loss:0.27829
loss:0.26042
loss:0.26161
loss:0.26340
loss:0.27783
loss:0.26765
loss:0.25930
loss:0.25043
loss:0.25448
loss:0.27227
loss:0.27334
loss:0.24278
loss:0.28069
loss:0.27448
loss:0.27772
loss:0.26849
loss:0.30070
loss:0.25681
loss:0.25221
loss:0.25415
loss:0.23829
loss:0.21672
loss:0.23375
loss:0.25337
loss:0.25902
loss:0.27644
loss:0.25621
loss:0.27094
loss:0.25020
loss:0.29770
loss:0.30637
loss:0.24368
loss:0.25077
loss:0.26730
loss:0.24753
loss:0.26300
loss:0.23271
loss:0.25797
loss:0.25923
loss:0.28722
loss:0.24496
loss:0.23675
loss:0.26750
loss:0.27364
loss:0.28190
loss:0.25594
loss:0.27380
loss:0.26947
loss:0.28219
loss:0.28240
loss:0.25435
loss:0.24504
loss:0.27725
loss:0.22964
loss:0.26688
loss:0.23697
loss:0.28794
loss:0.27092
loss:0.24460
loss:0.23963
loss:0.27215
loss:0.29291
loss:0.27041
loss:0.27116
loss:0.25157
loss:0.24171
loss:0.27068
loss:0.25164
loss:0.26557
loss:0.28904
loss:0.28003
loss:0.25765
loss:0.24193
loss:0.24401
loss:0.31276
loss:0.25325
loss:0.25135
loss:0.28819
loss:0.24651
loss:0.27352
loss:0.26262
loss:0.26842
loss:0.25630
loss:0.27319
loss:0.25831
loss:0.27418
loss:0.27389
loss:0.26474
loss:0.25597
loss:0.28182
loss:0.24000
loss:0.26216
loss:0.26626
loss:0.25003
Evaluation at Epoch 5/10. Step:4460/8920. AccuracyMetric: acc=0.890678

loss:0.24879
loss:0.23323
loss:0.28364
loss:0.23210
loss:0.22815
loss:0.23198
loss:0.25972
loss:0.23199
loss:0.28815
loss:0.25417
loss:0.25883
loss:0.24345
loss:0.24597
loss:0.24251
loss:0.25890
loss:0.24892
loss:0.27689
loss:0.26964
loss:0.24243
loss:0.26143
loss:0.24621
loss:0.25718
loss:0.25938
loss:0.23212
loss:0.23745
loss:0.25440
loss:0.22434
loss:0.24739
loss:0.27644
loss:0.24320
loss:0.23725
loss:0.24486
loss:0.26245
loss:0.23755
loss:0.27544
loss:0.25561
loss:0.28343
loss:0.26587
loss:0.24894
loss:0.25431
loss:0.26852
loss:0.23669
loss:0.25597
loss:0.24514
loss:0.23683
loss:0.25076
loss:0.24818
loss:0.24793
loss:0.26855
loss:0.26769
loss:0.25935
loss:0.27744
loss:0.22374
loss:0.27505
loss:0.23970
loss:0.21556
loss:0.26204
loss:0.26012
loss:0.23551
loss:0.28759
loss:0.25741
loss:0.24725
loss:0.26960
loss:0.25929
loss:0.26547
loss:0.26933
loss:0.25749
loss:0.26991
loss:0.26599
loss:0.23857
loss:0.26285
loss:0.26842
loss:0.28405
loss:0.24835
loss:0.27155
loss:0.24198
loss:0.23607
loss:0.27416
loss:0.25579
loss:0.24138
loss:0.28043
loss:0.25253
loss:0.26232
loss:0.26819
loss:0.27428
loss:0.25281
loss:0.27885
loss:0.27489
loss:0.23611
loss:0.29104
loss:0.25591
loss:0.24919
loss:0.25833
loss:0.24769
loss:0.26497
loss:0.23091
loss:0.21227
loss:0.24820
loss:0.26780
loss:0.26356
loss:0.27847
loss:0.28618
loss:0.26547
loss:0.25102
loss:0.24670
loss:0.27131
loss:0.24592
loss:0.23157
loss:0.27215
loss:0.26149
loss:0.25978
loss:0.27244
loss:0.24759
loss:0.25095
loss:0.25356
loss:0.23735
loss:0.23828
loss:0.23546
loss:0.26631
loss:0.26755
loss:0.26922
loss:0.24506
loss:0.27501
loss:0.24665
loss:0.25839
loss:0.25276
loss:0.26346
loss:0.25388
loss:0.30700
loss:0.29029
loss:0.26332
loss:0.25536
loss:0.26028
loss:0.22531
loss:0.24556
loss:0.27827
loss:0.29074
loss:0.25184
loss:0.25266
loss:0.23647
loss:0.23979
loss:0.26998
loss:0.24486
loss:0.24344
loss:0.24032
loss:0.22115
loss:0.23645
loss:0.24016
loss:0.26709
loss:0.23957
loss:0.25921
loss:0.24098
loss:0.25278
loss:0.23154
loss:0.25599
loss:0.26501
loss:0.25406
loss:0.27735
loss:0.24437
loss:0.26555
loss:0.25109
loss:0.24366
loss:0.24407
loss:0.26246
loss:0.26680
loss:0.24689
loss:0.26591
loss:0.27816
loss:0.26639
loss:0.27273
loss:0.27118
loss:0.26850
loss:0.27101
loss:0.23818
loss:0.26103
loss:0.25040
loss:0.24217
loss:0.27396
Evaluation at Epoch 6/10. Step:5352/8920. AccuracyMetric: acc=0.890468

loss:0.23223
loss:0.23905
loss:0.23304
loss:0.23321
loss:0.25307
loss:0.24167
loss:0.24893
loss:0.25239
loss:0.24335
loss:0.24085
loss:0.26275
loss:0.23466
loss:0.24472
loss:0.24808
loss:0.23172
loss:0.24885
loss:0.24993
loss:0.23720
loss:0.23500
loss:0.24677
loss:0.27311
loss:0.25013
loss:0.26543
loss:0.26587
loss:0.23925
loss:0.25496
loss:0.25161
loss:0.26685
loss:0.24457
loss:0.24988
loss:0.23843
loss:0.23415
loss:0.24221
loss:0.23557
loss:0.23442
loss:0.24818
loss:0.24444
loss:0.26240
loss:0.25008
loss:0.23477
loss:0.22630
loss:0.27860
loss:0.26110
loss:0.25467
loss:0.25284
loss:0.23527
loss:0.23495
loss:0.22392
loss:0.25366
loss:0.26743
loss:0.26303
loss:0.23974
loss:0.23977
loss:0.24946
loss:0.24375
loss:0.24118
loss:0.22461
loss:0.25083
loss:0.22574
loss:0.24018
loss:0.23084
loss:0.26858
loss:0.27280
loss:0.24841
loss:0.24332
loss:0.26209
loss:0.22861
loss:0.24891
loss:0.25999
loss:0.25683
loss:0.23863
loss:0.23946
loss:0.23713
loss:0.24141
loss:0.27351
loss:0.23720
loss:0.26873
loss:0.25109
loss:0.26818
loss:0.23423
loss:0.23551
loss:0.24453
loss:0.25001
loss:0.23002
loss:0.24980
loss:0.25978
loss:0.26810
loss:0.23935
loss:0.23935
loss:0.24792
loss:0.28130
loss:0.27096
loss:0.23088
loss:0.26697
loss:0.21117
loss:0.25254
loss:0.27128
loss:0.23314
loss:0.27617
loss:0.24684
loss:0.23939
loss:0.24055
loss:0.24997
loss:0.25441
loss:0.24474
loss:0.24318
loss:0.25565
loss:0.24732
loss:0.25960
loss:0.23215
loss:0.25375
loss:0.24290
loss:0.23666
loss:0.25404
loss:0.28371
loss:0.26459
loss:0.21123
loss:0.23239
loss:0.24932
loss:0.24412
loss:0.23485
loss:0.24802
loss:0.24838
loss:0.23064
loss:0.23402
loss:0.24511
loss:0.23935
loss:0.23483
loss:0.24485
loss:0.28108
loss:0.27612
loss:0.25520
loss:0.25729
loss:0.25093
loss:0.25687
loss:0.23256
loss:0.25012
loss:0.25485
loss:0.26203
loss:0.25846
loss:0.25879
loss:0.21144
loss:0.26022
loss:0.23188
loss:0.27029
loss:0.24942
loss:0.25657
loss:0.24596
loss:0.26780
loss:0.24260
loss:0.26263
loss:0.24572
loss:0.24884
loss:0.26020
loss:0.23469
loss:0.25401
loss:0.23853
loss:0.23491
loss:0.26486
loss:0.25221
loss:0.23045
loss:0.26639
loss:0.26620
loss:0.25062
loss:0.25928
loss:0.26517
loss:0.24727
loss:0.24827
loss:0.24433
loss:0.25017
loss:0.23727
loss:0.31160
loss:0.28268
loss:0.28085
loss:0.27780
loss:0.27835
loss:0.26103
loss:0.23144
Evaluation at Epoch 7/10. Step:6244/8920. AccuracyMetric: acc=0.890686

loss:0.28689
loss:0.24999
loss:0.23532
loss:0.25914
loss:0.25686
loss:0.23332
loss:0.23781
loss:0.24227
loss:0.25013
loss:0.25308
loss:0.25719
loss:0.24457
loss:0.25146
loss:0.26698
loss:0.26260
loss:0.24780
loss:0.23955
loss:0.24522
loss:0.23230
loss:0.24662
loss:0.23718
loss:0.26560
loss:0.24820
loss:0.24683
loss:0.22889
loss:0.20354
loss:0.25659
loss:0.24337
loss:0.24135
loss:0.24406
loss:0.28377
loss:0.23673
loss:0.27805
loss:0.24241
loss:0.28274
loss:0.24432
loss:0.26704
loss:0.23209
loss:0.23185
loss:0.25480
loss:0.22928
loss:0.26217
loss:0.24040
loss:0.24836
loss:0.24882
loss:0.23549
loss:0.27450
loss:0.23697
loss:0.23940
loss:0.23943
loss:0.24428
loss:0.24120
loss:0.27160
loss:0.24432
loss:0.21824
loss:0.24362
loss:0.23203
loss:0.23261
loss:0.23961
loss:0.24124
loss:0.24020
loss:0.22254
loss:0.26383
loss:0.24323
loss:0.21727
loss:0.24506
loss:0.21571
loss:0.24289
loss:0.28072
loss:0.23839
loss:0.25389
loss:0.23293
loss:0.24008
loss:0.24550
loss:0.25866
loss:0.22702
loss:0.25430
loss:0.24700
loss:0.26498
loss:0.24452
loss:0.26592
loss:0.26791
loss:0.22506
loss:0.27044
loss:0.22536
loss:0.25209
loss:0.23731
loss:0.21331
loss:0.24385
loss:0.22489
loss:0.24713
loss:0.26190
loss:0.25924
loss:0.26417
loss:0.27601
loss:0.26684
loss:0.23270
loss:0.23602
loss:0.23306
loss:0.22856
loss:0.23825
loss:0.24869
loss:0.27766
loss:0.27187
loss:0.21825
loss:0.21370
loss:0.23568
loss:0.26147
loss:0.25072
loss:0.22699
loss:0.24769
loss:0.21712
loss:0.24404
loss:0.26598
loss:0.24231
loss:0.24674
loss:0.23174
loss:0.25460
loss:0.27738
loss:0.22597
loss:0.24000
loss:0.25432
loss:0.27605
loss:0.24111
loss:0.23757
loss:0.28889
loss:0.25575
loss:0.24247
loss:0.26000
loss:0.23725
loss:0.24663
loss:0.23816
loss:0.23252
loss:0.21941
loss:0.24596
loss:0.25691
loss:0.23459
loss:0.22539
loss:0.23247
loss:0.24542
loss:0.24338
loss:0.24875
loss:0.25601
loss:0.24545
loss:0.22702
loss:0.24220
loss:0.22294
loss:0.23232
loss:0.22820
loss:0.24576
loss:0.24273
loss:0.23267
loss:0.22930
loss:0.23103
loss:0.24346
loss:0.22456
loss:0.24351
loss:0.23151
loss:0.22555
loss:0.23181
loss:0.23185
loss:0.23694
loss:0.23846
loss:0.24459
loss:0.25929
loss:0.22993
loss:0.21663
loss:0.22835
loss:0.24914
loss:0.24778
loss:0.24054
loss:0.24340
loss:0.27218
loss:0.22005
loss:0.25517
loss:0.26946
loss:0.24181
loss:0.23844
loss:0.23014
Evaluation at Epoch 8/10. Step:7136/8920. AccuracyMetric: acc=0.890242

loss:0.22673
loss:0.22785
loss:0.23952
loss:0.23565
loss:0.21912
loss:0.22962
loss:0.21365
loss:0.24477
loss:0.25382
loss:0.23437
loss:0.23444
loss:0.21411
loss:0.22808
loss:0.22587
loss:0.22861
loss:0.23319
loss:0.24536
loss:0.23987
loss:0.22793
loss:0.22633
loss:0.23325
loss:0.22579
loss:0.24508
loss:0.23825
loss:0.24598
loss:0.23856
loss:0.22540
loss:0.22858
loss:0.23434
loss:0.21048
loss:0.23155
loss:0.23815
loss:0.23920
loss:0.23064
loss:0.21602
loss:0.25148
loss:0.24059
loss:0.24586
loss:0.21873
loss:0.23163
loss:0.23926
loss:0.26229
loss:0.23416
loss:0.20450
loss:0.25301
loss:0.22913
loss:0.23604
loss:0.25626
loss:0.24165
loss:0.23754
loss:0.20430
loss:0.23575
loss:0.22733
loss:0.22712
loss:0.21131
loss:0.25129
loss:0.24798
loss:0.22459
loss:0.22710
loss:0.21880
loss:0.24006
loss:0.23259
loss:0.23556
loss:0.23674
loss:0.25838
loss:0.21607
loss:0.24201
loss:0.24934
loss:0.23811
loss:0.21805
loss:0.25769
loss:0.22985
loss:0.23578
loss:0.23706
loss:0.22794
loss:0.23389
loss:0.23819
loss:0.26417
loss:0.22139
loss:0.24095
loss:0.21845
loss:0.21756
loss:0.20795
loss:0.23230
loss:0.20216
loss:0.24844
loss:0.24453
loss:0.23767
loss:0.23795
loss:0.21995
loss:0.23652
loss:0.24521
loss:0.22786
loss:0.24298
loss:0.26139
loss:0.21055
loss:0.24222
loss:0.23425
loss:0.22705
loss:0.24972
loss:0.23748
loss:0.22067
loss:0.24759
loss:0.25666
loss:0.24825
loss:0.25791
loss:0.21197
loss:0.22148
loss:0.22181
loss:0.21669
loss:0.22570
loss:0.23630
loss:0.24061
loss:0.23491
loss:0.25788
loss:0.24511
loss:0.18631
loss:0.23898
loss:0.25160
loss:0.25113
loss:0.24225
loss:0.22122
loss:0.22576
loss:0.21435
loss:0.22346
loss:0.24371
loss:0.20891
loss:0.24524
loss:0.22955
loss:0.22052
loss:0.24073
loss:0.22192
loss:0.26626
loss:0.24913
loss:0.23853
loss:0.22432
loss:0.23561
loss:0.22808
loss:0.25542
loss:0.25499
loss:0.23380
loss:0.23499
loss:0.21471
loss:0.24154
loss:0.23857
loss:0.23604
loss:0.22331
loss:0.23137
loss:0.24728
loss:0.22792
loss:0.23601
loss:0.24874
loss:0.24622
loss:0.22563
loss:0.24295
loss:0.22233
loss:0.24934
loss:0.23792
loss:0.24906
loss:0.26096
loss:0.23803
loss:0.25088
loss:0.24697
loss:0.24994
loss:0.22284
loss:0.23518
loss:0.22236
loss:0.24019
loss:0.21018
loss:0.25873
loss:0.24080
loss:0.23455
loss:0.25680
loss:0.25171
loss:0.24009
loss:0.22078
loss:0.21916
loss:0.23816
Evaluation at Epoch 9/10. Step:8028/8920. AccuracyMetric: acc=0.888051

loss:0.22721
loss:0.26459
loss:0.23206
loss:0.21433
loss:0.21809
loss:0.22902
loss:0.22921
loss:0.23574
loss:0.23132
loss:0.22783
loss:0.24291
loss:0.20596
loss:0.25628
loss:0.22171
loss:0.22997
loss:0.20260
loss:0.23859
loss:0.19931
loss:0.24515
loss:0.19773
loss:0.24188
loss:0.22828
loss:0.19648
loss:0.21529
loss:0.23464
loss:0.22963
loss:0.20653
loss:0.25112
loss:0.20239
loss:0.24110
loss:0.21232
loss:0.22199
loss:0.24071
loss:0.21855
loss:0.25305
loss:0.22572
loss:0.21225
loss:0.21907
loss:0.24394
loss:0.23304
loss:0.21059
loss:0.21851
loss:0.22990
loss:0.22189
loss:0.21649
loss:0.21379
loss:0.22948
loss:0.22570
loss:0.19089
loss:0.26147
loss:0.22852
loss:0.24399
loss:0.23077
loss:0.22550
loss:0.24639
loss:0.22680
loss:0.22471
loss:0.21250
loss:0.22363
loss:0.25500
loss:0.23490
loss:0.21639
loss:0.21900
loss:0.22471
loss:0.21014
loss:0.19625
loss:0.21676
loss:0.21178
loss:0.21762
loss:0.23233
loss:0.22336
loss:0.22580
loss:0.20984
loss:0.24509
loss:0.25622
loss:0.19878
loss:0.21009
loss:0.21836
loss:0.26103
loss:0.21463
loss:0.21722
loss:0.23291
loss:0.23669
loss:0.23432
loss:0.21804
loss:0.21518
loss:0.20501
loss:0.22807
loss:0.22183
loss:0.21884
loss:0.20503
loss:0.22240
loss:0.20162
loss:0.22694
loss:0.22006
loss:0.21908
loss:0.22144
loss:0.23217
loss:0.21554
loss:0.25217
loss:0.20075
loss:0.23451
loss:0.22752
loss:0.22074
loss:0.21282
loss:0.23165
loss:0.20277
loss:0.24843
loss:0.21801
loss:0.21389
loss:0.22939
loss:0.25355
loss:0.23767
loss:0.24055
loss:0.22396
loss:0.22159
loss:0.24198
loss:0.23860
loss:0.22425
loss:0.22694
loss:0.21836
loss:0.22438
loss:0.22143
loss:0.22619
loss:0.24947
loss:0.22563
loss:0.22397
loss:0.22888
loss:0.22850
loss:0.25071
loss:0.21594
loss:0.23976
loss:0.24171
loss:0.23218
loss:0.23339
loss:0.21950
loss:0.23661
loss:0.22098
loss:0.20975
loss:0.22209
loss:0.21589
loss:0.23147
loss:0.20785
loss:0.21987
loss:0.23351
loss:0.22437
loss:0.21705
loss:0.22643
loss:0.22672
loss:0.22335
loss:0.23510
loss:0.23750
loss:0.22770
loss:0.25295
loss:0.21602
loss:0.25090
loss:0.21846
loss:0.23564
loss:0.23744
loss:0.23013
loss:0.26092
loss:0.24628
loss:0.24212
loss:0.22213
loss:0.23431
loss:0.24818
loss:0.22416
loss:0.24657
loss:0.25041
loss:0.21026
loss:0.22730
loss:0.24863
loss:0.22943
loss:0.21717
loss:0.23634
loss:0.21898
loss:0.23735
loss:0.24880
loss:0.23046
Evaluation at Epoch 10/10. Step:8920/8920. AccuracyMetric: acc=0.889119


In Epoch:1/Step:892, got best dev performance:AccuracyMetric: acc=0.89071
Reloaded the best model.
Train Done.
[tester] 
AccuracyMetric: acc=0.89071
Test Done.
[34mAuto commit by fitlog[0m
[34mAuto commit by fitlog[0m
(vocab_size,class_num,seq_len):(27658,2,1070)
[34mAuto commit by fitlog[0m
(vocab_size,class_num,seq_len):(27658,2,1070)
test_size:43444
[34mAuto commit by fitlog[0m
(vocab_size,class_num,seq_len):(27658,2,1070)
test_size:43444
[34mAuto commit by fitlog[0m
(vocab_size,class_num,seq_len):(27658,2,1070)
test_size:43444
Loading the model ../../data/models/LSTMText/default/best_LSTMText_accuracy_2019-06-14-04-01-48
LSTMText(
  (embeds): Embedding(
    27658, 128
    (dropout): Dropout(p=0.0)
  )
  (lstm): LSTM(128, 128, num_layers=2, dropout=0.2, bidirectional=True)
  (fc): Linear(in_features=256, out_features=2, bias=True)
  (dropout): Dropout(p=0.2)
)
[34mAuto commit by fitlog[0m
(vocab_size,class_num,seq_len):(27658,2,1070)
test_size:43444
Loading the model ../../data/models/LSTMText/default/best_LSTMText_accuracy_2019-06-14-04-01-48
LSTMText(
  (embeds): Embedding(
    27658, 128
    (dropout): Dropout(p=0.0)
  )
  (lstm): LSTM(128, 128, num_layers=2, dropout=0.2, bidirectional=True)
  (fc): Linear(in_features=256, out_features=2, bias=True)
  (dropout): Dropout(p=0.2)
)
DataSet({'text': ['my', 'kindergarten', 'students', 'come', 'from', 'a', 'variety', 'of', 'backgrounds', 'as', 'their', 'teacher', 'it', 'is', 'my', 'duty', 'to', 'provide', 'a', 'wide', 'variety', 'of', 'texts', 'as', 'we', 'begin', 'to', 'learn', 'how', 'to', 'read', 'n', 'nkindergarten', 'students', 'are', 'excited', 'about', 'school', 'especially', 'about', 'learning', 'to', 'read', 'i', 'hope', 'to', 'feed', 'their', 'excitement', 'by', 'introducing', 'new', 'types', 'of', 'text', 'and', 'different', 'reasons', 'for', 'learning', 'learning', 'to', 'read', 'non', 'fiction', 'text', 'is', 'often', 'a', 'struggle', 'for', 'beginning', 'readers', 'but', 'let', 's', 'find', 'out', 'magazine', 'makes', 'it', 'fun', 'n', 'n', 'let', 's', 'find', 'out', 'is', 'an', 'awesome', 'magazine', 'that', 'keeps', 'students', 'entertained', 'with', 'colorful', 'thematic', 'stories', 'presented', 'at', 'age', 'appropriate', 'levels', 'the', 'simple', 'repetitive', 'text', 'will', 'introduce', 'our', 'students', 'to', 'important', 'early', 'reading', 'skills', 'the', 'magazines', 'also', 'include', 'posters', 'to', 'use', 'in', 'our', 'classroom', 'n', 'nyour', 'generous', 'donation', 'will', 'help', 'a', 'wonderful', 'group', 'of', 'kindergarten', 'students', 'read', 'and', 'enjoy', 'non', 'fiction', 'texts', 'as', 'a', 'teacher', 'it', 'is', 'my', 'job', 'to', 'empower', 'students', 'by', 'giving', 'them', 'the', 'tools', 'needed', 'to', 'become', 'successful', 'lifelong', 'learners', 'with', 'your', 'donation', 'you', 'are', 'taking', 'part', 'in', 'this', 'wonderful', 'process'] type=list,
'class': 0.0 type=float,
'words': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 185, 5, 79, 42, 6, 222, 7, 382, 29, 14, 153, 28, 17, 9, 2673, 2, 98, 6, 525, 222, 7, 644, 29, 25, 509, 2, 40, 76, 2, 48, 13, 3684, 5, 11, 195, 57, 23, 443, 57, 33, 2, 48, 19, 332, 2, 1927, 14, 590, 56, 2445, 65, 745, 7, 414, 4, 144, 1734, 16, 33, 33, 2, 48, 462, 487, 414, 17, 205, 6, 364, 16, 530, 136, 54, 513, 72, 227, 123, 961, 475, 28, 132, 13, 13, 513, 72, 227, 123, 17, 44, 1137, 961, 15, 2082, 5, 5457, 20, 938, 4139, 214, 1335, 38, 386, 612, 284, 3, 565, 5023, 414, 10, 1083, 21, 5, 2, 119, 555, 32, 58, 3, 926, 66, 633, 1435, 2, 55, 8, 21, 35, 13, 516, 937, 276, 10, 31, 6, 242, 149, 7, 185, 5, 48, 4, 200, 462, 487, 644, 29, 6, 153, 28, 17, 9, 617, 2, 1722, 5, 56, 476, 24, 3, 236, 327, 2, 134, 220, 842, 150, 20, 93, 276, 37, 11, 515, 219, 8, 26, 242, 413] type=list,
'seq_len': 178 type=int,
'target': 0 type=int},
{'text': ['my', 'special', 'education', 'students', 'are', 'on', 'the', 'path', 'to', 'developing', 'a', 'life', 'long', 'love', 'of', 'learning', 'but', 'get', 'frustrated', 'with', 'their', 'current', 'reading', 'material', 'options', 'as', 'they', 'are', 'not', 'high', 'interest', 'age', 'appropriate', 'or', 'accessible', 'help', 'them', 'fall', 'in', 'love', 'with', 'books', 'n', 'nour', 'school', 'is', 'a', 'charter', 'on', 'the', 'south', 'side', 'of', 'chicago', 'that', 'focuses', 'on', 'social', 'emotional', 'learning', 'my', 'students', 'receive', 'instruction', 'in', 'smaller', 'more', 'structured', 'special', 'education', 'classes', 'due', 'to', 'their', 'reading', 'deficiencies', 'they', 'want', 'to', 'be', 'able', 'to', 'read', 'like', 'their', 'non', 'disabled', 'peers', 'but', 'many', 'of', 'them', 'have', 'given', 'up', 'on', 'school', 'and', 'are', 'not', 'engaged', 'in', 'their', 'classwork', 'i', 'would', 'love', 'to', 'provide', 'them', 'with', 'high', 'interest', 'novels', 'to', 'get', 'them', 're', 'invested', 'in', 'their', 'learning', 'and', 'help', 'them', 'grow', 'both', 'as', 'readers', 'and', 'as', 'individuals', 'n', 'norca', 'soundings', 'are', 'teen', 'fiction', 'for', 'reluctant', 'readers', 'we', 'will', 'use', 'orca', 'soundings', 'in', 'leveled', 'small', 'group', 'literature', 'circles', 'students', 'will', 'progress', 'through', 'the', 'levels', 'as', 'they', 'strengthen', 'their', 'reading', 'comprehension', 'and', 'fluency', 'our', 'literature', 'circles', 'will', 'be', 'a', 'combination', 'of', 'discussion', 'based', 'activities', 'and', 'comprehension', 'checks', 'and', 'will', 'make', 'students', 'feel', 'safe', 'in', 'their', 'reading', 'abilities', 'in', 'front', 'of', 'their', 'peers', 'n', 'nfunding', 'this', 'project', 'will', 'make', 'such', 'a', 'difference', 'in', 'the', 'lives', 'of', 'my', 'students', 'because', 'you', 'will', 'be', 'providing', 'them', 'with', 'novels', 'that', 'they', 'won', 't', 'want', 'to', 'put', 'down', 'and', 'creating', 'ravenous', 'readers', 'for', 'life'] type=list,
'class': 0.0 type=float,
'words': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 192, 118, 5, 11, 27, 3, 1450, 2, 791, 6, 112, 333, 61, 7, 33, 54, 86, 1476, 20, 14, 363, 32, 511, 1662, 29, 12, 11, 36, 90, 288, 386, 612, 52, 1098, 31, 24, 968, 8, 61, 20, 39, 13, 235, 23, 17, 6, 616, 27, 3, 1203, 1293, 7, 2493, 15, 2136, 27, 262, 1218, 33, 9, 5, 283, 302, 8, 1764, 46, 2249, 192, 118, 282, 366, 2, 14, 32, 6725, 12, 69, 2, 22, 49, 2, 48, 89, 14, 462, 2126, 454, 54, 45, 7, 24, 18, 440, 102, 27, 23, 4, 11, 36, 354, 8, 14, 2848, 19, 51, 61, 2, 98, 24, 20, 90, 288, 660, 2, 86, 24, 600, 2639, 8, 14, 33, 4, 31, 24, 371, 321, 29, 136, 4, 29, 1113, 13, 1, 18928, 11, 2817, 487, 16, 1215, 136, 25, 10, 55, 13741, 18928, 8, 739, 154, 149, 278, 1772, 5, 10, 720, 95, 3, 284, 29, 12, 1485, 14, 32, 326, 4, 445, 21, 278, 1772, 10, 22, 6, 2407, 7, 1409, 442, 129, 4, 326, 6161, 4, 10, 63, 5, 194, 495, 8, 14, 32, 570, 8, 897, 7, 14, 454, 13, 4392, 26, 83, 10, 63, 169, 6, 316, 8, 3, 176, 7, 9, 5, 100, 37, 10, 22, 341, 24, 20, 660, 15, 12, 1061, 85, 69, 2, 330, 449, 4, 453, 8243, 136, 16, 112] type=list,
'seq_len': 232 type=int,
'target': 0 type=int},
{'text': ['our', 'classroom', 'is', 'a', 'diverse', 'engaging', 'place', 'where', 'we', 'encourage', 'risk', 'taking', 'we', 'celebrate', 'our', 'individual', 'strengths', 'and', 'support', 'each', 'others', 'weaknesses', 'our', 'superpower', 'is', 'learning', 'we', 'love', 'exploring', 'asking', 'questions', 'then', 'conducting', 'our', 'own', 'research', 'to', 'answer', 'these', 'questions', 'n', 'nour', 'classroom', 'is', 'made', 'of', 'students', 'from', 'all', 'over', 'the', 'world', 'literally', 'we', 'have', 'students', 'from', 'various', 'parts', 'of', 'africa', 'thailand', 'burma', 'mexico', 'puerto', 'rico', 'and', 'even', 'different', 'parts', 'of', 'the', 'united', 'states', 'all', 'of', 'these', 'backgrounds', 'makes', 'our', 'classroom', 'a', 'very', 'culture', 'rich', 'place', 'as', 'a', 'class', 'we', 'are', 'working', 'on', 'strengthening', 'our', 'english', 'language', 'proficiency', 'we', 'have', 'high', 'expectations', 'for', 'ourselves', 'and', 'hold', 'ourselves', 'to', 'them', 'our', 'school', 'is', 'a', 'title', 'i', 'low', 'income', 'school', 'in', '3rd', 'grade', 'as', 'we', 'set', 'our', 'educational', 'foundation', 'we', 'are', 'preparing', 'our', 'students', 'to', 'stop', 'the', 'cycle', 'of', 'poverty', 'as', 'we', 'prepare', 'them', 'to', 'continue', 'their', 'educational', 'career', 'which', 'includes', 'college', 'we', 'are', 'very', 'intentional', 'when', 'instilling', 'the', 'importance', 'of', 'goal', 'setting', 'and', 'achieving', 'n', 'n', 'n', 'nthe', 'live', 'butterfly', 'pavilion', 'will', 'allow', 'our', 'class', 'to', 'observe', 'the', 'live', 'cycle', 'of', 'butterflies', 'up', 'close', 'and', 'personal', 'currently', 'we', 'are', 'researching', 'ants', 'and', 'have', 'our', 'own', 'ant', 'farm', 'the', 'nonfiction', 'books', 'we', 'are', 'requesting', 'will', 'allow', 'us', 'to', 'practice', 'using', 'text', 'features', 'to', 'locate', 'information', 'in', 'books', 'and', 'not', 'just', 'rely', 'on', 'the', 'internet', 'finally', 'pencils', 'are', 'a', 'valuable', 'commodity', 'in', '3rd', 'grade', 'and', 'receiving', 'additional', 'pencils', 'will', 'help', 'us', 'when', 'it', 'is', 'time', 'to', 'write', 'our', 'observations', 'n', 'ndonations', 'to', 'this', 'project', 'will', 'bring', 'valuable', 'science', 'into', 'our', 'classroom', 'through', 'this', 'project', 'our', 'classroom', 'will', 'be', 'able', 'to', 'practice', 'reading', 'writing', 'and', 'math', 'through', 'an', 'authentic', 'experience', 'this', 'will', 'allow', 'our', 'class', 'to', 'see', 'science', 'happen', 'in', 'real', 'time', 'this', 'will', 'tie', 'an', 'experience', 'to', 'their', 'academics', 'creating', 'a', 'lasting', 'impact', 'on', 'their', 'educational', 'career'] type=list,
'class': 0.0 type=float,
'words': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 21, 35, 17, 6, 275, 334, 212, 130, 25, 439, 896, 515, 25, 2167, 21, 480, 2210, 4, 173, 87, 270, 3846, 21, 25904, 17, 33, 25, 61, 1223, 444, 542, 280, 3621, 21, 122, 312, 2, 861, 34, 542, 13, 235, 35, 17, 425, 7, 5, 42, 41, 181, 3, 84, 1926, 25, 18, 5, 42, 394, 1037, 7, 3214, 7883, 6701, 2203, 6026, 6710, 4, 172, 144, 1037, 7, 3, 1299, 1021, 41, 7, 34, 382, 475, 21, 35, 6, 81, 831, 799, 212, 29, 6, 53, 25, 11, 175, 27, 4170, 21, 142, 106, 2139, 25, 18, 90, 1066, 16, 1865, 4, 740, 1865, 2, 24, 21, 23, 17, 6, 257, 19, 165, 232, 23, 8, 808, 68, 29, 25, 248, 21, 311, 669, 25, 11, 1406, 21, 5, 2, 1058, 3, 1262, 7, 221, 29, 25, 526, 24, 2, 317, 14, 311, 869, 113, 952, 230, 25, 11, 81, 10282, 67, 4248, 3, 701, 7, 298, 605, 4, 1562, 13, 13, 13, 138, 225, 2177, 11155, 10, 97, 21, 53, 2, 1510, 3, 225, 1262, 7, 2656, 102, 806, 4, 521, 372, 25, 11, 2365, 6133, 4, 18, 21, 122, 6642, 2508, 3, 704, 39, 25, 11, 184, 10, 97, 116, 2, 170, 140, 414, 2076, 2, 3253, 325, 8, 39, 4, 36, 127, 1644, 27, 3, 541, 970, 498, 11, 6, 767, 8158, 8, 808, 68, 4, 1003, 676, 498, 10, 31, 116, 67, 28, 17, 64, 2, 260, 21, 2349, 13, 1921, 2, 26, 83, 10, 287, 767, 96, 126, 21, 35, 95, 26, 83, 21, 35, 10, 22, 49, 2, 170, 32, 125, 4, 78, 95, 44, 1670, 137, 26, 10, 97, 21, 53, 2, 111, 96, 887, 8, 291, 64, 26, 10, 2993, 44, 137, 2, 14, 1302, 453, 6, 1650, 346, 27, 14, 311, 869] type=list,
'seq_len': 304 type=int,
'target': 0 type=int},
{'text': ['have', 'you', 'ever', 'wonder', 'what', 'goes', 'on', 'in', 'a', 'classroom', 'on', 'a', 'rainy', 'day', 'that', 's', 'right', 'the', 'kids', 'still', 'need', 'to', 'move', 'around', 'and', 'burn', 'some', 'calories', 'well', 'in', 'my', 'class', 'we', 'dance', 'to', 'child', 'friendly', 'songs', 'or', 'simply', 'just', 'play', 'quiet', 'games', 'but', 'if', 'there', 's', 'no', 'rain', 'we', 'like', 'to', 'move', 'around', 'n', 'nmy', 'students', 'are', 'smart', 'driven', 'and', 'very', 'verbal', 'all', 'my', 'students', 'are', 'english', 'learners', 'with', 'low', 'economic', 'means', 'most', 'of', 'them', 'do', 'not', 'have', 'a', 'computer', 'or', 'other', 'technology', 'at', 'home', 'they', 'are', 'exposed', 'to', 'violence', 'and', 'socioeconomic', 'limitations', 'like', 'lack', 'of', 'food', 'that', 'others', 'take', 'for', 'granted', 'our', 'school', 'provides', 'a', 'safe', 'environment', 'away', 'from', 'gangs', 'and', 'other', 'socioeconomic', 'limitations', 'in', 'my', 'classroom', 'they', 'get', 'to', 'dream', 'the', 'sky', 'is', 'the', 'limit', 'they', 've', 'planned', 'future', 'traveling', 'plans', 'designed', 'their', 'dream', 'homes', 'and', 'have', 'become', 'celebrated', 'community', 'heroes', 'n', 'nthe', 'classroom', 'is', 'a', 'safe', 'haven', 'where', 'dreaming', 'high', 'is', 'encouraged', 'nothing', 'is', 'impossible', 'when', 'they', 'dream', 'write', 'and', 'share', 'their', 'future', 'plans', 'they', 'would', 'thrive', 'as', 'enthusiastic', 'learners', 'if', 'they', 'are', 'provided', 'with', 'the', 'memories', 'that', 'will', 'last', 'a', 'lifetime', 'they', 'are', 'enthusiastic', 'learners', 'because', 'they', 'are', 'provided', 'with', 'the', 'hands', 'on', 'experiences', 'and', 'activities', 'that', 'at', 'home', 'are', 'simply', 'unattainable', 'n', 'nin', 'today', 's', 'classroom', 'teachers', 'require', 'so', 'much', 'from', 'their', 'students', 'in', 'language', 'arts', 'math', 'and', 'science', 'however', 'physical', 'fitness', 'art', 'health', 'though', 'they', 'are', 'part', 'of', 'a', 'well', 'planned', 'curriculum', 'they', 'take', 'a', 'back', 'seat', 'it', 's', 'not', 'until', 'our', 'students', 'reach', '5th', 'grade', 'that', 'we', 'see', 'the', 'need', 'for', 'physical', 'fitness', 'fifth', 'graders', 'are', 'required', 'to', 'take', 'a', 'physical', 'exam', 'by', 'then', 'most', 'of', 'the', 'students', 'that', 'are', 'overweight', 'could', 'have', 'benefited', 'from', 'direct', 'instruction', 'in', 'health', 'and', 'nutrition', 'teachers', 'make', 'the', 'most', 'impact', 'when', 'the', 'students', 'are', 'in', 'the', 'lower', 'grades', 'they', 'are', 'like', 'little', 'sponges', 'that', 'learn', 'and', 'do', 'what', 'is', 'asked', 'of', 'them', 'this', 'is', 'the', 'reason', 'i', 'have', 'requested', 'material', 'that', 'will', 'instruct', 'my', 'students', 'directly', 'in', 'nutrition', 'physical', 'fitness', 'and', 'healthy', 'habits', 'in', 'order', 'to', 'have', 'more', 'active', 'students', 'i', 'will', 'like', 'to', 'have', 'resources', 'like', 'a', 'portable', 'speaker', 'system', 'ipod', 'touch', 'adapter', 'good', 'morning', 'exercises', 'cd', 'cool', 'aerobics', 'for', 'kids', 'cd', 'kimbo', 'educational', 'aerobic', 'power', 'cd', 'aa', 'batteries', 'we', 'will', 'move', 'n', 'ni', 'need', 'your', 'generosity', 'to', 'help', 'me', 'teach', 'healthy', 'habits', 'in', 'my', 'classroom', 'i', 'have', 'high', 'expectations', 'i', 'know', 'that', 'my', 'students', 'can', 'one', 'day', 'be', 'a', 'big', 'asset', 'to', 'our', 'country', 'in', 'order', 'to', 'contribute', 'to', 'our', 'society', 'when', 'need', 'to', 'guide', 'their', 'path', 'from', 'an', 'early', 'age', 'if', 'they', 'have', 'healthy', 'brains', 'and', 'body', 'then', 'gasping', 'new', 'concepts', 'in', 'any', 'subject', 'matter', 'is', 'much', 'easier', 'they', 'can', 'learn', 'to', 'be', 'more', 'active', 'be', 'healthy', 'and', 'at', 'the', 'same', 'time', 'benefit', 'themselves', 'by', 'being', 'well', 'rounded', 'students', 'in', 'all', 'academic', 'subjects'] type=list,
'class': 0.0 type=float,
'words': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 18, 37, 310, 1575, 75, 1390, 27, 8, 6, 35, 27, 6, 4429, 103, 15, 72, 318, 3, 114, 397, 43, 2, 507, 203, 4, 4669, 108, 8195, 115, 8, 9, 53, 25, 1080, 2, 202, 1362, 1020, 52, 735, 127, 190, 1817, 178, 54, 152, 151, 72, 174, 3468, 25, 89, 2, 507, 203, 13, 59, 5, 11, 768, 1666, 4, 81, 1781, 41, 9, 5, 11, 142, 150, 20, 165, 471, 428, 99, 7, 24, 47, 36, 18, 6, 177, 52, 101, 70, 38, 128, 12, 11, 629, 2, 1583, 4, 723, 2862, 89, 374, 7, 683, 15, 270, 147, 16, 1626, 21, 23, 677, 6, 495, 240, 653, 42, 2604, 4, 101, 723, 2862, 8, 9, 35, 12, 86, 2, 827, 3, 2796, 17, 3, 2305, 12, 508, 3285, 187, 3192, 1975, 1167, 14, 827, 380, 4, 18, 134, 4913, 160, 3598, 13, 138, 35, 17, 6, 495, 1312, 130, 5481, 90, 17, 1669, 878, 17, 1446, 67, 12, 827, 260, 4, 206, 14, 187, 1975, 12, 51, 1252, 29, 686, 150, 152, 12, 11, 770, 20, 3, 1146, 15, 10, 385, 6, 1151, 12, 11, 686, 150, 100, 12, 11, 770, 20, 3, 139, 27, 241, 4, 129, 15, 38, 128, 11, 735, 9808, 13, 580, 393, 72, 35, 252, 843, 50, 143, 42, 14, 5, 8, 106, 343, 78, 4, 96, 272, 538, 1327, 159, 1004, 659, 12, 11, 219, 7, 6, 115, 3285, 286, 12, 147, 6, 429, 1452, 28, 72, 36, 1006, 21, 5, 400, 569, 68, 15, 25, 111, 3, 43, 16, 538, 1327, 628, 167, 11, 830, 2, 147, 6, 538, 1930, 56, 280, 99, 7, 3, 5, 15, 11, 5202, 193, 18, 7158, 42, 1733, 302, 8, 1004, 4, 2241, 252, 63, 3, 99, 346, 67, 3, 5, 11, 8, 3, 911, 502, 12, 11, 89, 249, 2541, 15, 40, 4, 47, 75, 17, 812, 7, 24, 26, 17, 3, 1269, 19, 18, 349, 511, 15, 10, 4116, 9, 5, 1315, 8, 2241, 538, 1327, 4, 709, 1690, 8, 234, 2, 18, 46, 514, 5, 19, 10, 89, 2, 18, 109, 89, 6, 1403, 3022, 497, 544, 576, 5194, 255, 1042, 2920, 460, 1514, 11207, 16, 114, 460, 1, 311, 8147, 711, 460, 9870, 2181, 25, 10, 507, 13, 77, 43, 93, 1195, 2, 31, 91, 94, 709, 1690, 8, 9, 35, 19, 18, 90, 1066, 19, 141, 15, 9, 5, 30, 74, 103, 22, 6, 419, 2586, 2, 21, 650, 8, 234, 2, 1787, 2, 21, 702, 67, 43, 2, 1353, 14, 1450, 42, 44, 555, 386, 152, 12, 18, 709, 2113, 4, 753, 280, 24191, 65, 247, 8, 231, 574, 841, 17, 143, 786, 12, 30, 40, 2, 22, 46, 514, 22, 709, 4, 38, 3, 256, 64, 329, 211, 56, 145, 115, 2031, 5, 8, 41, 285, 773] type=list,
'seq_len': 471 type=int,
'target': 0 type=int},
{'text': ['i', 'teach', 'pre', 'k', 'and', 'we', 'have', 'an', 'incubator', 'in', 'our', 'classroom', 'we', 'love', 'to', 'see', 'the', 'chicks', 'hatch', 'and', 'can', 'hardly', 'wait', 'until', 'they', 'do', 'but', 'some', 'classrooms', 'do', 'not', 'get', 'to', 'enjoy', 'this', 'so', 'we', 'want', 'to', 'get', 'a', 'chicken', 'hutch', 'to', 'put', 'in', 'our', 'main', 'hallway', 'for', 'everyone', 'in', 'the', 'school', 'to', 'see', 'n', 'ni', 'teach', 'at', 'an', 'early', 'childhood', 'school', 'with', 'about', '500', 'children', 'in', 'the', 'grades', 'ppcd', 'pre', 'k', 'and', 'kindergarten', 'this', 'age', 'loves', 'to', 'learn', 'by', 'seeing', 'and', 'touching', 'we', 'love', 'to', 'hatch', 'eggs', 'and', 'raise', 'butterflies', 'we', 'have', 'a', 'large', 'habitat', 'for', 'our', 'butterflies', 'to', 'put', 'in', 'the', 'main', 'hallway', 'of', 'our', 'school', 'but', 'we', 'do', 'not', 'have', 'a', 'way', 'to', 'let', 'others', 'see', 'our', 'baby', 'chicks', 'that', 'hatch', 'unless', 'they', 'come', 'to', 'our', 'room', 'and', 'see', 'them', 'in', 'a', 'box', 'we', 'have', 'n', 'nwe', 'want', 'to', 'be', 'able', 'to', 'show', 'all', 'of', 'our', 'school', 'the', 'young', 'chicks', 'that', 'hatch', 'also', 'the', 'children', 'will', 'learn', 'responsibility', 'in', 'caring', 'for', 'them', 'by', 'feeding', 'watering', 'and', 'cleaning', 'them', 'daily', 'as', 'a', 'teacher', 'i', 'have', 'had', 'chicks', 'hatch', 'in', 'my', 'classroom', 'and', 'we', 'just', 'put', 'them', 'in', 'a', 'box', 'in', 'our', 'room', 'with', 'a', 'lamp', 'other', 'classrooms', 'that', 'do', 'not', 'hatch', 'eggs', 'have', 'to', 'come', 'tour', 'our', 'classroom', 'to', 'see', 'the', 'chicks', 'or', 'some', 'never', 'come', 'at', 'all', 'n', 'nthe', 'donations', 'will', 'help', 'us', 'purchase', 'a', 'chicken', 'hutch', 'that', 'we', 'can', 'put', 'in', 'the', 'main', 'hallway', 'of', 'our', 'school', 'for', 'other', 'classes', 'to', 'be', 'able', 'to', 'see', 'the', 'chicks', 'that', 'hatch', 'and', 'or', 'watch', 'them', 'grow', 'before', 'they', 'have', 'to', 'go', 'back', 'to', 'the', 'farm', 'neven', 'after', 'many', 'years', 'of', 'teaching', 'the', 'one', 'thing', 'that', 'former', 'students', 'of', 'mine', 'remember', 'the', 'most', 'is', 'when', 'we', 'hatched', 'chickens', 'or', 'raised', 'the', 'butterflies', 'from', 'eggs', 'we', 'want', 'the', 'rest', 'of', 'our', 'school', 'to', 'enjoy', 'this', 'and', 'want', 'it', 'to', 'be', 'very', 'memorable', 'for', 'all', 'of', 'them'] type=list,
'class': 0.0 type=float,
'words': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 19, 94, 645, 472, 4, 25, 18, 44, 4339, 8, 21, 35, 25, 61, 2, 111, 3, 4411, 4440, 4, 30, 3619, 838, 1006, 12, 47, 54, 108, 512, 47, 36, 86, 2, 200, 26, 50, 25, 69, 2, 86, 6, 4503, 20001, 2, 330, 8, 21, 1200, 3981, 16, 537, 8, 3, 23, 2, 111, 13, 77, 94, 38, 44, 555, 1396, 23, 20, 57, 2279, 62, 8, 3, 502, 17023, 645, 472, 4, 185, 26, 386, 2200, 2, 40, 56, 733, 4, 2999, 25, 61, 2, 4440, 2910, 4, 1505, 2656, 25, 18, 6, 323, 3075, 16, 21, 2656, 2, 330, 8, 3, 1200, 3981, 7, 21, 23, 54, 25, 47, 36, 18, 6, 107, 2, 513, 270, 111, 21, 3169, 4411, 15, 4440, 2554, 12, 79, 2, 21, 290, 4, 111, 24, 8, 6, 1144, 25, 18, 13, 268, 69, 2, 22, 49, 2, 246, 41, 7, 21, 23, 3, 271, 4411, 15, 4440, 66, 3, 62, 10, 40, 945, 8, 1086, 16, 24, 56, 5254, 8768, 4, 3405, 24, 204, 29, 6, 153, 19, 18, 218, 4411, 4440, 8, 9, 35, 4, 25, 127, 330, 24, 8, 6, 1144, 8, 21, 290, 20, 6, 7102, 101, 512, 15, 47, 36, 4440, 2910, 18, 2, 79, 4432, 21, 35, 2, 111, 3, 4411, 52, 108, 289, 79, 38, 41, 13, 138, 595, 10, 31, 116, 531, 6, 4503, 20001, 15, 25, 30, 330, 8, 3, 1200, 3981, 7, 21, 23, 16, 101, 282, 2, 22, 49, 2, 111, 3, 4411, 15, 4440, 4, 52, 797, 24, 371, 431, 12, 18, 2, 207, 429, 2, 3, 2508, 3896, 313, 45, 164, 7, 213, 3, 74, 610, 15, 3184, 5, 7, 2288, 201, 3, 99, 17, 67, 25, 9999, 6015, 52, 2130, 3, 2656, 42, 2910, 25, 69, 3, 726, 7, 21, 23, 2, 200, 26, 4, 69, 28, 2, 22, 81, 1904, 16, 41, 7, 24] type=list,
'seq_len': 318 type=int,
'target': 0 type=int}
...
{'text': ['i', 'teach', 'high', 'school', 'english', 'for', 'college', 'prep', '12th', 'graders', 'and', 'some', 'english', 'second', 'language', 'esl', 'students', 'many', 'of', 'the', 'students', 'in', 'my', 'classes', 'have', 'not', 'developed', 'sufficient', 'academic', 'skills', 'to', 'be', 'successful', 'in', 'school', 'in', 'addition', 'many', 'of', 'my', 'students', 'believe', 'they', 'are', 'not', 'meant', 'to', 'go', 'to', 'college', 'n', 'nin', 'order', 'to', 'convince', 'them', 'that', 'they', 'belong', 'in', 'college', 'and', 'for', 'them', 'to', 'truly', 'be', 'ready', 'to', 'go', 'my', 'students', 'must', 'have', 'access', 'to', 'technology', 'which', 'will', 'increase', 'their', 'academic', 'skills', 'although', 'it', 'is', 'required', 'that', 'i', 'teach', 'academic', 'essays', 'and', 'speeches', 'to', 'my', 'students', 'my', 'school', 'is', 'financially', 'unable', 'to', 'provide', 'sufficient', 'access', 'to', 'technology', 'especially', 'when', 'writing', 'research', 'papers', 'and', 'other', 'academic', 'assignments', 'for', 'all', 'my', 'students', 'the', 'technology', 'that', 'my', 'students', 'are', 'lacking', 'because', 'their', 'families', 'do', 'not', 'have', 'the', 'resources', 'to', 'contribute', 'to', 'their', 'children', 's', 'education', 'too', 'often', 'prevents', 'them', 'meeting', 'the', 'standards', 'required', 'to', 'pass', 'my', 'courses', 'when', 'students', 'do', 'not', 'have', 'access', 'to', 'the', 'appropriate', 'technology', 'it', 'is', 'difficult', 'to', 'effectively', 'teach', 'writing', 'and', 'speech', 'standards', 'at', 'a', 'college', 'prep', 'level', 'which', 'will', 'lead', 'to', 'a', 'college', 'prep', 'diploma', 'and', 'transitioning', 'to', 'college', 'however', 'i', 'am', 'a', 'teacher', 'focused', 'on', 'reaching', 'students', 'where', 'they', 'are', 'which', 'right', 'now', 'is', 'in', 'need', 'of', 'a', 'hp', 'computer', 'n', 'nif', 'a', 'hp', 'computer', 'is', 'purchased', 'i', 'will', 'be', 'able', 'to', 'successfully', 'guide', 'my', 'student', 'across', 'the', 'technological', 'divide', 'within', 'the', 'english', 'classroom', 'when', 'this', 'is', 'accomplished', 'my', 'students', 'can', 'improve', 'their', 'academic', 'writing', 'and', 'speech', 'skills', 'and', 'successfully', 'complete', 'the', 'english', 'language', 'arts', 'standards', 'for', '12th', 'grade', 'after', 'they', 'do', 'this', 'i', 'am', 'certain', 'that', 'my', 'students', 'will', 'more', 'likely', 'be', 'interested', 'in', 'attending', 'and', 'be', 'prepared', 'for', 'college', 'because', 'they', 'experienced', 'legitimate', 'academic', 'success', 'via', 'quality', 'instruction', 'and', 'tech', 'support', 'when', 'they', 'get', 'to', 'college', 'they', 'are', 'more', 'likely', 'to', 'experience', 'continued', 'success', 'because', 'they', 'were', 'supported', 'with', 'access', 'to', 'hp', 'technology', 'that', 'was', 'part', 'of', 'improving', 'their', 'academic', 'performance', 'n', 'ni', 'know', 'times', 'are', 'tough', 'for', 'everyone', 'right', 'now', 'but', 'the', 'hp', 'technology', 'can', 'make', 'a', 'huge', 'difference', 'in', 'students', 'lives', 'as', 'they', 'direct', 'their', 'future', 'aspirations', 'toward', 'college', 'you', 'can', 'make', 'it', 'possible', 'for', 'students', 'to', 'more', 'powerfully', 'interact', 'with', 'the', 'world', 'we', 'live', 'in', 'thus', 'making', 'it', 'a', 'better', 'place'] type=list,
'class': 0.0 type=float,
'words': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 19, 94, 90, 23, 142, 16, 230, 2189, 2176, 167, 4, 108, 142, 243, 106, 1289, 5, 45, 7, 3, 5, 8, 9, 282, 18, 36, 1676, 2986, 285, 58, 2, 22, 220, 8, 23, 8, 373, 45, 7, 9, 5, 303, 12, 11, 36, 2960, 2, 207, 2, 230, 13, 580, 234, 2, 4874, 24, 15, 12, 3045, 8, 230, 4, 16, 24, 2, 459, 22, 417, 2, 207, 9, 5, 434, 18, 146, 2, 70, 113, 10, 335, 14, 285, 58, 743, 28, 17, 830, 15, 19, 94, 285, 2094, 4, 4934, 2, 9, 5, 9, 23, 17, 1934, 752, 2, 98, 2986, 146, 2, 70, 443, 67, 125, 312, 1217, 4, 101, 285, 780, 16, 41, 9, 5, 3, 70, 15, 9, 5, 11, 1140, 100, 14, 183, 47, 36, 18, 3, 109, 2, 1787, 2, 14, 62, 72, 118, 415, 205, 5220, 24, 1141, 3, 499, 830, 2, 1290, 9, 1899, 67, 5, 47, 36, 18, 146, 2, 3, 612, 70, 28, 17, 273, 2, 1253, 94, 125, 4, 986, 499, 38, 6, 230, 2189, 124, 113, 10, 989, 2, 6, 230, 2189, 3348, 4, 4371, 2, 230, 272, 19, 82, 6, 153, 873, 27, 1779, 5, 130, 12, 11, 113, 318, 237, 17, 8, 43, 7, 6, 1845, 177, 13, 1029, 6, 1845, 177, 17, 1415, 19, 10, 22, 49, 2, 1696, 1353, 9, 88, 765, 3, 951, 3367, 493, 3, 142, 35, 67, 26, 17, 2550, 9, 5, 30, 294, 14, 285, 125, 4, 986, 58, 4, 1696, 396, 3, 142, 106, 343, 499, 16, 2176, 68, 313, 12, 47, 26, 19, 82, 1301, 15, 9, 5, 10, 46, 1288, 22, 608, 8, 1294, 4, 22, 872, 16, 230, 100, 12, 1365, 10286, 285, 251, 1792, 421, 302, 4, 1813, 173, 67, 12, 86, 2, 230, 12, 11, 46, 1288, 2, 137, 2729, 251, 100, 12, 245, 2688, 20, 146, 2, 1845, 70, 15, 182, 219, 7, 1291, 14, 285, 1153, 13, 77, 141, 375, 11, 1432, 16, 537, 318, 237, 54, 3, 1845, 70, 30, 63, 6, 602, 316, 8, 5, 176, 29, 12, 1733, 14, 187, 3931, 1280, 230, 37, 30, 63, 28, 314, 16, 5, 2, 46, 10306, 746, 20, 3, 84, 25, 225, 8, 1234, 301, 28, 6, 156, 212] type=list,
'seq_len': 380 type=int,
'target': 0 type=int},
{'text': ['my', 'classroom', 'is', 'the', 'new', 'kindergarten', 'classroom', 'that', 'started', 'in', 'october', 'we', 'are', 'working', 'hard', 'implementing', 'lucy', 'calkin', 's', 'reading', 'and', 'writing', 'workshop', 'we', 'need', 'some', 'supplies', 'to', 'help', 'us', 'make', 'these', 'parts', 'of', 'the', 'day', 'even', 'better', 'n', 'ni', 'have', 'a', 'classroom', 'of', '22', 'students', 'i', 'have', 'a', 'great', 'bunch', 'of', 'kiddos', 'that', 'are', 'very', 'interested', 'in', 'reading', 'and', 'writing', 'our', 'school', 'uses', 'reading', 'and', 'writing', 'workshop', 'as', 'a', 'very', 'important', 'part', 'of', 'the', 'kindergarten', 'curriculum', 'it', 'is', 'important', 'that', 'my', 'students', 'get', 'hands', 'on', 'experience', 'with', 'real', 'books', 'and', 'are', 'able', 'to', 'listen', 'to', 'stories', 'as', 'often', 'as', 'possible', 'many', 'of', 'my', 'students', 'speak', 'more', 'than', 'one', 'language', 'so', 'it', 'is', 'important', 'that', 'they', 'hear', 'the', 'english', 'writing', 'in', 'stories', 'to', 'help', 'in', 'their', 'own', 'reading', 'and', 'writing', 'n', 'nmany', 'of', 'my', 'students', 'are', 'reading', 'at', 'levels', 'that', 'are', 'higher', 'than', 'i', 'expected', 'i', 'am', 'very', 'excited', 'for', 'them', 'but', 'do', 'not', 'have', 'the', 'books', 'in', 'our', 'classroom', 'that', 'are', 'appropriate', 'to', 'their', 'level', 'to', 'read', 'during', 'our', 'reading', 'workshop', 'the', 'materials', 'that', 'i', 'am', 'requesting', 'are', 'going', 'to', 'help', 'challenge', 'my', 'kiddos', 'to', 'become', 'better', 'readers', 'than', 'they', 'already', 'are', 'my', 'students', 'need', '3', 'leveled', 'book', 'sets', 'and', 'a', 'storage', 'bin', 'for', 'their', 'individual', 'book', 'bags', 'n', 'ndonating', 'to', 'this', 'project', 'will', 'make', 'a', 'difference', 'in', 'my', 'kindergarten', 'students', 'lives', 'we', 'are', 'striving', 'to', 'become', 'great', 'readers', 'and', 'with', 'the', 'addition', 'of', 'books', 'at', 'appropriate', 'levels', 'my', 'students', 'will', 'be', 'able', 'to', 'excel', 'in', 'reading'] type=list,
'class': 0.0 type=float,
'words': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 35, 17, 3, 65, 185, 35, 15, 975, 8, 6320, 25, 11, 175, 155, 3003, 10611, 26242, 72, 32, 4, 125, 1119, 25, 43, 108, 162, 2, 31, 116, 63, 34, 1037, 7, 3, 103, 172, 156, 13, 77, 18, 6, 35, 7, 1421, 5, 19, 18, 6, 135, 1348, 7, 2547, 15, 11, 81, 608, 8, 32, 4, 125, 21, 23, 1503, 32, 4, 125, 1119, 29, 6, 81, 119, 219, 7, 3, 185, 286, 28, 17, 119, 15, 9, 5, 86, 139, 27, 137, 20, 291, 39, 4, 11, 49, 2, 361, 2, 214, 29, 205, 29, 314, 45, 7, 9, 5, 547, 46, 171, 74, 106, 50, 28, 17, 119, 15, 12, 377, 3, 142, 125, 8, 214, 2, 31, 8, 14, 122, 32, 4, 125, 13, 889, 7, 9, 5, 11, 32, 38, 284, 15, 11, 611, 171, 19, 1236, 19, 82, 81, 195, 16, 24, 54, 47, 36, 18, 3, 39, 8, 21, 35, 15, 11, 612, 2, 14, 124, 2, 48, 180, 21, 32, 1119, 3, 80, 15, 19, 82, 184, 11, 407, 2, 31, 336, 9, 2547, 2, 134, 156, 136, 171, 12, 486, 11, 9, 5, 43, 468, 739, 92, 632, 4, 6, 994, 3282, 16, 14, 480, 92, 1454, 13, 3550, 2, 26, 83, 10, 63, 6, 316, 8, 9, 185, 5, 176, 25, 11, 2312, 2, 134, 135, 136, 4, 20, 3, 373, 7, 39, 38, 612, 284, 9, 5, 10, 22, 49, 2, 1214, 8, 32] type=list,
'seq_len': 248 type=int,
'target': 0 type=int},
{'text': ['one', 'of', 'the', 'biggest', 'challenges', 'in', 'my', 'classroom', 'is', 'making', 'our', 'lessons', 'interactive', 'for', 'my', 'students', 'i', 'would', 'love', 'to', 'have', 'access', 'to', 'the', 'now', 'interactive', 'board', 'so', 'that', 'i', 'can', 'help', 'my', 'students', 'participate', 'in', 'the', 'lessons', 'and', 'have', 'them', 'become', 'more', 'interactive', 'in', 'our', 'classroom', 'n', 'nmy', 'students', 'are', 'such', 'wonderful', 'happy', 'friendly', 'children', 'who', 'truly', 'want', 'to', 'learn', 'the', 'problem', 'is', 'that', 'they', 'have', 'some', 'cognitive', 'issues', 'that', 'prevent', 'them', 'from', 'learning', 'in', 'the', 'same', 'ways', 'as', 'other', 'students', 'they', 'need', 'to', 'be', 'able', 'to', 'do', 'and', 'have', 'more', 'hands', 'on', 'learning', 'activities', 'so', 'that', 'they', 'can', 'truly', 'begin', 'to', 'grasp', 'the', 'concepts', 'being', 'taught', 'to', 'them', 'all', 'my', 'students', 'want', 'to', 'do', 'their', 'best', 'and', 'try', 'really', 'hard', 'at', 'every', 'lesson', 'n', 'nif', 'we', 'had', 'the', 'now', 'interactive', 'board', 'in', 'our', 'classroom', 'then', 'my', 'students', 'would', 'be', 'able', 'to', 'manipulate', 'items', 'on', 'the', 'screen', 'instead', 'of', 'just', 'watching', 'the', 'teacher', 'move', 'things', 'from', 'the', 'computer', 'this', 'would', 'bring', 'the', 'lessons', 'to', 'life', 'for', 'these', 'students', 'and', 'would', 'help', 'make', 'concepts', 'really', 'make', 'sense', 'n', 'nthe', 'ipad', 'would', 'be', 'a', 'great', 'tool', 'to', 'help', 'students', 'to', 'be', 'able', 'to', 'interact', 'using', 'apps', 'and', 'they', 'could', 'see', 'their', 'work', 'being', 'projected', 'on', 'the', 'board', 'with', 'the', 'now', 'interactive', 'board', 'n', 'nhaving', 'these', 'two', 'tools', 'together', 'in', 'my', 'classroom', 'would', 'truly', 'bring', 'instruction', 'to', 'life', 'and', 'have', 'students', 'really', 'wanting', 'to', 'work', 'with', 'our', 'lessons', 'n', 'ndonations', 'to', 'this', 'classroom', 'project', 'will', 'help', 'many', 'students', 'to', 'truly', 'be', 'able', 'to', 'understand', 'and', 'make', 'sense', 'of', 'the', 'many', 'complex', 'lessons', 'and', 'topics', 'being', 'taught', 'in', 'middle', 'school', 'these', 'items', 'will', 'help', 'students', 'to', 'be', 'able', 'to', 'interact', 'with', 'the', 'content', 'and', 'will', 'help', 'them', 'to', 'learn', 'how', 'to', 'work', 'together', 'to', 'make', 'something', 'work', 'on', 'the', 'board', 'being', 'able', 'to', 'truly', 'understand', 'these', 'concepts', 'and', 'to', 'actually', 'see', 'these', 'students', 'when', 'they', 'finally', 'get', 'the', 'topic', 'will', 'be', 'priceless'] type=list,
'class': 0.0 type=float,
'words': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 74, 7, 3, 1894, 348, 8, 9, 35, 17, 301, 21, 244, 352, 16, 9, 5, 19, 51, 61, 2, 18, 146, 2, 3, 237, 352, 403, 50, 15, 19, 30, 31, 9, 5, 466, 8, 3, 244, 4, 18, 24, 134, 46, 352, 8, 21, 35, 13, 59, 5, 11, 169, 242, 1118, 1362, 62, 73, 459, 69, 2, 40, 3, 390, 17, 15, 12, 18, 108, 1835, 652, 15, 1859, 24, 42, 33, 8, 3, 256, 265, 29, 101, 5, 12, 43, 2, 22, 49, 2, 47, 4, 18, 46, 139, 27, 33, 129, 50, 15, 12, 30, 459, 509, 2, 1391, 3, 247, 145, 500, 2, 24, 41, 9, 5, 69, 2, 47, 14, 179, 4, 409, 267, 155, 38, 120, 520, 13, 1029, 25, 218, 3, 237, 352, 403, 8, 21, 35, 280, 9, 5, 51, 22, 49, 2, 1384, 391, 27, 3, 798, 634, 7, 127, 1174, 3, 153, 507, 197, 42, 3, 177, 26, 51, 287, 3, 244, 2, 112, 16, 34, 5, 4, 51, 31, 63, 247, 267, 63, 572, 13, 138, 540, 51, 22, 6, 135, 405, 2, 31, 5, 2, 22, 49, 2, 746, 140, 820, 4, 12, 193, 111, 14, 60, 145, 2230, 27, 3, 403, 20, 3, 237, 352, 403, 13, 725, 34, 254, 236, 266, 8, 9, 35, 51, 459, 287, 302, 2, 112, 4, 18, 5, 267, 1582, 2, 60, 20, 21, 244, 13, 1921, 2, 26, 35, 83, 10, 31, 45, 5, 2, 459, 22, 49, 2, 233, 4, 63, 572, 7, 3, 45, 1585, 244, 4, 591, 145, 500, 8, 295, 23, 34, 391, 10, 31, 5, 2, 22, 49, 2, 746, 20, 3, 578, 4, 10, 31, 24, 2, 40, 76, 2, 60, 266, 2, 63, 274, 60, 27, 3, 403, 145, 49, 2, 459, 233, 34, 247, 4, 2, 751, 111, 34, 5, 67, 12, 970, 86, 3, 1105, 10, 22, 2413] type=list,
'seq_len': 319 type=int,
'target': 0 type=int},
{'text': ['my', 'students', 'have', 'been', 'eagerly', 'typing', 'their', 'essays', 'and', 'stories', 'n', 'nmy', 'developing', 'writers', 'are', 'practicing', 'their', 'writing', 'skills', 'this', 'month', 'they', 'prepared', 'argumentative', 'papers', 'defending', 'either', 'athens', 'or', 'sparta', 'their', 'follow', 'up', 'project', 'was', 'to', 'create', 'their', 'own', 'hero', 'myth', 'they', 'eagerly', 'accepted', 'the', 'challenge', 'and', 'surpassed', 'my', 'expectations', 'n', 'nfortunately', 'with', 'our', 'new', 'printer', 'students', 'can', 'now', 'print', 'in', 'the', 'classroom', 'this', 'has', 'been', 'a', 'great', 'convenience', 'for', 'my', 'students', 'who', 'do', 'not', 'otherwise', 'have', 'access', 'to', 'printers', 'these', '4', 'ink', 'cartridges', 'will', 'insure', 'that', 'my', 'students', 'can', 'continue', 'printing', 'their', 'work', 'in', 'the', 'classroom', 'n', 'nhaving', 'the', 'ability', 'to', 'print', 'their', 'work', 'in', 'the', 'classroom', 'has', 'motivated', 'students', 'they', 'are', 'able', 'to', 'see', 'their', 'work', 'and', 'get', 'feedback', 'quickly', 'students', 'are', 'motivated', 'because', 'they', 'can', 'submit', 'their', 'work', 'in', 'a', 'timely', 'fashion', 'without', 'have', 'to', 'worry', 'about', 'printing'] type=list,
'class': 0.0 type=float,
'words': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 5, 18, 157, 3440, 2671, 14, 2094, 4, 214, 13, 59, 791, 703, 11, 1169, 14, 125, 58, 26, 1533, 12, 872, 14428, 1217, 13445, 1057, 21117, 52, 1, 14, 814, 102, 83, 182, 2, 133, 14, 122, 4294, 9177, 12, 3440, 3024, 3, 336, 4, 10242, 9, 1066, 13, 11088, 20, 21, 65, 560, 5, 30, 237, 551, 8, 3, 35, 26, 104, 157, 6, 135, 7238, 16, 9, 5, 73, 47, 36, 1099, 18, 146, 2, 2825, 34, 573, 1007, 2593, 10, 4711, 15, 9, 5, 30, 317, 1901, 14, 60, 8, 3, 35, 13, 725, 3, 305, 2, 551, 14, 60, 8, 3, 35, 104, 436, 5, 12, 11, 49, 2, 111, 14, 60, 4, 86, 1334, 772, 5, 11, 436, 100, 12, 30, 4340, 14, 60, 8, 6, 3966, 3052, 199, 18, 2, 1784, 57, 1901] type=list,
'seq_len': 141 type=int,
'target': 0 type=int},
{'text': ['we', 'have', 'done', 'a', 'lot', 'around', 'design', 'thinking', 'this', 'year', 'a', '3d', 'printer', 'in', 'our', 'design', 'lab', 'would', 'add', 'to', 'the', 'experience', 'greatly', 'n', 'nmy', 'students', 'are', 'mainly', 'high', 'school', 'freshmen', 'although', 'the', 'design', 'lab', 'will', 'be', 'used', 'by', 'students', 'in', 'grades', '9', '12', 'in', 'a', 'team', 'taught', 'humanities', 'course', 'our', 'school', 'is', 'progressive', 'and', 'working', 'towards', 'more', 'hands', 'on', 'real', 'world', 'applicable', 'work', 'we', 'have', 'completed', 'a', 'large', 'construction', 'project', 'and', 'have', 'a', 'space', 'to', 'build', 'a', 'design', 'lab', 'we', 'have', 'begged', 'borrowed', 'and', 'stolen', 'some', 'pieces', 'we', 'need', 'the', 'more', 'advanced', 'technology', 'that', 'this', '3d', 'printer', 'represents', 'n', 'ni', 'am', 'requesting', '1', '3d', 'printer', 'as', 'part', 'of', 'our', 'design', 'lab', 'throughout', 'the', 'design', 'process', 'we', 'are', 'working', 'on', 'creating', 'prototypes', 'for', 'students', 'designs', 'this', 'printer', 'will', 'help', 'our', 'students', 'to', 'see', 'their', 'ideas', 'in', '3d', 'earlier', 'this', 'year', 'students', 'worked', 'with', 'cardboard', 'to', 'create', 'some', 'pretty', 'cool', 'products', 'this', 'printer', 'will', 'allow', 'us', 'to', 'take', 'our', 'learning', 'and', 'production', 'to', 'the', 'next', 'level', 'n', 'nthis', 'will', 'make', 'a', 'difference', 'as', 'school', 'budgets', 'make', 'it', 'hard', 'for', 'us', 'to', 'keep', 'ahead', 'of', 'the', 'curve', 'with', 'our', 'students', 'technology', 'changes', 'quickly', 'and', 'we', 'need', 'to', 'prepare', 'our', 'students', 'for', 'what', 'they', 'will', 'encounter', 'as', 'we', 'know', 'many', 'of', 'the', 'jobs', 'our', 'students', 'will', 'have', 'haven', 't', 'even', 'been', 'created', 'yet', 'this', 'will', 'help', 'us', 'to', 'stay', 'one', 'step', 'ahead'] type=list,
'class': 0.0 type=float,
'words': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 25, 18, 763, 6, 368, 203, 1041, 489, 26, 71, 6, 3221, 560, 8, 21, 1041, 473, 51, 588, 2, 3, 137, 505, 13, 59, 5, 11, 2551, 90, 23, 3294, 743, 3, 1041, 473, 10, 22, 161, 56, 5, 8, 502, 1349, 947, 8, 6, 690, 500, 5256, 774, 21, 23, 17, 6048, 4, 175, 929, 46, 139, 27, 291, 84, 4732, 60, 25, 18, 1698, 6, 323, 1610, 83, 4, 18, 6, 447, 2, 250, 6, 1041, 473, 25, 18, 7517, 3578, 4, 5652, 108, 969, 25, 43, 3, 46, 905, 70, 15, 26, 3221, 560, 4113, 13, 77, 82, 184, 309, 3221, 560, 29, 219, 7, 21, 1041, 473, 367, 3, 1041, 413, 25, 11, 175, 27, 453, 18361, 16, 5, 3250, 26, 560, 10, 31, 21, 5, 2, 111, 14, 484, 8, 3221, 3058, 26, 71, 5, 1402, 20, 4255, 2, 133, 108, 2381, 1514, 1777, 26, 560, 10, 97, 116, 2, 147, 21, 33, 4, 2242, 2, 3, 450, 124, 13, 217, 10, 63, 6, 316, 29, 23, 1776, 63, 28, 155, 16, 116, 2, 198, 2008, 7, 3, 6970, 20, 21, 5, 70, 1256, 772, 4, 25, 43, 2, 526, 21, 5, 16, 75, 12, 10, 2078, 29, 25, 141, 45, 7, 3, 1081, 21, 5, 10, 18, 1312, 85, 172, 157, 847, 452, 26, 10, 31, 116, 2, 621, 74, 691, 2008] type=list,
'seq_len': 229 type=int,
'target': 0 type=int})
[34mAuto commit by fitlog[0m
(vocab_size,class_num,seq_len):(27658,2,1070)
test_size:43444
Loading the model ../../data/models/LSTMText/default/best_LSTMText_accuracy_2019-06-14-04-01-48
LSTMText(
  (embeds): Embedding(
    27658, 128
    (dropout): Dropout(p=0.0)
  )
  (lstm): LSTM(128, 128, num_layers=2, dropout=0.2, bidirectional=True)
  (fc): Linear(in_features=256, out_features=2, bias=True)
  (dropout): Dropout(p=0.2)
)
[34mAuto commit by fitlog[0m
Checking the data files...
(vocab_size,class_num,seq_len):(27658,2,1070)
test_size:43444
Loading the model ../../data/models/LSTMText/default/best_LSTMText_accuracy_2019-06-14-04-01-48
LSTMText(
  (embeds): Embedding(
    27658, 128
    (dropout): Dropout(p=0.0)
  )
  (lstm): LSTM(128, 128, num_layers=2, dropout=0.2, bidirectional=True)
  (fc): Linear(in_features=256, out_features=2, bias=True)
  (dropout): Dropout(p=0.2)
)
[34mAuto commit by fitlog[0m
Checking the data files...
(vocab_size,class_num,seq_len):(27658,2,1070)
test_size:43444
Data type:<class 'fastNLP.core.dataset.DataSet'>
Loading the model ../../data/models/LSTMText/default/best_LSTMText_accuracy_2019-06-14-04-01-48
LSTMText(
  (embeds): Embedding(
    27658, 128
    (dropout): Dropout(p=0.0)
  )
  (lstm): LSTM(128, 128, num_layers=2, dropout=0.2, bidirectional=True)
  (fc): Linear(in_features=256, out_features=2, bias=True)
  (dropout): Dropout(p=0.2)
)
[34mAuto commit by fitlog[0m
Checking the data files...
(vocab_size,class_num,seq_len):(27658,2,1070)
test_size:43444
Data type:<class 'fastNLP.core.dataset.DataSet'>
Loading the model ../../data/models/LSTMText/default/best_LSTMText_accuracy_2019-06-14-04-01-48
LSTMText(
  (embeds): Embedding(
    27658, 128
    (dropout): Dropout(p=0.0)
  )
  (lstm): LSTM(128, 128, num_layers=2, dropout=0.2, bidirectional=True)
  (fc): Linear(in_features=256, out_features=2, bias=True)
  (dropout): Dropout(p=0.2)
)
[34mAuto commit by fitlog[0m
Checking the data files...
(vocab_size,class_num,seq_len):(27658,2,1070)
test_size:43444
Data type:<class 'fastNLP.core.dataset.DataSet'>
Loading the model ../../data/models/LSTMText/default/best_LSTMText_accuracy_2019-06-14-04-01-48
LSTMText(
  (embeds): Embedding(
    27658, 128
    (dropout): Dropout(p=0.0)
  )
  (lstm): LSTM(128, 128, num_layers=2, dropout=0.2, bidirectional=True)
  (fc): Linear(in_features=256, out_features=2, bias=True)
  (dropout): Dropout(p=0.2)
)
[34mAuto commit by fitlog[0m
Checking the data files...
(vocab_size,class_num,seq_len):(27658,2,1070)
test_size:43444
Data type:<class 'fastNLP.core.dataset.DataSet'>
Loading the model ../../data/models/LSTMText/default/best_LSTMText_accuracy_2019-06-14-04-01-48
LSTMText(
  (embeds): Embedding(
    27658, 128
    (dropout): Dropout(p=0.0)
  )
  (lstm): LSTM(128, 128, num_layers=2, dropout=0.2, bidirectional=True)
  (fc): Linear(in_features=256, out_features=2, bias=True)
  (dropout): Dropout(p=0.2)
)
[34mAuto commit by fitlog[0m
Checking the data files...
(vocab_size,class_num,seq_len):(27658,2,1070)
test_size:43444
Data type:<class 'fastNLP.core.dataset.DataSet'>
Loading the model ../../data/models/LSTMText/default/best_LSTMText_accuracy_2019-06-14-04-01-48
LSTMText(
  (embeds): Embedding(
    27658, 128
    (dropout): Dropout(p=0.0)
  )
  (lstm): LSTM(128, 128, num_layers=2, dropout=0.2, bidirectional=True)
  (fc): Linear(in_features=256, out_features=2, bias=True)
  (dropout): Dropout(p=0.2)
)
({'words': tensor([[   0,    0,    0,  ...,    8,   43,   13],
        [   0,    0,    0,  ..., 1310,    8,  112],
        [   0,    0,    0,  ...,  168,    4,  345],
        ...,
        [   0,    0,    0,  ...,    4, 5170,  241],
        [   0,    0,    0,  ..., 1576,   14,  455],
        [   0,    0,    0,  ...,   29,    6,  433]]), 'seq_len': tensor([226, 352, 288, 281, 453, 385, 285, 491, 207, 310, 223, 311, 269, 333,
        272, 330, 336, 317, 247, 323, 361, 274, 359, 259, 297, 377, 303, 170,
        415, 187, 286, 324, 282, 392, 231, 159, 478, 319, 236, 343, 266, 332,
        366, 350, 213, 346, 167, 347, 304, 193, 350, 395, 398, 341, 242, 246,
        148, 326, 381, 170, 330, 379, 210, 449, 383, 258, 196, 243, 257, 465,
        368, 223, 235, 327, 294, 282, 195, 203, 416, 246, 289, 194, 255, 444,
        221, 249, 238, 328, 301, 388, 201, 247, 174, 280, 340, 172, 202, 444,
        248, 327, 243, 219, 225, 192, 421, 489, 346, 369, 393, 305, 290, 267,
        255, 248, 208, 355, 289, 424, 279, 204, 441, 238, 262, 408, 321, 243,
        425, 274, 362, 299, 268, 307, 379, 359, 463, 284, 261, 324, 400, 280,
        285, 214, 280, 435, 183, 481, 171, 501, 391, 432, 359, 170, 484, 276,
        285, 267, 249, 266, 434, 239, 324, 393, 301, 356, 293, 454, 318, 324,
        197, 351, 240, 173, 283, 259, 220, 299, 197, 190, 286, 242, 219, 473,
        466, 469, 417, 457, 307, 277, 398, 371, 316, 170, 296, 207, 297, 203,
        275, 248, 287, 252, 261, 352, 240, 205, 284, 207, 337, 174, 194, 286,
        190, 224, 443, 422, 255, 401, 238, 327, 278, 376, 442, 385, 414, 312,
        220, 378, 355, 222, 266, 416, 432, 384, 221, 381, 384, 396, 159, 264,
        251, 339, 280, 244, 267, 243, 392, 287, 385, 355, 295, 274, 155, 207,
        228, 166, 455, 449, 260, 199, 223, 284, 482, 328, 265, 261, 372, 313,
        429, 465, 228, 308, 421, 359, 324, 283, 284, 225, 211, 371, 380, 272,
        240, 452, 309, 304, 412, 304, 294, 359, 257, 291, 237, 404, 209, 284,
        240, 195, 389, 446, 196, 362, 356, 268, 390, 185, 330, 310, 246, 294,
        362, 393, 287, 322, 380, 216, 269, 239, 340, 245, 258, 227])}, {'target': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0])})
[34mAuto commit by fitlog[0m
Running with args:Namespace(batch_size=320, cuda=True, data_src='all_data', dropout=0.2, embed_size=128, epochs=10, gpu='0', hidden_size=128, learning_rate=0.001, log_path='./run_records.log', max_seq_len=500, min_count=10, model='LSTMText', model_dir='../../data/models', model_suffix='default', num_layers=2, optim='Adam', patience=10, predict=True, prepare=False, prepare_dir='../../data/prepare/', pretrain=False, pretrain_model='None', reload_model_name='best_LSTMText_accuracy_2019-06-14-04-01-48', result_dir='../../data/results/', train=False, vocab_data='vocab.data', vocab_dir='../../data/vocab', weight_decay=0.0)
Checking the data files...
(vocab_size,class_num,seq_len):(27658,2,1070)
test_size:43444
Data type:<class 'fastNLP.core.dataset.DataSet'>
Loading the model ../../data/models/LSTMText/default/best_LSTMText_accuracy_2019-06-14-04-01-48
LSTMText(
  (embeds): Embedding(
    27658, 128
    (dropout): Dropout(p=0.0)
  )
  (lstm): LSTM(128, 128, num_layers=2, dropout=0.2, bidirectional=True)
  (fc): Linear(in_features=256, out_features=2, bias=True)
  (dropout): Dropout(p=0.2)
)
[34mAuto commit by fitlog[0m
Running with args:Namespace(batch_size=320, cuda=True, data_src='all_data', dropout=0.2, embed_size=128, epochs=10, gpu='0', hidden_size=128, learning_rate=0.001, log_path='./run_records.log', max_seq_len=500, min_count=10, model='LSTMText', model_dir='../../data/models', model_suffix='default', num_layers=2, optim='Adam', patience=10, predict=True, prepare=False, prepare_dir='../../data/prepare/', pretrain=False, pretrain_model='None', reload_model_name='best_LSTMText_accuracy_2019-06-14-04-01-48', result_dir='../../data/results/', train=False, vocab_data='vocab.data', vocab_dir='../../data/vocab', weight_decay=0.0)
Checking the data files...
(vocab_size,class_num,seq_len):(27658,2,1070)
test_size:43444
Data type:<class 'fastNLP.core.dataset.DataSet'>
Loading the model ../../data/models/LSTMText/default/best_LSTMText_accuracy_2019-06-14-04-01-48
LSTMText(
  (embeds): Embedding(
    27658, 128
    (dropout): Dropout(p=0.0)
  )
  (lstm): LSTM(128, 128, num_layers=2, dropout=0.2, bidirectional=True)
  (fc): Linear(in_features=256, out_features=2, bias=True)
  (dropout): Dropout(p=0.2)
)
[34mAuto commit by fitlog[0m
Running with args:Namespace(batch_size=320, cuda=True, data_src='all_data', dropout=0.2, embed_size=128, epochs=10, gpu='0', hidden_size=128, learning_rate=0.001, log_path='./run_records.log', max_seq_len=500, min_count=10, model='LSTMText', model_dir='../../data/models', model_suffix='default', num_layers=2, optim='Adam', patience=10, predict=True, prepare=False, prepare_dir='../../data/prepare/', pretrain=False, pretrain_model='None', reload_model_name='best_LSTMText_accuracy_2019-06-14-04-01-48', result_dir='../../data/results/', train=False, vocab_data='vocab.data', vocab_dir='../../data/vocab', weight_decay=0.0)
Checking the data files...
(vocab_size,class_num,seq_len):(27658,2,1070)
test_size:43444
Data type:<class 'fastNLP.core.dataset.DataSet'>
Loading the model ../../data/models/LSTMText/default/best_LSTMText_accuracy_2019-06-14-04-01-48
LSTMText(
  (embeds): Embedding(
    27658, 128
    (dropout): Dropout(p=0.0)
  )
  (lstm): LSTM(128, 128, num_layers=2, dropout=0.2, bidirectional=True)
  (fc): Linear(in_features=256, out_features=2, bias=True)
  (dropout): Dropout(p=0.2)
)
Predict Done.136 records
[34mAuto commit by fitlog[0m
Running with args:Namespace(batch_size=320, cuda=True, data_src='all_data', dropout=0.2, embed_size=128, epochs=10, gpu='0', hidden_size=128, learning_rate=0.001, log_path='./run_records.log', max_seq_len=500, min_count=10, model='LSTMText', model_dir='../../data/models', model_suffix='default', num_layers=2, optim='Adam', patience=10, predict=True, prepare=False, prepare_dir='../../data/prepare/', pretrain=False, pretrain_model='None', reload_model_name='best_LSTMText_accuracy_2019-06-14-04-01-48', result_dir='../../data/results/', train=False, vocab_data='vocab.data', vocab_dir='../../data/vocab', weight_decay=0.0)
Checking the data files...
(vocab_size,class_num,seq_len):(27658,2,1070)
test_size:43444
Data type:<class 'fastNLP.core.dataset.DataSet'>
Loading the model ../../data/models/LSTMText/default/best_LSTMText_accuracy_2019-06-14-04-01-48
LSTMText(
  (embeds): Embedding(
    27658, 128
    (dropout): Dropout(p=0.0)
  )
  (lstm): LSTM(128, 128, num_layers=2, dropout=0.2, bidirectional=True)
  (fc): Linear(in_features=256, out_features=2, bias=True)
  (dropout): Dropout(p=0.2)
)
Predict Done.136 records
[34mAuto commit by fitlog[0m
Running with args:Namespace(batch_size=320, cuda=True, data_src='all_data', dropout=0.2, embed_size=128, epochs=10, gpu='0', hidden_size=128, learning_rate=0.001, log_path='./run_records.log', max_seq_len=500, min_count=10, model='LSTMText', model_dir='../../data/models', model_suffix='default', num_layers=2, optim='Adam', patience=10, predict=True, prepare=False, prepare_dir='../../data/prepare/', pretrain=False, pretrain_model='None', reload_model_name='best_LSTMText_accuracy_2019-06-14-04-01-48', result_dir='../../data/results/', train=False, vocab_data='vocab.data', vocab_dir='../../data/vocab', weight_decay=0.0)
Checking the data files...
(vocab_size,class_num,seq_len):(27658,2,1070)
test_size:43444
Data type:<class 'fastNLP.core.dataset.DataSet'>
Loading the model ../../data/models/LSTMText/default/best_LSTMText_accuracy_2019-06-14-04-01-48
LSTMText(
  (embeds): Embedding(
    27658, 128
    (dropout): Dropout(p=0.0)
  )
  (lstm): LSTM(128, 128, num_layers=2, dropout=0.2, bidirectional=True)
  (fc): Linear(in_features=256, out_features=2, bias=True)
  (dropout): Dropout(p=0.2)
)
Predict Done.43520 records
[34mAuto commit by fitlog[0m
Running with args:Namespace(batch_size=320, cuda=True, data_src='all_data', dropout=0.2, embed_size=128, epochs=10, gpu='0', hidden_size=128, learning_rate=0.001, log_path='./run_records.log', max_seq_len=500, min_count=10, model='LSTMText', model_dir='../../data/models', model_suffix='default', num_layers=2, optim='Adam', patience=10, predict=True, prepare=False, prepare_dir='../../data/prepare/', pretrain=False, pretrain_model='None', reload_model_name='best_LSTMText_accuracy_2019-06-14-04-01-48', result_dir='../../data/results/', train=False, vocab_data='vocab.data', vocab_dir='../../data/vocab', weight_decay=0.0)
Checking the data files...
(vocab_size,class_num,seq_len):(27658,2,1070)
test_size:43444
Data type:<class 'fastNLP.core.dataset.DataSet'>
Loading the model ../../data/models/LSTMText/default/best_LSTMText_accuracy_2019-06-14-04-01-48
LSTMText(
  (embeds): Embedding(
    27658, 128
    (dropout): Dropout(p=0.0)
  )
  (lstm): LSTM(128, 128, num_layers=2, dropout=0.2, bidirectional=True)
  (fc): Linear(in_features=256, out_features=2, bias=True)
  (dropout): Dropout(p=0.2)
)
Predict Done.43520 records
[34mAuto commit by fitlog[0m
Running with args:Namespace(batch_size=160, cuda=True, data_src='all_data', dropout=0.2, embed_size=128, epochs=10, gpu='0', hidden_size=128, learning_rate=0.001, log_path='./run_records.log', max_seq_len=500, min_count=10, model='LSTMText', model_dir='../../data/models', model_suffix='default', num_layers=2, optim='Adam', patience=10, predict=True, prepare=False, prepare_dir='../../data/prepare/', pretrain=False, pretrain_model='None', reload_model_name='best_LSTMText_accuracy_2019-06-14-04-01-48', result_dir='../../data/results/', train=False, vocab_data='vocab.data', vocab_dir='../../data/vocab', weight_decay=0.0)
Checking the data files...
(vocab_size,class_num,seq_len):(27658,2,1070)
test_size:43444
Data type:<class 'fastNLP.core.dataset.DataSet'>
Loading the model ../../data/models/LSTMText/default/best_LSTMText_accuracy_2019-06-14-04-01-48
LSTMText(
  (embeds): Embedding(
    27658, 128
    (dropout): Dropout(p=0.0)
  )
  (lstm): LSTM(128, 128, num_layers=2, dropout=0.2, bidirectional=True)
  (fc): Linear(in_features=256, out_features=2, bias=True)
  (dropout): Dropout(p=0.2)
)
Predict Done.43520 records
[34mAuto commit by fitlog[0m
Running with args:Namespace(batch_size=160, cuda=True, data_src='all_data', dropout=0.2, embed_size=128, epochs=10, gpu='0', hidden_size=128, learning_rate=0.001, log_path='./run_records.log', max_seq_len=500, min_count=10, model='LSTMText', model_dir='../../data/models', model_suffix='default', num_layers=2, optim='Adam', patience=10, predict=True, prepare=False, prepare_dir='../../data/prepare/', pretrain=False, pretrain_model='None', reload_model_name='best_LSTMText_accuracy_2019-06-14-04-01-48', result_dir='../../data/results/', train=False, vocab_data='vocab.data', vocab_dir='../../data/vocab', weight_decay=0.0)
Checking the data files...
(vocab_size,class_num,seq_len):(27658,2,1070)
test_size:43444
Data type:<class 'fastNLP.core.dataset.DataSet'>
Loading the model ../../data/models/LSTMText/default/best_LSTMText_accuracy_2019-06-14-04-01-48
LSTMText(
  (embeds): Embedding(
    27658, 128
    (dropout): Dropout(p=0.0)
  )
  (lstm): LSTM(128, 128, num_layers=2, dropout=0.2, bidirectional=True)
  (fc): Linear(in_features=256, out_features=2, bias=True)
  (dropout): Dropout(p=0.2)
)
Predict Done.43520 records
[34mAuto commit by fitlog[0m
Running with args:Namespace(batch_size=160, cuda=True, data_src='all_data', dropout=0.2, embed_size=128, epochs=10, gpu='0', hidden_size=128, learning_rate=0.001, log_path='./run_records.log', max_seq_len=500, min_count=10, model='LSTMText', model_dir='../../data/models', model_suffix='default', num_layers=2, optim='Adam', patience=10, predict=True, prepare=False, prepare_dir='../../data/prepare/', pretrain=False, pretrain_model='None', reload_model_name='best_LSTMText_accuracy_2019-06-14-04-01-48', result_dir='../../data/results/', train=False, vocab_data='vocab.data', vocab_dir='../../data/vocab', weight_decay=0.0)
Checking the data files...
(vocab_size,class_num,seq_len):(27658,2,1070)
test_size:43444
Data type:<class 'fastNLP.core.dataset.DataSet'>
Loading the model ../../data/models/LSTMText/default/best_LSTMText_accuracy_2019-06-14-04-01-48
LSTMText(
  (embeds): Embedding(
    27658, 128
    (dropout): Dropout(p=0.0)
  )
  (lstm): LSTM(128, 128, num_layers=2, dropout=0.2, bidirectional=True)
  (fc): Linear(in_features=256, out_features=2, bias=True)
  (dropout): Dropout(p=0.2)
)
Predict Done.43520 records
[34mAuto commit by fitlog[0m
Running with args:Namespace(batch_size=160, cuda=True, data_src='all_data', dropout=0.2, embed_size=128, epochs=10, gpu='0', hidden_size=128, learning_rate=0.001, log_path='./run_records.log', max_seq_len=500, min_count=10, model='LSTMText', model_dir='../../data/models', model_suffix='default', num_layers=2, optim='Adam', patience=10, predict=True, prepare=False, prepare_dir='../../data/prepare/', pretrain=False, pretrain_model='None', reload_model_name='best_LSTMText_accuracy_2019-06-14-04-01-48', result_dir='../../data/results/', train=False, vocab_data='vocab.data', vocab_dir='../../data/vocab', weight_decay=0.0)
Checking the data files...
(vocab_size,class_num,seq_len):(27658,2,1070)
test_size:43444
Data type:<class 'fastNLP.core.dataset.DataSet'>
Loading the model ../../data/models/LSTMText/default/best_LSTMText_accuracy_2019-06-14-04-01-48
LSTMText(
  (embeds): Embedding(
    27658, 128
    (dropout): Dropout(p=0.0)
  )
  (lstm): LSTM(128, 128, num_layers=2, dropout=0.2, bidirectional=True)
  (fc): Linear(in_features=256, out_features=2, bias=True)
  (dropout): Dropout(p=0.2)
)
Predict Done.43520 records
[34mAuto commit by fitlog[0m
Running with args:Namespace(batch_size=160, cuda=True, data_src='all_data', dropout=0.2, embed_size=128, epochs=10, gpu='0', hidden_size=128, learning_rate=0.001, log_path='./run_records.log', max_seq_len=500, min_count=10, model='LSTMText', model_dir='../../data/models', model_suffix='default', num_layers=2, optim='Adam', patience=10, predict=True, prepare=False, prepare_dir='../../data/prepare/', pretrain=False, pretrain_model='None', reload_model_name='best_LSTMText_accuracy_2019-06-14-04-01-48', result_dir='../../data/results/', train=False, vocab_data='vocab.data', vocab_dir='../../data/vocab', weight_decay=0.0)
Checking the data files...
(vocab_size,class_num,seq_len):(27658,2,1070)
test_size:43444
Data type:<class 'fastNLP.core.dataset.DataSet'>
Loading the model ../../data/models/LSTMText/default/best_LSTMText_accuracy_2019-06-14-04-01-48
LSTMText(
  (embeds): Embedding(
    27658, 128
    (dropout): Dropout(p=0.0)
  )
  (lstm): LSTM(128, 128, num_layers=2, dropout=0.2, bidirectional=True)
  (fc): Linear(in_features=256, out_features=2, bias=True)
  (dropout): Dropout(p=0.2)
)
Predict Done.43520 records
[34mAuto commit by fitlog[0m
Running with args:Namespace(batch_size=160, cuda=True, data_src='all_data', dropout=0.2, embed_size=128, epochs=10, gpu='0', hidden_size=128, learning_rate=0.001, log_path='./run_records.log', max_seq_len=500, min_count=10, model='LSTMText', model_dir='../../data/models', model_suffix='default', num_layers=2, optim='Adam', patience=10, predict=True, prepare=False, prepare_dir='../../data/prepare/', pretrain=False, pretrain_model='None', reload_model_name='best_LSTMText_accuracy_2019-06-14-04-01-48', result_dir='../../data/results/', train=False, vocab_data='vocab.data', vocab_dir='../../data/vocab', weight_decay=0.0)
Checking the data files...
(vocab_size,class_num,seq_len):(27658,2,1070)
test_size:43444
Data type:<class 'fastNLP.core.dataset.DataSet'>
Loading the model ../../data/models/LSTMText/default/best_LSTMText_accuracy_2019-06-14-04-01-48
LSTMText(
  (embeds): Embedding(
    27658, 128
    (dropout): Dropout(p=0.0)
  )
  (lstm): LSTM(128, 128, num_layers=2, dropout=0.2, bidirectional=True)
  (fc): Linear(in_features=256, out_features=2, bias=True)
  (dropout): Dropout(p=0.2)
)
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([160, 2])
torch.Size([84, 2])
Predict Done.43520 records
[34mAuto commit by fitlog[0m
Running with args:Namespace(batch_size=160, cuda=True, data_src='all_data', dropout=0.2, embed_size=128, epochs=10, gpu='0', hidden_size=128, learning_rate=0.001, log_path='./run_records.log', max_seq_len=500, min_count=10, model='LSTMText', model_dir='../../data/models', model_suffix='default', num_layers=2, optim='Adam', patience=10, predict=False, prepare=False, prepare_dir='../../data/prepare/', pretrain=False, pretrain_model='None', reload_model_name='best_LSTMText_accuracy_2019-06-14-04-01-48', result_dir='../../data/results/', train=True, vocab_data='vocab.data', vocab_dir='../../data/vocab', weight_decay=0.0)
Checking the data files...
(vocab_size,class_num,seq_len):(27658,1,1070)
No pretrained model with be used.
vocabsize:27658
Using LSTM Model.
LSTMText(
  (embeds): Embedding(
    27658, 128
    (dropout): Dropout(p=0.0)
  )
  (lstm): LSTM(128, 128, num_layers=2, dropout=0.2, bidirectional=True)
  (fc): Linear(in_features=256, out_features=1, bias=True)
  (dropout): Dropout(p=0.2)
)
train_size:285354 ; val_size:128282 ; test_size:43444
Using Adam as optimizer.
input fields after batch(if batch size is 2):
	words: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2, 1070]) 
	seq_len: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2]) 
target fields after batch(if batch size is 2):
	target: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2]) 

training epochs started 2019-06-14-09-54-52
