[34mAuto commit by fitlog[0m
Running with args:Namespace(batch_size=128, cuda=True, data_dir='../../data', data_src='all_data', dropout=0.3, embed_size=128, epochs=4, gpu='2', hidden_size=128, learning_rate=0.001, log_path='./run_records.log', max_seq_len=500, min_count=10, model='CNNText', model_dir='../../data/models', model_suffix='essay_at_least_1_green_donation', num_layers=2, optim='Adam', patience=2, predict=False, prepare=True, prepare_dir='../../data/prepare/', pretrain=False, pretrain_model='None', reload_model_name='best_LSTMText_accuracy_2019-06-14-04-01-48', result_dir='../../data/results/', show_data=False, target_var='at_least_1_green_donation', text_var='essay', train=True, vocab_data='vocab_essay_at_least_1_green_donation.data', vocab_dir='../../data/vocab', weight_decay=0.0)
Checking the data files...
Checking the data files...
Generateing essay - at_least_1_green_donation
Loading data from ../../data/essays_all_outcome.csv ...
nums:(291519,131329,44772)
Over Sample mode
subset nums:(291519,131329,44772)
0
tokenize train set
Tokenizing data , total num:291519
Tokenized:291519/291519
ATTENTION TYPE:<class 'float'> * 1038
tokenize val set
Tokenizing data , total num:131329
Tokenized:131329/131329
ATTENTION TYPE:<class 'float'> * 811
tokenize test set
Tokenizing data , total num:44772
Tokenized:44772/44772
ATTENTION TYPE:<class 'float'> * 688
Building Fastnlp dataset.
Over Sampling...
Building Fastnlp vocabulary.
Building id-presentation for train_set and test_set.
1070
Building target-vector for train_set and test_set.
Data Sizes (354254, 179800, 44772)
Saving vocab(TextData)...
Done with preparing!
(vocab_size,class_num,seq_len):(29540,2,1070)
No pretrained model with be used.
vocabsize:29540
Using CNN Model.
CNNText(
  (embed): Embedding(
    29540, 128
    (dropout): Dropout(p=0.0)
  )
  (conv_pool): ConvMaxpool(
    (convs): ModuleList(
      (0): Conv1d(128, 3, kernel_size=(3,), stride=(1,), padding=(2,))
      (1): Conv1d(128, 4, kernel_size=(4,), stride=(1,), padding=(2,))
      (2): Conv1d(128, 5, kernel_size=(5,), stride=(1,), padding=(2,))
    )
  )
  (dropout): Dropout(p=0.3)
  (fc): Linear(in_features=12, out_features=2, bias=True)
)
train_size:354254 ; val_size:179800 ; test_size:44772
Using Adam as optimizer.
input fields after batch(if batch size is 2):
	words: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2, 1070]) 
	seq_len: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2]) 
target fields after batch(if batch size is 2):
	target: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2]) 

training epochs started 2019-06-15-12-02-43
loss:1.26389
loss:1.12948
loss:1.18187
loss:1.06659
loss:1.08182
loss:1.01829
loss:0.92806
loss:0.98523
loss:0.94447
loss:0.95916
loss:0.93417
loss:0.91775
loss:0.86356
loss:0.88781
loss:0.90550
loss:0.83509
loss:0.83918
loss:0.81048
loss:0.80559
loss:0.81888
loss:0.79061
loss:0.77739
loss:0.77899
loss:0.76170
loss:0.78348
loss:0.75852
loss:0.76427
loss:0.78371
loss:0.72268
loss:0.73932
loss:0.74245
loss:0.75992
loss:0.76854
loss:0.73495
loss:0.72539
loss:0.73195
loss:0.72053
loss:0.73397
loss:0.73460
loss:0.72868
loss:0.70535
loss:0.72156
loss:0.71800
loss:0.71028
loss:0.72733
loss:0.70976
loss:0.70623
loss:0.71658
loss:0.73061
loss:0.69990
loss:0.71099
loss:0.70624
loss:0.70486
loss:0.72094
loss:0.71416
loss:0.69911
loss:0.70615
loss:0.71068
loss:0.68886
loss:0.70874
loss:0.69508
loss:0.69743
loss:0.70941
loss:0.71136
loss:0.69719
loss:0.70966
loss:0.69874
loss:0.69791
loss:0.69490
loss:0.69889
loss:0.69793
loss:0.70221
loss:0.69926
loss:0.70532
loss:0.70347
loss:0.70690
loss:0.69470
loss:0.70226
loss:0.70152
loss:0.69427
loss:0.69311
loss:0.70247
loss:0.69716
loss:0.68980
loss:0.69719
loss:0.70017
loss:0.68757
loss:0.69592
loss:0.68997
loss:0.69483
loss:0.70904
loss:0.69907
loss:0.69864
loss:0.69845
loss:0.69656
loss:0.69171
loss:0.69108
loss:0.68854
loss:0.69384
loss:0.70482
loss:0.68936
loss:0.68772
loss:0.69597
loss:0.69252
loss:0.69229
loss:0.68705
loss:0.69546
loss:0.69096
loss:0.69559
loss:0.69514
loss:0.68932
loss:0.69064
loss:0.69834
loss:0.69299
loss:0.69268
loss:0.69195
loss:0.69469
loss:0.69752
loss:0.69866
loss:0.69253
loss:0.69543
loss:0.69133
loss:0.69165
loss:0.69664
loss:0.68275
loss:0.69416
loss:0.69375
loss:0.69680
loss:0.68968
loss:0.69202
loss:0.68635
loss:0.69337
loss:0.69101
loss:0.69290
loss:0.68623
loss:0.68924
loss:0.68972
loss:0.69465
loss:0.69185
loss:0.69146
loss:0.69363
loss:0.68825
loss:0.68981
loss:0.68774
loss:0.69944
loss:0.69600
loss:0.68907
loss:0.69284
loss:0.69440
loss:0.70028
loss:0.69486
loss:0.69061
loss:0.69239
loss:0.69532
loss:0.69139
loss:0.68601
loss:0.68486
loss:0.69570
loss:0.69040
loss:0.68473
loss:0.69304
loss:0.69449
loss:0.69145
loss:0.69513
loss:0.68859
loss:0.69515
loss:0.69475
loss:0.69165
loss:0.68857
loss:0.68743
loss:0.69392
loss:0.69265
loss:0.68905
loss:0.69803
loss:0.69298
loss:0.68948
loss:0.68762
loss:0.68770
loss:0.68684
loss:0.69105
loss:0.69249
loss:0.68512
loss:0.68396
loss:0.69855
loss:0.69026
loss:0.69018
loss:0.69092
loss:0.68877
loss:0.69506
loss:0.68965
loss:0.68965
loss:0.68880
loss:0.69389
loss:0.69018
loss:0.69075
loss:0.69158
loss:0.68893
loss:0.69247
loss:0.68898
loss:0.68497
loss:0.70824
loss:0.69118
loss:0.69093
loss:0.70025
loss:0.68870
loss:0.69035
loss:0.69233
loss:0.69116
loss:0.69413
loss:0.68680
loss:0.68613
loss:0.68637
loss:0.68566
loss:0.69054
loss:0.68799
loss:0.67967
loss:0.68373
loss:0.69940
loss:0.68268
loss:0.68351
loss:0.68541
loss:0.68754
loss:0.68401
loss:0.68939
loss:0.69227
loss:0.69519
loss:0.68748
loss:0.69372
loss:0.68913
loss:0.68590
loss:0.68305
loss:0.68808
loss:0.68829
loss:0.68238
loss:0.68857
loss:0.68318
loss:0.68644
loss:0.68417
loss:0.68829
loss:0.68582
loss:0.69151
loss:0.69062
loss:0.69387
loss:0.68816
loss:0.69139
loss:0.68408
loss:0.68722
loss:0.69474
loss:0.68268
loss:0.68881
loss:0.68486
loss:0.68556
loss:0.68692
loss:0.67840
loss:0.68870
loss:0.69070
loss:0.69407
loss:0.68450
loss:0.68871
loss:0.68489
loss:0.68452
loss:0.68688
loss:0.68361
loss:0.68897
loss:0.68488
loss:0.68597
loss:0.68571
loss:0.68708
loss:0.69026
loss:0.69137
loss:0.68680
loss:0.68160
loss:0.68916
loss:0.68992
loss:0.68411
loss:0.68676
loss:0.68584
loss:0.69472
loss:0.69062
loss:0.68767
loss:0.68717
loss:0.69344
loss:0.68981
loss:0.67996
loss:0.68500
loss:0.68083
loss:0.68758
loss:0.69045
loss:0.68423
loss:0.68163
loss:0.68445
loss:0.69003
loss:0.68513
loss:0.68709
loss:0.68775
loss:0.68955
loss:0.69127
loss:0.68086
loss:0.69880
loss:0.68844
loss:0.68509
loss:0.68547
loss:0.68903
loss:0.69541
loss:0.68543
loss:0.69532
loss:0.68458
loss:0.69774
loss:0.68779
loss:0.68568
loss:0.68646
loss:0.69257
loss:0.68857
loss:0.68922
loss:0.68335
loss:0.68605
loss:0.68718
loss:0.68882
loss:0.68661
loss:0.68739
loss:0.68521
loss:0.67972
loss:0.68880
loss:0.68068
loss:0.68664
loss:0.68173
loss:0.68557
loss:0.68779
loss:0.68946
loss:0.68076
loss:0.69318
loss:0.69120
loss:0.67971
loss:0.69255
loss:0.68401
loss:0.68266
loss:0.68531
loss:0.68704
loss:0.68576
loss:0.68336
loss:0.68425
loss:0.67628
loss:0.69140
loss:0.68431
loss:0.68467
loss:0.68231
loss:0.68342
loss:0.69766
loss:0.67885
loss:0.68166
loss:0.68954
loss:0.68404
loss:0.69530
loss:0.69283
loss:0.68814
loss:0.68957
loss:0.67975
loss:0.68936
loss:0.68740
loss:0.68202
loss:0.68839
loss:0.68464
loss:0.68597
loss:0.68279
loss:0.68855
loss:0.68681
loss:0.68970
loss:0.68701
loss:0.67580
loss:0.69470
loss:0.69102
loss:0.68629
loss:0.68021
loss:0.69100
loss:0.68389
loss:0.67937
loss:0.68813
loss:0.69134
loss:0.68865
loss:0.68661
loss:0.68808
loss:0.68892
loss:0.68511
loss:0.68549
loss:0.67416
loss:0.69497
loss:0.68004
loss:0.69071
loss:0.69149
loss:0.68103
loss:0.68591
loss:0.69140
loss:0.67871
loss:0.68705
loss:0.68506
loss:0.68978
loss:0.68435
loss:0.68600
loss:0.68007
loss:0.68893
loss:0.68688
loss:0.68674
loss:0.68255
loss:0.68830
loss:0.69032
loss:0.68854
loss:0.68904
loss:0.68877
loss:0.67458
loss:0.67929
loss:0.68381
loss:0.67528
loss:0.67777
loss:0.69283
loss:0.67744
loss:0.68707
loss:0.68420
loss:0.68202
loss:0.68053
loss:0.69099
loss:0.68824
loss:0.68423
loss:0.67853
loss:0.67286
loss:0.67886
loss:0.68428
loss:0.68451
loss:0.68576
loss:0.68961
loss:0.68419
loss:0.68188
loss:0.68757
loss:0.68818
loss:0.68761
loss:0.68211
loss:0.67839
loss:0.68859
loss:0.69460
loss:0.68185
loss:0.69505
loss:0.68318
loss:0.68736
loss:0.68410
loss:0.68262
loss:0.67889
loss:0.67697
loss:0.69458
loss:0.68982
loss:0.68280
loss:0.68262
loss:0.68733
loss:0.68515
loss:0.67756
loss:0.67906
loss:0.68424
loss:0.67292
loss:0.67891
loss:0.68896
loss:0.68500
loss:0.68850
loss:0.68685
loss:0.69200
loss:0.67802
loss:0.69031
loss:0.68799
loss:0.67874
loss:0.68814
loss:0.67929
loss:0.67973
loss:0.68505
loss:0.69127
loss:0.67767
loss:0.68206
loss:0.68630
loss:0.68387
loss:0.69565
loss:0.67842
loss:0.68891
loss:0.67809
loss:0.68873
loss:0.68862
loss:0.68644
loss:0.68589
loss:0.68256
loss:0.68596
loss:0.69040
loss:0.68392
loss:0.68984
loss:0.67267
loss:0.68230
loss:0.68558
loss:0.67885
loss:0.68393
loss:0.67899
loss:0.68115
loss:0.68345
loss:0.68620
loss:0.69110
loss:0.68872
loss:0.67981
loss:0.68389
loss:0.68010
loss:0.69173
loss:0.68823
loss:0.68196
loss:0.68879
loss:0.68070
loss:0.67801
loss:0.68032
loss:0.67378
loss:0.68131
loss:0.68194
loss:0.69004
loss:0.68035
loss:0.68403
loss:0.69363
loss:0.69012
loss:0.67557
loss:0.68608
loss:0.69281
loss:0.67529
loss:0.67814
loss:0.68683
loss:0.67843
loss:0.68433
loss:0.68406
loss:0.68358
loss:0.68364
loss:0.67648
loss:0.67493
loss:0.68609
loss:0.68032
loss:0.68886
loss:0.67905
loss:0.67870
loss:0.68754
loss:0.67824
loss:0.68610
loss:0.68921
loss:0.69241
loss:0.68871
loss:0.68577
loss:0.67827
loss:0.67220
loss:0.68906
loss:0.68608
loss:0.68959
loss:0.69313
loss:0.68470
loss:0.67557
loss:0.68349
loss:0.68384
loss:0.67817
Evaluation at Epoch 1/4. Step:2768/11072. AccuracyMetric: acc=0.565211

loss:0.67357
loss:0.67677
loss:0.67691
loss:0.67060
loss:0.68250
loss:0.67511
loss:0.68033
loss:0.67981
loss:0.67569
loss:0.68654
loss:0.67858
loss:0.67497
loss:0.68261
loss:0.67628
loss:0.67250
loss:0.67186
loss:0.66901
loss:0.67139
loss:0.67411
loss:0.68179
loss:0.68884
loss:0.67680
loss:0.68726
loss:0.67930
loss:0.68028
loss:0.67247
loss:0.67983
loss:0.68536
loss:0.67776
loss:0.68316
loss:0.67521
loss:0.67747
loss:0.67380
loss:0.66832
loss:0.67776
loss:0.67809
loss:0.67760
loss:0.68645
loss:0.67075
loss:0.68066
loss:0.68087
loss:0.67269
loss:0.67928
loss:0.65664
loss:0.67151
loss:0.66279
loss:0.68937
loss:0.68693
loss:0.67079
loss:0.69022
loss:0.67558
loss:0.67504
loss:0.67306
loss:0.68579
loss:0.67070
loss:0.68936
loss:0.67647
loss:0.67516
loss:0.68278
loss:0.68433
loss:0.68727
loss:0.68163
loss:0.68082
loss:0.68965
loss:0.67410
loss:0.68657
loss:0.66735
loss:0.67847
loss:0.68322
loss:0.68493
loss:0.68867
loss:0.67918
loss:0.69091
loss:0.67410
loss:0.68500
loss:0.69635
loss:0.68439
loss:0.67701
loss:0.67421
loss:0.67641
loss:0.68858
loss:0.67740
loss:0.68354
loss:0.67351
loss:0.68504
loss:0.68572
loss:0.67411
loss:0.68051
loss:0.67229
loss:0.67905
loss:0.67199
loss:0.68077
loss:0.68072
loss:0.67982
loss:0.68490
loss:0.66683
loss:0.67107
loss:0.67180
loss:0.69453
loss:0.67221
loss:0.68532
loss:0.67934
loss:0.67796
loss:0.68344
loss:0.67574
loss:0.68956
loss:0.68858
loss:0.67663
loss:0.66832
loss:0.68049
loss:0.68402
loss:0.67775
loss:0.67981
loss:0.69063
loss:0.67911
loss:0.67556
loss:0.67021
loss:0.67264
loss:0.67039
loss:0.68367
loss:0.67350
loss:0.67165
loss:0.67357
loss:0.68206
loss:0.67762
loss:0.68175
loss:0.67517
loss:0.67607
loss:0.67002
loss:0.67664
loss:0.68576
loss:0.68433
loss:0.66892
loss:0.67695
loss:0.67614
loss:0.67759
loss:0.67558
loss:0.68115
loss:0.68459
loss:0.67858
loss:0.68193
loss:0.68959
loss:0.68682
loss:0.68050
loss:0.67813
loss:0.68219
loss:0.67889
loss:0.68657
loss:0.67014
loss:0.66707
loss:0.68651
loss:0.67563
loss:0.67220
loss:0.67965
loss:0.67162
loss:0.68299
loss:0.69521
loss:0.66789
loss:0.66883
loss:0.67870
loss:0.68168
loss:0.68654
loss:0.68777
loss:0.68116
loss:0.66331
loss:0.67220
loss:0.68295
loss:0.69399
loss:0.67562
loss:0.67485
loss:0.67669
loss:0.67043
loss:0.67860
loss:0.67135
loss:0.68624
loss:0.66534
loss:0.69053
loss:0.68039
loss:0.67676
loss:0.67522
loss:0.67584
loss:0.68096
loss:0.67653
loss:0.67813
loss:0.67648
loss:0.67960
loss:0.68594
loss:0.65472
loss:0.67767
loss:0.68532
loss:0.67830
loss:0.68688
loss:0.66511
loss:0.67676
loss:0.67313
loss:0.67973
loss:0.68617
loss:0.67990
loss:0.68481
loss:0.66870
loss:0.67853
loss:0.68191
loss:0.67591
loss:0.67358
loss:0.68019
loss:0.67839
loss:0.68325
loss:0.67680
loss:0.67303
loss:0.67998
loss:0.67955
loss:0.69174
loss:0.67493
loss:0.67593
loss:0.68094
loss:0.67193
loss:0.66707
loss:0.66779
loss:0.67240
loss:0.68400
loss:0.66902
loss:0.67634
loss:0.66857
loss:0.69021
loss:0.67858
loss:0.66722
loss:0.67891
loss:0.68739
loss:0.68145
loss:0.69298
loss:0.66659
loss:0.67390
loss:0.66816
loss:0.67772
loss:0.67343
loss:0.68025
loss:0.67942
loss:0.68171
loss:0.67139
loss:0.67416
loss:0.67973
loss:0.66789
loss:0.66742
loss:0.68692
loss:0.67793
loss:0.67920
loss:0.68358
loss:0.67236
loss:0.67988
loss:0.67211
loss:0.67848
loss:0.67184
loss:0.68209
loss:0.67543
loss:0.67204
loss:0.68312
loss:0.67573
loss:0.67293
loss:0.68868
loss:0.68320
loss:0.67788
loss:0.67463
loss:0.68326
loss:0.66077
loss:0.68038
loss:0.67331
loss:0.68290
loss:0.67030
loss:0.67326
loss:0.67235
loss:0.66715
loss:0.68343
loss:0.67454
loss:0.66629
loss:0.66418
loss:0.67404
loss:0.68638
loss:0.66504
loss:0.67398
loss:0.67794
loss:0.68060
loss:0.68300
loss:0.68082
loss:0.68144
loss:0.67286
loss:0.67626
loss:0.67212
loss:0.68166
loss:0.67525
loss:0.67391
loss:0.67335
loss:0.67550
loss:0.67981
loss:0.68196
loss:0.67126
loss:0.66832
loss:0.67395
loss:0.67310
loss:0.67693
loss:0.68116
loss:0.67125
loss:0.67481
loss:0.67107
loss:0.67838
loss:0.68348
loss:0.66526
loss:0.67378
loss:0.67628
loss:0.68809
loss:0.66337
loss:0.66690
loss:0.66571
loss:0.67460
loss:0.68229
loss:0.67925
loss:0.68435
loss:0.68556
loss:0.67907
loss:0.66492
loss:0.67163
loss:0.67948
loss:0.67988
loss:0.67728
loss:0.67270
loss:0.69588
loss:0.67550
loss:0.68082
loss:0.68246
loss:0.68252
loss:0.67351
loss:0.67409
loss:0.68351
loss:0.67228
loss:0.67123
loss:0.66733
loss:0.68091
loss:0.66877
loss:0.68827
loss:0.68724
loss:0.67546
loss:0.66717
loss:0.68518
loss:0.67303
loss:0.67678
loss:0.67561
loss:0.67669
loss:0.68724
loss:0.67128
loss:0.67113
loss:0.67064
loss:0.66914
loss:0.67431
loss:0.66944
loss:0.68045
loss:0.66875
loss:0.67504
loss:0.66956
loss:0.67289
loss:0.67686
loss:0.67155
loss:0.66317
loss:0.66945
loss:0.67335
loss:0.68041
loss:0.68053
loss:0.67924
loss:0.67030
loss:0.68044
loss:0.67291
loss:0.65882
loss:0.68536
loss:0.68395
loss:0.68507
loss:0.67404
loss:0.67738
loss:0.68466
loss:0.66989
loss:0.67100
loss:0.67231
loss:0.66890
loss:0.68285
loss:0.68465
loss:0.66967
loss:0.66901
loss:0.68885
loss:0.66918
loss:0.66519
loss:0.68305
loss:0.68546
loss:0.68015
loss:0.68301
loss:0.67918
loss:0.67957
loss:0.67694
loss:0.67878
loss:0.67540
loss:0.66852
loss:0.68438
loss:0.66687
loss:0.67575
loss:0.66683
loss:0.67688
loss:0.66545
loss:0.67233
loss:0.66586
loss:0.66872
loss:0.67090
loss:0.67931
loss:0.67549
loss:0.66667
loss:0.67973
loss:0.67202
loss:0.67859
loss:0.67850
loss:0.67775
loss:0.66770
loss:0.66993
loss:0.67320
loss:0.66381
loss:0.66091
loss:0.67468
loss:0.67305
loss:0.69025
loss:0.65940
loss:0.67917
loss:0.68305
loss:0.68229
loss:0.66656
loss:0.67564
loss:0.67958
loss:0.67028
loss:0.68749
loss:0.67723
loss:0.67474
loss:0.67623
loss:0.68010
loss:0.67917
loss:0.68603
loss:0.67819
loss:0.66244
loss:0.66402
loss:0.68156
loss:0.68164
loss:0.67760
loss:0.67927
loss:0.67072
loss:0.68785
loss:0.65877
loss:0.67723
loss:0.65996
loss:0.67609
loss:0.67335
loss:0.67680
loss:0.67623
loss:0.66276
loss:0.67395
loss:0.66889
loss:0.66815
loss:0.67111
loss:0.67164
loss:0.68079
loss:0.67885
loss:0.65788
loss:0.67470
loss:0.66926
loss:0.66949
loss:0.68075
loss:0.67163
loss:0.66579
loss:0.67307
loss:0.67128
loss:0.67318
loss:0.67522
loss:0.66256
loss:0.68501
loss:0.66023
loss:0.66350
loss:0.68604
loss:0.67346
loss:0.67050
loss:0.66189
loss:0.67100
loss:0.66964
loss:0.67124
loss:0.67060
loss:0.66977
loss:0.67259
loss:0.67554
loss:0.67522
loss:0.67428
loss:0.68941
loss:0.67486
loss:0.67365
loss:0.66914
loss:0.66800
loss:0.67402
loss:0.67297
loss:0.66598
loss:0.66220
loss:0.66946
loss:0.68209
loss:0.67792
loss:0.66401
loss:0.66010
loss:0.65354
loss:0.68533
loss:0.68399
loss:0.67692
loss:0.66177
loss:0.68923
loss:0.67322
loss:0.68099
loss:0.68579
loss:0.67446
loss:0.67605
loss:0.67381
loss:0.68501
loss:0.67136
loss:0.67879
loss:0.66622
loss:0.65801
loss:0.65783
loss:0.67722
loss:0.68253
loss:0.67325
loss:0.65478
loss:0.66958
loss:0.67862
loss:0.67957
loss:0.66881
loss:0.66791
loss:0.66308
loss:0.66031
loss:0.68732
loss:0.68535
loss:0.67413
loss:0.67307
loss:0.66493
loss:0.67252
loss:0.66729
loss:0.67078
loss:0.66331
loss:0.68517
loss:0.67287
loss:0.67103
loss:0.67364
loss:0.67363
loss:0.66345
loss:0.67297
loss:0.66585
loss:0.67232
loss:0.66611
loss:0.66962
loss:0.68078
Evaluation at Epoch 2/4. Step:5536/11072. AccuracyMetric: acc=0.576429

loss:0.65334
loss:0.67004
loss:0.66722
loss:0.65699
loss:0.67089
loss:0.65597
loss:0.66419
loss:0.67680
loss:0.68360
loss:0.66943
loss:0.67433
loss:0.66902
loss:0.67290
loss:0.68043
loss:0.67037
loss:0.67751
loss:0.66516
loss:0.66472
loss:0.65580
loss:0.67422
loss:0.67248
loss:0.65165
loss:0.67939
loss:0.65801
loss:0.67277
loss:0.66883
loss:0.66618
loss:0.66125
loss:0.65781
loss:0.67626
loss:0.65814
loss:0.66634
loss:0.65309
loss:0.67234
loss:0.65460
loss:0.66037
loss:0.66903
loss:0.65688
loss:0.67009
loss:0.66285
loss:0.67708
loss:0.67043
loss:0.66596
loss:0.67085
loss:0.64985
loss:0.65583
loss:0.66605
loss:0.66752
loss:0.67319
loss:0.67105
loss:0.66657
loss:0.65710
loss:0.67353
loss:0.65782
loss:0.65040
loss:0.65959
loss:0.65171
loss:0.65371
loss:0.66702
loss:0.66765
loss:0.67570
loss:0.65990
loss:0.68519
loss:0.66319
loss:0.67297
loss:0.66983
loss:0.66050
loss:0.67300
loss:0.64603
loss:0.66825
loss:0.65170
loss:0.64706
loss:0.67033
loss:0.66983
loss:0.66989
loss:0.67278
loss:0.65033
loss:0.65620
loss:0.66539
loss:0.66549
loss:0.65237
loss:0.66158
loss:0.66433
loss:0.66248
loss:0.67890
loss:0.66717
loss:0.65854
loss:0.66034
loss:0.67104
loss:0.65096
loss:0.66609
loss:0.66709
loss:0.67015
loss:0.65175
loss:0.65762
loss:0.64952
loss:0.66912
loss:0.67180
loss:0.68910
loss:0.65823
loss:0.66547
loss:0.67570
loss:0.66037
loss:0.65623
loss:0.67658
loss:0.66087
loss:0.68544
loss:0.67351
loss:0.65348
loss:0.67040
loss:0.66642
loss:0.66132
loss:0.66980
loss:0.66061
loss:0.65804
loss:0.66968
loss:0.68498
loss:0.65628
loss:0.65284
loss:0.64841
loss:0.68556
loss:0.67577
loss:0.67721
loss:0.64820
loss:0.67706
loss:0.67246
loss:0.66604
loss:0.66979
loss:0.66335
loss:0.66316
loss:0.67022
loss:0.67002
loss:0.68394
loss:0.67808
loss:0.66411
loss:0.66429
loss:0.65015
loss:0.67656
loss:0.66471
loss:0.66328
loss:0.66686
loss:0.68682
loss:0.66079
loss:0.65927
loss:0.67340
loss:0.67828
loss:0.68493
loss:0.66161
loss:0.66753
loss:0.67129
loss:0.67151
loss:0.67269
loss:0.67245
loss:0.66030
loss:0.65649
loss:0.64750
loss:0.66138
loss:0.66257
loss:0.66669
loss:0.66530
loss:0.65818
loss:0.66119
loss:0.66587
loss:0.65548
loss:0.65366
loss:0.65536
loss:0.66361
loss:0.64700
loss:0.66265
loss:0.65132
loss:0.67119
loss:0.66615
loss:0.65832
loss:0.66381
loss:0.66112
loss:0.67290
loss:0.67116
loss:0.66981
loss:0.66666
loss:0.66744
loss:0.65927
loss:0.66748
loss:0.66465
loss:0.65850
loss:0.66561
loss:0.66505
loss:0.67780
loss:0.66228
loss:0.66637
loss:0.65861
loss:0.65609
loss:0.64961
loss:0.66176
loss:0.65318
loss:0.64438
loss:0.66507
loss:0.66648
loss:0.67550
loss:0.65395
loss:0.66365
loss:0.67751
loss:0.66795
loss:0.67330
loss:0.67119
loss:0.66983
loss:0.67024
loss:0.65678
loss:0.67268
loss:0.67358
loss:0.66155
loss:0.65942
loss:0.65637
loss:0.65919
loss:0.65645
loss:0.65531
loss:0.65918
loss:0.66440
loss:0.67260
loss:0.64713
loss:0.67319
loss:0.65867
loss:0.66388
loss:0.65259
loss:0.66311
loss:0.66938
loss:0.66904
loss:0.67412
loss:0.67284
loss:0.66970
loss:0.66718
loss:0.67941
loss:0.66091
loss:0.65384
loss:0.66908
loss:0.66363
loss:0.66708
loss:0.65800
loss:0.67516
loss:0.66997
loss:0.66615
loss:0.66049
loss:0.66519
loss:0.63671
loss:0.67125
loss:0.65915
loss:0.68246
loss:0.65797
loss:0.65941
loss:0.66505
loss:0.66305
loss:0.67370
loss:0.64903
loss:0.67030
loss:0.64777
loss:0.65004
loss:0.67555
loss:0.64803
loss:0.67212
loss:0.65806
loss:0.68120
loss:0.67112
loss:0.66377
loss:0.66303
loss:0.65454
loss:0.66390
loss:0.67160
loss:0.67269
loss:0.66039
loss:0.67250
loss:0.66578
loss:0.66664
loss:0.68084
loss:0.67158
loss:0.66231
loss:0.67091
loss:0.65170
loss:0.66000
loss:0.66088
loss:0.66136
loss:0.65769
loss:0.65939
loss:0.66874
loss:0.66230
loss:0.65569
loss:0.65261
loss:0.66321
loss:0.66707
loss:0.65743
loss:0.65505
loss:0.66395
loss:0.67648
loss:0.66833
loss:0.68443
loss:0.66452
loss:0.67326
loss:0.67223
loss:0.66387
loss:0.66268
loss:0.65500
loss:0.68674
loss:0.66537
loss:0.65932
loss:0.66830
loss:0.66386
loss:0.67429
loss:0.67142
loss:0.65057
loss:0.66110
loss:0.65271
loss:0.67412
loss:0.67980
loss:0.67043
loss:0.65958
loss:0.65904
loss:0.65626
loss:0.65744
loss:0.65527
loss:0.66027
loss:0.66325
loss:0.68828
loss:0.64708
loss:0.65466
loss:0.65349
loss:0.66224
loss:0.66576
loss:0.66816
loss:0.65369
loss:0.66300
loss:0.66444
loss:0.66923
loss:0.66744
loss:0.67813
loss:0.66050
loss:0.66274
loss:0.66375
loss:0.65648
loss:0.66025
loss:0.68181
loss:0.65876
loss:0.64938
loss:0.67380
loss:0.66373
loss:0.67458
loss:0.65615
loss:0.65293
loss:0.65875
loss:0.69269
loss:0.66252
loss:0.65642
loss:0.66895
loss:0.66799
loss:0.67867
loss:0.65840
loss:0.66345
loss:0.66671
loss:0.65840
loss:0.67444
loss:0.65900
loss:0.66652
loss:0.66705
loss:0.67112
loss:0.64330
loss:0.66791
loss:0.66421
loss:0.66785
loss:0.67876
loss:0.65718
loss:0.67107
loss:0.66182
loss:0.67527
loss:0.66456
loss:0.65773
loss:0.66888
loss:0.67137
loss:0.66087
loss:0.66907
loss:0.65025
loss:0.66011
loss:0.66557
loss:0.67210
loss:0.66240
loss:0.68166
loss:0.66238
loss:0.65409
loss:0.68017
loss:0.65975
loss:0.65335
loss:0.68275
loss:0.66661
loss:0.65553
loss:0.67555
loss:0.65833
loss:0.67289
loss:0.68159
loss:0.65379
loss:0.68443
loss:0.66762
loss:0.65996
loss:0.67046
loss:0.66642
loss:0.65700
loss:0.66025
loss:0.66793
loss:0.65638
loss:0.65756
loss:0.66999
loss:0.67889
loss:0.66471
loss:0.66989
loss:0.68136
loss:0.66827
loss:0.66363
loss:0.64638
loss:0.67400
loss:0.66461
loss:0.67148
loss:0.65964
loss:0.65320
loss:0.66350
loss:0.66903
loss:0.65686
loss:0.67140
loss:0.66277
loss:0.67840
loss:0.67061
loss:0.66965
loss:0.66907
loss:0.65700
loss:0.66005
loss:0.66595
loss:0.64625
loss:0.65882
loss:0.65811
loss:0.64700
loss:0.66865
loss:0.66888
loss:0.67019
loss:0.66309
loss:0.65385
loss:0.66536
loss:0.66747
loss:0.64918
loss:0.65039
loss:0.66090
loss:0.66760
loss:0.65961
loss:0.65806
loss:0.66497
loss:0.67087
loss:0.65045
loss:0.64636
loss:0.65480
loss:0.67709
loss:0.66368
loss:0.66808
loss:0.66390
loss:0.64929
loss:0.65024
loss:0.66909
loss:0.66867
loss:0.68417
loss:0.64799
loss:0.65696
loss:0.65685
loss:0.65060
loss:0.65328
loss:0.66825
loss:0.65762
loss:0.68564
loss:0.66588
loss:0.68374
loss:0.65660
loss:0.65414
loss:0.66656
loss:0.66265
loss:0.64186
loss:0.67102
loss:0.64811
loss:0.66389
loss:0.66009
loss:0.67190
loss:0.67394
loss:0.66885
loss:0.65807
loss:0.67568
loss:0.66837
loss:0.65802
loss:0.65826
loss:0.64644
loss:0.65862
loss:0.66818
loss:0.66867
loss:0.64555
loss:0.65713
loss:0.65443
loss:0.65316
loss:0.64089
loss:0.64837
loss:0.65493
loss:0.66364
loss:0.65527
loss:0.64792
loss:0.67253
loss:0.63246
loss:0.65800
loss:0.66353
loss:0.67551
loss:0.66929
loss:0.66303
loss:0.65692
loss:0.65723
loss:0.66624
loss:0.66582
loss:0.66311
loss:0.66774
loss:0.65557
loss:0.65189
loss:0.66003
loss:0.63771
loss:0.64326
loss:0.64778
loss:0.67245
loss:0.67262
loss:0.66128
loss:0.65887
loss:0.65710
loss:0.64840
loss:0.65215
loss:0.65140
loss:0.65784
loss:0.67675
loss:0.67834
loss:0.67179
loss:0.69646
loss:0.66422
loss:0.64920
loss:0.66239
loss:0.67368
loss:0.67461
loss:0.67770
loss:0.66579
loss:0.64758
loss:0.66793
loss:0.66410
loss:0.65399
loss:0.66636
loss:0.66632
loss:0.65142
loss:0.65435
loss:0.67007
loss:0.66024
loss:0.65327
loss:0.66115
Evaluation at Epoch 3/4. Step:8304/11072. AccuracyMetric: acc=0.569967

loss:0.65729
loss:0.64815
loss:0.65843
loss:0.64340
loss:0.65902
loss:0.64726
loss:0.64772
loss:0.64426
loss:0.63848
loss:0.65405
loss:0.66175
loss:0.65384
loss:0.65544
loss:0.65072
loss:0.65192
loss:0.64654
loss:0.65897
loss:0.65841
loss:0.65160
loss:0.65882
loss:0.65635
loss:0.66396
loss:0.65161
loss:0.64206
loss:0.64258
loss:0.67201
loss:0.65358
loss:0.64987
loss:0.65593
loss:0.65550
loss:0.66879
loss:0.63876
loss:0.67954
loss:0.64687
loss:0.63142
loss:0.66130
loss:0.63615
loss:0.64842
loss:0.65380
loss:0.64091
loss:0.63822
loss:0.65911
loss:0.65827
loss:0.65691
loss:0.64396
loss:0.66146
loss:0.64882
loss:0.63633
loss:0.64298
loss:0.64747
loss:0.64730
loss:0.64065
loss:0.66625
loss:0.65863
loss:0.67249
loss:0.65377
loss:0.65398
loss:0.65441
loss:0.62680
loss:0.65635
loss:0.63211
loss:0.63210
loss:0.65582
loss:0.65770
loss:0.63672
loss:0.66026
loss:0.64780
loss:0.64457
loss:0.63461
loss:0.64199
loss:0.65354
loss:0.66672
loss:0.65443
loss:0.63395
loss:0.64806
loss:0.64882
loss:0.65558
loss:0.66320
loss:0.63385
loss:0.64240
loss:0.64679
loss:0.65592
loss:0.66644
loss:0.64742
loss:0.64399
loss:0.65922
loss:0.64467
loss:0.64272
loss:0.63017
loss:0.63935
loss:0.66106
loss:0.66963
loss:0.64162
loss:0.65360
loss:0.67439
loss:0.64406
loss:0.65852
loss:0.64582
loss:0.64178
loss:0.64696
loss:0.63675
loss:0.64070
loss:0.64475
loss:0.63925
loss:0.63498
loss:0.65350
loss:0.65969
loss:0.63702
loss:0.65430
loss:0.64654
loss:0.65711
loss:0.66657
loss:0.65662
loss:0.66272
loss:0.63811
loss:0.64877
loss:0.64044
loss:0.66872
loss:0.64043
loss:0.65573
loss:0.66213
loss:0.65970
loss:0.66614
loss:0.63885
loss:0.66052
loss:0.65201
loss:0.66082
loss:0.64609
loss:0.66620
loss:0.63829
loss:0.65079
loss:0.65649
loss:0.66343
loss:0.62930
loss:0.65037
loss:0.64753
loss:0.64312
loss:0.66374
loss:0.66796
loss:0.65255
loss:0.66076
loss:0.63404
loss:0.63195
loss:0.64329
loss:0.65030
loss:0.65947
loss:0.66701
loss:0.66491
loss:0.63622
loss:0.64002
loss:0.66333
loss:0.66137
loss:0.65484
loss:0.65512
loss:0.65480
loss:0.65607
loss:0.65092
loss:0.64587
loss:0.65698
loss:0.65885
loss:0.65680
loss:0.65483
loss:0.64723
loss:0.64904
loss:0.64924
loss:0.65387
loss:0.66436
loss:0.64569
loss:0.65370
loss:0.64694
loss:0.66056
loss:0.65969
loss:0.66932
loss:0.66629
loss:0.63691
loss:0.65005
loss:0.66559
loss:0.64883
loss:0.68294
loss:0.63973
loss:0.65202
loss:0.65970
loss:0.64948
loss:0.65405
loss:0.64573
loss:0.65429
loss:0.64927
loss:0.66474
loss:0.67271
loss:0.65557
loss:0.64840
loss:0.64721
loss:0.63320
loss:0.64397
loss:0.64200
loss:0.65974
loss:0.66676
loss:0.63872
loss:0.62732
loss:0.66117
loss:0.66287
loss:0.65499
loss:0.65296
loss:0.64279
loss:0.67587
loss:0.65342
loss:0.64904
loss:0.67172
loss:0.64584
loss:0.64684
loss:0.64587
loss:0.67091
loss:0.65719
loss:0.65750
loss:0.64885
loss:0.63925
loss:0.66039
loss:0.66144
loss:0.65915
loss:0.66631
loss:0.65933
loss:0.65010
loss:0.65409
loss:0.65785
loss:0.65928
loss:0.65304
loss:0.66881
loss:0.64203
loss:0.61528
loss:0.64760
loss:0.66234
loss:0.64173
loss:0.64257
loss:0.64856
loss:0.64720
loss:0.64135
loss:0.64035
loss:0.64340
loss:0.63610
loss:0.64845
loss:0.67695
loss:0.63864
loss:0.66180
loss:0.65755
loss:0.63133
loss:0.65347
loss:0.65438
loss:0.64419
loss:0.65397
loss:0.64647
loss:0.63432
loss:0.64720
loss:0.65999
loss:0.64452
loss:0.62469
loss:0.66050
loss:0.64571
loss:0.64916
loss:0.62766
loss:0.66855
loss:0.62570
loss:0.65036
loss:0.66567
loss:0.64379
loss:0.66724
loss:0.65184
loss:0.64727
loss:0.68236
loss:0.65045
loss:0.63061
loss:0.63256
loss:0.63814
loss:0.64611
loss:0.63597
loss:0.63543
loss:0.64988
loss:0.64666
loss:0.65303
loss:0.63639
loss:0.64793
loss:0.64741
loss:0.67225
loss:0.64893
loss:0.66021
loss:0.67220
loss:0.65842
loss:0.65558
loss:0.65811
loss:0.65474
loss:0.65929
loss:0.65540
loss:0.63822
loss:0.66122
loss:0.64008
loss:0.63420
loss:0.64932
loss:0.65861
loss:0.63835
loss:0.65324
loss:0.61991
loss:0.63707
loss:0.64991
loss:0.67095
loss:0.64615
loss:0.65689
loss:0.64816
loss:0.65301
loss:0.65385
loss:0.64609
loss:0.67575
loss:0.64710
loss:0.64540
loss:0.64559
loss:0.62836
loss:0.64454
loss:0.64238
loss:0.64704
loss:0.66270
loss:0.64462
loss:0.64719
loss:0.65128
loss:0.63738
loss:0.65601
loss:0.65367
loss:0.64710
loss:0.64753
loss:0.63913
loss:0.66223
loss:0.62805
loss:0.63795
loss:0.65797
loss:0.65121
loss:0.66880
loss:0.65317
loss:0.64136
loss:0.64336
loss:0.65911
loss:0.65885
loss:0.65274
loss:0.65666
loss:0.66065
loss:0.62155
loss:0.65305
loss:0.66328
loss:0.65245
loss:0.65728
loss:0.64594
loss:0.66519
loss:0.65162
loss:0.66242
loss:0.62365
loss:0.65733
loss:0.66757
loss:0.65935
loss:0.63079
loss:0.64776
loss:0.63078
loss:0.64948
loss:0.66126
loss:0.66215
loss:0.65138
loss:0.65687
loss:0.65287
loss:0.65589
loss:0.66195
loss:0.66097
loss:0.67118
loss:0.64370
loss:0.63674
loss:0.65073
loss:0.65066
loss:0.64303
loss:0.63949
loss:0.63744
loss:0.66521
loss:0.64809
loss:0.63259
loss:0.65497
loss:0.63504
loss:0.63859
loss:0.66908
loss:0.66095
loss:0.64357
loss:0.65790
loss:0.65353
loss:0.66009
loss:0.64568
loss:0.65365
loss:0.66244
loss:0.65174
loss:0.64045
loss:0.64923
loss:0.64101
loss:0.64019
loss:0.66645
loss:0.63986
loss:0.66017
loss:0.65281
loss:0.65101
loss:0.66715
loss:0.65550
loss:0.64694
loss:0.64353
loss:0.65069
loss:0.65411
loss:0.65754
loss:0.64164
loss:0.63184
loss:0.64650
loss:0.64047
loss:0.63702
loss:0.63785
loss:0.65339
loss:0.65018
loss:0.66550
loss:0.65948
loss:0.64607
loss:0.65500
loss:0.63153
loss:0.62435
loss:0.65378
loss:0.64712
loss:0.62710
loss:0.62533
loss:0.63975
loss:0.66521
loss:0.67130
loss:0.66886
loss:0.64557
loss:0.64156
loss:0.65914
loss:0.63111
loss:0.63072
loss:0.66098
loss:0.66197
loss:0.64497
loss:0.65991
loss:0.65800
loss:0.64330
loss:0.64686
loss:0.64212
loss:0.65413
loss:0.65444
loss:0.65896
loss:0.66093
loss:0.66395
loss:0.64697
loss:0.64075
loss:0.63737
loss:0.64562
loss:0.64184
loss:0.64463
loss:0.64311
loss:0.65048
loss:0.63703
loss:0.64148
loss:0.66401
loss:0.67621
loss:0.64686
loss:0.64019
loss:0.62544
loss:0.64243
loss:0.63499
loss:0.66917
loss:0.64438
loss:0.65681
loss:0.64320
loss:0.65792
loss:0.65856
loss:0.63190
loss:0.63908
loss:0.65163
loss:0.66037
loss:0.65044
loss:0.63695
loss:0.64864
loss:0.64876
loss:0.63734
loss:0.63912
loss:0.64230
loss:0.65650
loss:0.65003
loss:0.66178
loss:0.63114
loss:0.65799
loss:0.65402
loss:0.64469
loss:0.62906
loss:0.64209
loss:0.64303
loss:0.64273
loss:0.64660
loss:0.64389
loss:0.64154
loss:0.65484
loss:0.67789
loss:0.65224
loss:0.65814
loss:0.65039
loss:0.65852
loss:0.62646
loss:0.64415
loss:0.64463
loss:0.66715
loss:0.66428
loss:0.64230
loss:0.66541
loss:0.64076
loss:0.63980
loss:0.64539
loss:0.65050
loss:0.66136
loss:0.63976
loss:0.62993
loss:0.63705
loss:0.65366
loss:0.63776
loss:0.62552
loss:0.63873
loss:0.65481
loss:0.64907
loss:0.63867
loss:0.66698
loss:0.66325
loss:0.65652
loss:0.64738
loss:0.63431
loss:0.64878
loss:0.65649
loss:0.67702
loss:0.64277
loss:0.63714
loss:0.65167
loss:0.64489
loss:0.65375
loss:0.65780
loss:0.63697
loss:0.67095
loss:0.66378
loss:0.65183
loss:0.66371
loss:0.65350
loss:0.63916
loss:0.64360
loss:0.63596
loss:0.64348
loss:0.65892
loss:0.65130
loss:0.64690
loss:0.65024
loss:0.63878
loss:0.67344
loss:0.66832
loss:0.66636
Evaluation at Epoch 4/4. Step:11072/11072. AccuracyMetric: acc=0.567497


In Epoch:2/Step:5536, got best dev performance:AccuracyMetric: acc=0.576429
Reloaded the best model.
Train Done.
[tester] 
AccuracyMetric: acc=0.576429
Test Done.
Predict the answer with best model...
(44772, 2)
Predict Done. 5730816 records
true sample count:21513
Add 0 default result in predict.
Predict Done, results saved to ../../data/results/CNNText_essay_at_least_1_green_donation
