[34mAuto commit by fitlog[0m
Running with args:Namespace(batch_size=320, cuda=True, data_src='all_data', dropout=0.2, embed_size=128, epochs=10, gpu='1', hidden_size=128, learning_rate=0.001, log_path='./run_records.log', max_seq_len=500, min_count=10, model='CNNText', model_dir='../../data/models', model_suffix='oversampling', num_layers=2, optim='Adam', patience=10, predict=True, prepare=False, prepare_dir='../../data/prepare/', pretrain=False, pretrain_model='None', reload_model_name='best_LSTMText_accuracy_2019-06-14-04-01-48', result_dir='../../data/results/', show_data=False, train=False, vocab_data='vocab_oversampling.data', vocab_dir='../../data/vocab', weight_decay=0.0)
Checking the data files...
(vocab_size,class_num,seq_len):(37272,2,1070)
test_size:43444
Data type:<class 'fastNLP.core.dataset.DataSet'>
Loading the model ../../data/models/CNNText/oversampling/best_LSTMText_accuracy_2019-06-14-04-01-48
[34mAuto commit by fitlog[0m
Running with args:Namespace(batch_size=320, cuda=True, data_src='all_data', dropout=0.2, embed_size=128, epochs=10, gpu='1', hidden_size=128, learning_rate=0.001, log_path='./run_records.log', max_seq_len=500, min_count=10, model='CNNText', model_dir='../../data/models', model_suffix='oversampling', num_layers=2, optim='Adam', patience=10, predict=False, prepare=False, prepare_dir='../../data/prepare/', pretrain=False, pretrain_model='None', reload_model_name='best_LSTMText_accuracy_2019-06-14-04-01-48', result_dir='../../data/results/', show_data=False, train=True, vocab_data='vocab_oversampling.data', vocab_dir='../../data/vocab', weight_decay=0.0)
Checking the data files...
(vocab_size,class_num,seq_len):(37272,2,1070)
No pretrained model with be used.
vocabsize:37272
Using CNN Model.
CNNText(
  (embed): Embedding(
    37272, 128
    (dropout): Dropout(p=0.0)
  )
  (conv_pool): ConvMaxpool(
    (convs): ModuleList(
      (0): Conv1d(128, 3, kernel_size=(3,), stride=(1,), padding=(2,))
      (1): Conv1d(128, 4, kernel_size=(4,), stride=(1,), padding=(2,))
      (2): Conv1d(128, 5, kernel_size=(5,), stride=(1,), padding=(2,))
    )
  )
  (dropout): Dropout(p=0.2)
  (fc): Linear(in_features=12, out_features=2, bias=True)
)
train_size:526406 ; val_size:228524 ; test_size:43444
Using Adam as optimizer.
input fields after batch(if batch size is 2):
	words: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2, 1070]) 
	seq_len: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2]) 
target fields after batch(if batch size is 2):
	target: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2]) 

training epochs started 2019-06-15-03-25-51
loss:1.84851
loss:1.35480
loss:1.05808
loss:1.04253
loss:1.02056
loss:1.02023
loss:0.98011
loss:0.92244
loss:0.90856
loss:0.90636
loss:0.90130
loss:0.88035
loss:0.87187
loss:0.83641
loss:0.85432
loss:0.86124
loss:0.80542
loss:0.82826
loss:0.81072
loss:0.80272
loss:0.78169
loss:0.78640
loss:0.76146
loss:0.73701
loss:0.76775
loss:0.76122
loss:0.75986
loss:0.73317
loss:0.68920
loss:0.74120
loss:0.74514
loss:0.74491
loss:0.72368
loss:0.72174
loss:0.72455
loss:0.71022
loss:0.72352
loss:0.72351
loss:0.70158
loss:0.69538
loss:0.69740
loss:0.71062
loss:0.70645
loss:0.71121
loss:0.68591
loss:0.68974
loss:0.69559
loss:0.69898
loss:0.69272
loss:0.69061
loss:0.69765
loss:0.69946
loss:0.68073
loss:0.69459
loss:0.68702
loss:0.68544
loss:0.69545
loss:0.67647
loss:0.68660
loss:0.66998
loss:0.66127
loss:0.67863
loss:0.66250
loss:0.69065
loss:0.65436
loss:0.67158
loss:0.67130
loss:0.66948
loss:0.68133
loss:0.65640
loss:0.66770
loss:0.68108
loss:0.66990
loss:0.66275
loss:0.65701
loss:0.64616
loss:0.66544
loss:0.65635
loss:0.65118
loss:0.66051
loss:0.65757
loss:0.65483
loss:0.66068
loss:0.63671
loss:0.63357
loss:0.65470
loss:0.66113
loss:0.63735
loss:0.65585
loss:0.65279
loss:0.65701
loss:0.66550
loss:0.65407
loss:0.64480
loss:0.64506
loss:0.65975
loss:0.64735
loss:0.63906
loss:0.65513
loss:0.63138
loss:0.65394
loss:0.64091
loss:0.63892
loss:0.63584
loss:0.64533
loss:0.64483
loss:0.67097
loss:0.64259
loss:0.65084
loss:0.64543
loss:0.64365
loss:0.62697
loss:0.63507
loss:0.64044
loss:0.63838
loss:0.63630
loss:0.64330
loss:0.65628
loss:0.64988
loss:0.64470
loss:0.62811
loss:0.62885
loss:0.63869
loss:0.63440
loss:0.64095
loss:0.64548
loss:0.64839
loss:0.65449
loss:0.64362
loss:0.63501
loss:0.63496
loss:0.64691
loss:0.63398
loss:0.64421
loss:0.64845
loss:0.64386
loss:0.63666
loss:0.64977
loss:0.63321
loss:0.64488
loss:0.64714
loss:0.63552
loss:0.63388
loss:0.63144
loss:0.63756
loss:0.63281
loss:0.63519
loss:0.64419
loss:0.64526
loss:0.62988
loss:0.63537
loss:0.63696
loss:0.63035
loss:0.63123
loss:0.64646
loss:0.63068
loss:0.62441
loss:0.63212
loss:0.64350
loss:0.63905
loss:0.64470
loss:0.62865
loss:0.63784
loss:0.64189
loss:0.63226
loss:0.63410
loss:0.63143
loss:0.62858
loss:0.65026
loss:0.63599
loss:0.64362
loss:0.63749
loss:0.63905
loss:0.63702
loss:0.63050
loss:0.63440
loss:0.63278
loss:0.63927
loss:0.62784
loss:0.62649
loss:0.63533
loss:0.61399
loss:0.63873
loss:0.62954
loss:0.62084
loss:0.63584
loss:0.63207
loss:0.63733
loss:0.64120
loss:0.64150
loss:0.62843
loss:0.63570
loss:0.62460
loss:0.62005
loss:0.61947
loss:0.63940
loss:0.63638
loss:0.63679
loss:0.61738
loss:0.63180
loss:0.63984
loss:0.63112
loss:0.64223
loss:0.64576
loss:0.62752
loss:0.63977
loss:0.62469
loss:0.62764
loss:0.63610
loss:0.61406
loss:0.61752
loss:0.63031
loss:0.62291
loss:0.61885
loss:0.62504
loss:0.62774
loss:0.63038
loss:0.62688
loss:0.62095
loss:0.61192
loss:0.61631
loss:0.61766
loss:0.61654
loss:0.63001
loss:0.63383
loss:0.62293
loss:0.63458
loss:0.62601
loss:0.62534
loss:0.63390
loss:0.62781
loss:0.61973
loss:0.62089
loss:0.62122
loss:0.62890
loss:0.63138
loss:0.62328
loss:0.63762
loss:0.63573
loss:0.61537
loss:0.61766
loss:0.61442
loss:0.61815
loss:0.61160
loss:0.62199
loss:0.61502
loss:0.64874
loss:0.62026
loss:0.61246
loss:0.61134
loss:0.62434
loss:0.62449
loss:0.62532
loss:0.62317
loss:0.61351
loss:0.60209
loss:0.63026
loss:0.63576
loss:0.62944
loss:0.62772
loss:0.63766
loss:0.62134
loss:0.61937
loss:0.62302
loss:0.62547
loss:0.60021
loss:0.61903
loss:0.61580
loss:0.63367
loss:0.61463
loss:0.61008
loss:0.61766
loss:0.61906
loss:0.61958
loss:0.61353
loss:0.61217
loss:0.61294
loss:0.61756
loss:0.62833
loss:0.61557
loss:0.62983
loss:0.61555
loss:0.59576
loss:0.61622
loss:0.62059
loss:0.62667
loss:0.61662
loss:0.61025
loss:0.60687
loss:0.61041
loss:0.60557
loss:0.62131
loss:0.60976
loss:0.61553
loss:0.62046
loss:0.60201
loss:0.62592
loss:0.59726
loss:0.61819
loss:0.63237
loss:0.60781
loss:0.61016
loss:0.61019
loss:0.61359
loss:0.60189
loss:0.60623
loss:0.61226
loss:0.61216
loss:0.61124
loss:0.61555
loss:0.60236
loss:0.61318
loss:0.59977
loss:0.60016
loss:0.59493
loss:0.60820
loss:0.59601
loss:0.60113
loss:0.60982
loss:0.59560
loss:0.61041
loss:0.59318
loss:0.62602
loss:0.61695
loss:0.60359
loss:0.61460
loss:0.59780
loss:0.61659
loss:0.60184
Evaluation at Epoch 1/10. Step:1646/16460. AccuracyMetric: acc=0.529279

loss:0.59022
loss:0.58623
loss:0.60695
loss:0.60299
loss:0.58867
loss:0.61343
loss:0.60549
loss:0.59607
loss:0.59348
loss:0.59662
loss:0.60855
loss:0.59122
loss:0.60267
loss:0.60525
loss:0.59179
loss:0.59396
loss:0.61089
loss:0.58539
loss:0.58737
loss:0.60363
loss:0.60022
loss:0.58837
loss:0.58852
loss:0.58263
loss:0.61226
loss:0.60115
loss:0.59588
loss:0.59137
loss:0.58359
loss:0.58550
loss:0.58693
loss:0.60155
loss:0.58696
loss:0.59865
loss:0.57892
loss:0.58369
loss:0.59406
loss:0.58458
loss:0.59132
loss:0.58746
loss:0.59352
loss:0.58788
loss:0.58314
loss:0.60760
loss:0.59445
loss:0.58997
loss:0.56689
loss:0.57254
loss:0.58604
loss:0.60296
loss:0.58726
loss:0.60285
loss:0.57420
loss:0.60307
loss:0.57657
loss:0.58057
loss:0.58608
loss:0.57697
loss:0.58507
loss:0.58379
loss:0.58993
loss:0.59382
loss:0.59786
loss:0.58839
loss:0.57706
loss:0.58630
loss:0.56056
loss:0.59145
loss:0.56879
loss:0.57057
loss:0.60452
loss:0.58519
loss:0.59641
loss:0.59252
loss:0.57226
loss:0.57456
loss:0.57135
loss:0.57621
loss:0.57654
loss:0.57478
loss:0.57916
loss:0.58895
loss:0.57381
loss:0.55261
loss:0.56738
loss:0.58237
loss:0.58054
loss:0.58956
loss:0.57750
loss:0.57801
loss:0.57604
loss:0.59190
loss:0.58048
loss:0.55541
loss:0.56033
loss:0.57901
loss:0.57038
loss:0.57969
loss:0.58148
loss:0.57119
loss:0.57100
loss:0.56650
loss:0.56406
loss:0.56646
loss:0.57067
loss:0.55649
loss:0.57910
loss:0.58046
loss:0.56465
loss:0.56391
loss:0.57275
loss:0.57014
loss:0.55940
loss:0.53818
loss:0.58258
loss:0.54751
loss:0.58241
loss:0.58802
loss:0.55068
loss:0.56189
loss:0.57654
loss:0.56083
loss:0.56798
loss:0.57246
loss:0.57248
loss:0.57055
loss:0.56145
loss:0.56649
loss:0.58282
loss:0.58002
loss:0.57414
loss:0.54554
loss:0.55256
loss:0.56543
loss:0.55000
loss:0.54828
loss:0.56217
loss:0.57853
loss:0.56868
loss:0.58337
loss:0.56437
loss:0.54342
loss:0.55872
loss:0.55168
loss:0.58783
loss:0.54559
loss:0.55378
loss:0.55865
loss:0.55744
loss:0.55803
loss:0.55647
loss:0.57246
loss:0.55547
loss:0.57033
loss:0.55033
loss:0.58340
loss:0.58485
loss:0.56420
loss:0.56109
loss:0.57084
loss:0.56707
loss:0.56915
loss:0.55471
loss:0.54913
loss:0.56519
loss:0.57566
loss:0.56910
loss:0.55507
loss:0.57756
loss:0.57290
loss:0.56087
loss:0.54591
loss:0.54468
loss:0.54831
loss:0.54786
loss:0.55840
loss:0.55647
loss:0.56401
loss:0.55302
loss:0.56920
loss:0.55415
loss:0.54366
loss:0.54859
loss:0.56535
loss:0.56154
loss:0.54998
loss:0.53987
loss:0.53867
loss:0.54314
loss:0.54962
loss:0.56174
loss:0.55943
loss:0.54061
loss:0.53898
loss:0.53315
loss:0.54480
loss:0.57845
loss:0.51911
loss:0.54659
loss:0.53940
loss:0.54567
loss:0.54771
loss:0.55187
loss:0.54800
loss:0.53396
loss:0.55969
loss:0.55869
loss:0.54628
loss:0.54184
loss:0.54487
loss:0.53690
loss:0.55260
loss:0.53775
loss:0.53637
loss:0.55449
loss:0.55236
loss:0.53978
loss:0.56190
loss:0.55456
loss:0.54226
loss:0.54659
loss:0.53747
loss:0.52704
loss:0.54473
loss:0.52762
loss:0.54170
loss:0.54094
loss:0.54489
loss:0.54591
loss:0.54726
loss:0.54679
loss:0.55182
loss:0.55202
loss:0.56270
loss:0.53872
loss:0.53216
loss:0.54825
loss:0.54810
loss:0.53150
loss:0.52402
loss:0.55752
loss:0.53390
loss:0.54637
loss:0.53317
loss:0.55850
loss:0.53888
loss:0.53519
loss:0.55307
loss:0.52734
loss:0.54069
loss:0.53752
loss:0.52601
loss:0.53037
loss:0.53207
loss:0.53627
loss:0.54348
loss:0.52723
loss:0.54184
loss:0.52979
loss:0.52407
loss:0.52930
loss:0.50849
loss:0.51596
loss:0.52462
loss:0.55011
loss:0.54580
loss:0.51664
loss:0.53793
loss:0.53703
loss:0.51719
loss:0.53422
loss:0.53427
loss:0.52790
loss:0.53084
loss:0.54286
loss:0.53151
loss:0.53772
loss:0.53163
loss:0.53495
loss:0.51741
loss:0.52839
loss:0.55410
loss:0.53692
loss:0.54804
loss:0.51223
loss:0.53061
loss:0.53649
loss:0.52661
loss:0.52113
loss:0.52903
loss:0.52325
loss:0.51984
loss:0.51081
loss:0.52410
loss:0.49344
loss:0.51662