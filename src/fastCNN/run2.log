[34mAuto commit by fitlog[0m
(vocab_size,class_num,seq_len):(27658,2,1070)
No pretrained model with be used.
vocabsize:27658
Using user defined CNNText
MyCNNText(
  (embeds): Embedding(27658, 128)
  (convs1): ModuleList(
    (0): Conv2d(1, 3, kernel_size=(3, 128), stride=(1, 1))
    (1): Conv2d(1, 4, kernel_size=(4, 128), stride=(1, 1))
    (2): Conv2d(1, 5, kernel_size=(5, 128), stride=(1, 1))
  )
  (convs2): ModuleList(
    (0): Conv1d(128, 3, kernel_size=(3,), stride=(1,), padding=(2,))
    (1): Conv1d(128, 4, kernel_size=(4,), stride=(1,), padding=(2,))
    (2): Conv1d(128, 5, kernel_size=(5,), stride=(1,), padding=(2,))
  )
  (dropout): Dropout(p=0.2)
  (fc): Linear(in_features=12, out_features=2, bias=True)
)
train_size:285354 ; val_size:128282 ; test_size:43444
Using Adam as optimizer.
input fields after batch(if batch size is 2):
	words: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2, 1070]) 
	seq_len: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2]) 
target fields after batch(if batch size is 2):
	target: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2]) 

training epochs started 2019-06-14-03-16-47
loss:0.34249
loss:0.28609
loss:0.31577
loss:0.30930
loss:0.27850
loss:0.27149
loss:0.30896
loss:0.26033
loss:0.31363
loss:0.29084
loss:0.28823
loss:0.28192
loss:0.32067
loss:0.30427
loss:0.28878
loss:0.29836
loss:0.27855
loss:0.27698
loss:0.26078
loss:0.30844
loss:0.28300
loss:0.26455
loss:0.27686
loss:0.28449
loss:0.26859
loss:0.28048
loss:0.27885
loss:0.30815
loss:0.25830
loss:0.28673
loss:0.26353
loss:0.27848
loss:0.28922
loss:0.28037
loss:0.26948
loss:0.29521
loss:0.27111
loss:0.27182
loss:0.30506
loss:0.26195
loss:0.25758
loss:0.29443
loss:0.29935
loss:0.28503
loss:0.30683
loss:0.27281
loss:0.26091
loss:0.31982
loss:0.30505
loss:0.28228
loss:0.29818
loss:0.30357
loss:0.27954
loss:0.28908
loss:0.25059
loss:0.29202
loss:0.27324
loss:0.26284
loss:0.32176
loss:0.28546
loss:0.25579
loss:0.28056
loss:0.28264
loss:0.28205
loss:0.25453
loss:0.26700
loss:0.28486
loss:0.28312
loss:0.31445
loss:0.27938
loss:0.31660
loss:0.29279
loss:0.27138
loss:0.26442
loss:0.29738
loss:0.27517
loss:0.29409
loss:0.29906
loss:0.30599
loss:0.30548
loss:0.28529
loss:0.26345
loss:0.29153
loss:0.29325
loss:0.26464
loss:0.28740
loss:0.28015
loss:0.30187
loss:0.23366
loss:0.28807
loss:0.28716
loss:0.26173
loss:0.27541
loss:0.30655
loss:0.26058
loss:0.24750
loss:0.26836
loss:0.28290
loss:0.27370
loss:0.27596
loss:0.27447
loss:0.27262
loss:0.28358
loss:0.29259
loss:0.26814
loss:0.27603
loss:0.28925
loss:0.26484
loss:0.27884
loss:0.29261
loss:0.27403
loss:0.27062
loss:0.27842
loss:0.28089
loss:0.27016
loss:0.27917
loss:0.26743
loss:0.28470
loss:0.28459
loss:0.30526
loss:0.27041
loss:0.26341
loss:0.27370
loss:0.28322
loss:0.28417
loss:0.28993
loss:0.27721
loss:0.26508
loss:0.28264
loss:0.27455
loss:0.26900
loss:0.31469
loss:0.26608
loss:0.24059
loss:0.28114
loss:0.28210
loss:0.27909
loss:0.27546
loss:0.28929
loss:0.28871
loss:0.24377
loss:0.27034
loss:0.26058
loss:0.27111
loss:0.26996
loss:0.27165
loss:0.28092
loss:0.29631
loss:0.26416
loss:0.28205
loss:0.27572
loss:0.30545
loss:0.28081
loss:0.25319
loss:0.26794
loss:0.26971
loss:0.26692
loss:0.28031
loss:0.28936
loss:0.27131
loss:0.29201
loss:0.31065
loss:0.29397
loss:0.28419
loss:0.28507
loss:0.30344
loss:0.24996
loss:0.26810
loss:0.27548
loss:0.27703
loss:0.30898
loss:0.28245
loss:0.28112
loss:0.28789
loss:0.27563
loss:0.28537
loss:0.30503
loss:0.27076
Evaluation at Epoch 1/20. Step:892/17840. AccuracyMetric: acc=0.89071

loss:0.27910
loss:0.28283
loss:0.28492
loss:0.26839
loss:0.27126
loss:0.28231
loss:0.27248
loss:0.24993
loss:0.27818
loss:0.29987
loss:0.25528
loss:0.28198
loss:0.30214
loss:0.29712
loss:0.26430
loss:0.26219
loss:0.26205
loss:0.25220
loss:0.27201
loss:0.31268
loss:0.26458
loss:0.27072
loss:0.24737
loss:0.28911
loss:0.28141
loss:0.29277
loss:0.27856
loss:0.25114
loss:0.27189
loss:0.26917
loss:0.27618
loss:0.28224
loss:0.25991
loss:0.25934
loss:0.26037
loss:0.26453
loss:0.28377
loss:0.27025
loss:0.32125
loss:0.28672
loss:0.27968
loss:0.28728
loss:0.31479
loss:0.29773
loss:0.27999
loss:0.28343
loss:0.27574
loss:0.26193
loss:0.27263
loss:0.26546
loss:0.26527
loss:0.24779
loss:0.28405
loss:0.25198
loss:0.30873
loss:0.27497
loss:0.27820
loss:0.29049
loss:0.25901
loss:0.28721
loss:0.26479
loss:0.29531
loss:0.29076
loss:0.26029
loss:0.26715
loss:0.27928
loss:0.27923
loss:0.25289
loss:0.27691
loss:0.26463
loss:0.26458
loss:0.27867
loss:0.25911
loss:0.26686
loss:0.25100
loss:0.31609
loss:0.24345
loss:0.28941
loss:0.24820
loss:0.27946
loss:0.30589
loss:0.26408
loss:0.29638
loss:0.28530
loss:0.27202
loss:0.30652
loss:0.26111
loss:0.28039
loss:0.30335
loss:0.29074
loss:0.26817
loss:0.27262
loss:0.25735
loss:0.27089
loss:0.29337
loss:0.24168
loss:0.24374
loss:0.31458
loss:0.26788
loss:0.25878
loss:0.29806
loss:0.31529
loss:0.26290
loss:0.27264
loss:0.25792
loss:0.29984
loss:0.25344
loss:0.28116
loss:0.23480
loss:0.26471
loss:0.27713
loss:0.26680
loss:0.26637
loss:0.25864
loss:0.27159
loss:0.26321
loss:0.27619
loss:0.28124
loss:0.25733
loss:0.28905
loss:0.26627
loss:0.26664
loss:0.28697
loss:0.27872
loss:0.29199
loss:0.23858
loss:0.27558
loss:0.26245
loss:0.27096
loss:0.27967
loss:0.29723
loss:0.27498
loss:0.28001
loss:0.28216
loss:0.28191
loss:0.29589
loss:0.27501
loss:0.25131
loss:0.25903
loss:0.27950
loss:0.24584
loss:0.28598
loss:0.26323
loss:0.27070
loss:0.25766
loss:0.29273
loss:0.28743
loss:0.25572
loss:0.27846
loss:0.27117
loss:0.27396
loss:0.27934
loss:0.25377
loss:0.26077
loss:0.26928
loss:0.27151
loss:0.27138
loss:0.28122
loss:0.29049
loss:0.26001
loss:0.28545
loss:0.26505
loss:0.23827
loss:0.27857
loss:0.28517
loss:0.27366
loss:0.29595
loss:0.27305
loss:0.29110
loss:0.27686
loss:0.27592
loss:0.26841
loss:0.27688
loss:0.28752
loss:0.25478
loss:0.26386
loss:0.27685
loss:0.28185
Evaluation at Epoch 2/20. Step:1784/17840. AccuracyMetric: acc=0.89071

loss:0.28494
loss:0.26630
loss:0.27578
loss:0.23377
loss:0.27585
loss:0.28547
loss:0.28843
loss:0.26718
loss:0.28961
loss:0.25298
loss:0.26452
loss:0.28069
loss:0.24415
loss:0.24319
loss:0.26936
loss:0.27638
loss:0.27292
loss:0.29850
loss:0.29342
loss:0.24806
loss:0.31344
loss:0.28069
loss:0.24012
loss:0.23775
loss:0.29876
loss:0.28588
loss:0.28412
loss:0.29506
loss:0.26609
loss:0.29253
loss:0.29574
loss:0.26947
loss:0.27393
loss:0.29881
loss:0.29302
loss:0.26125
loss:0.25807
loss:0.25910
loss:0.29405
loss:0.21895
loss:0.29385
loss:0.29429
loss:0.25790
loss:0.25008
loss:0.31214
loss:0.26030
loss:0.29717
loss:0.26829
loss:0.27196
loss:0.26123
loss:0.27854
loss:0.25460
loss:0.31638
loss:0.24250
loss:0.29713
loss:0.27914
loss:0.24845
loss:0.27008
loss:0.26545
loss:0.27270
loss:0.25962
loss:0.25634
loss:0.26371
loss:0.26425
loss:0.29248
loss:0.25063
loss:0.27877
loss:0.25916
loss:0.28346
loss:0.27060
loss:0.26343
loss:0.27868
loss:0.25923
loss:0.26806
loss:0.28225
loss:0.28240
loss:0.27135
loss:0.27357
loss:0.26690
loss:0.27562
loss:0.28130
loss:0.25007
loss:0.27220
loss:0.26526
loss:0.28441
loss:0.28222
loss:0.28529
loss:0.27596
loss:0.24615
loss:0.24741
loss:0.26586
loss:0.28101
loss:0.27970
loss:0.26932
loss:0.28970
loss:0.27543
loss:0.26842
loss:0.27109
loss:0.26937
loss:0.23398
loss:0.27566
loss:0.28064
loss:0.28921
loss:0.28652
loss:0.27496
loss:0.27007
loss:0.27373
loss:0.27998
loss:0.28920
loss:0.27399
loss:0.25280
loss:0.26274
loss:0.28254
loss:0.28475
loss:0.29339
loss:0.24497
loss:0.28094
loss:0.27493
loss:0.29339
loss:0.26543
loss:0.24206
loss:0.27709
loss:0.26920
loss:0.28812
loss:0.27829
loss:0.29534
loss:0.30144
loss:0.27171
loss:0.28270
loss:0.27366
loss:0.28579
loss:0.27827
loss:0.26979
loss:0.26827
loss:0.27746
loss:0.27002
loss:0.27620
loss:0.25623
loss:0.26953
loss:0.26898
loss:0.30313
loss:0.25387
loss:0.29377
loss:0.28164
loss:0.27132
loss:0.26525
loss:0.26321
loss:0.26470
loss:0.26997
loss:0.28004
loss:0.27658
loss:0.28924
loss:0.27686
loss:0.28157
loss:0.27730
loss:0.27252
loss:0.28652
loss:0.23446
loss:0.28275
loss:0.27090
loss:0.27968
loss:0.27651
loss:0.25047
loss:0.24377
loss:0.28013
loss:0.28218
loss:0.26619
loss:0.24715
loss:0.26761
loss:0.29069
loss:0.27187
loss:0.26481
loss:0.25915
loss:0.25989
loss:0.26438
loss:0.25717
loss:0.25669
loss:0.28178
loss:0.25592
Evaluation at Epoch 3/20. Step:2676/17840. AccuracyMetric: acc=0.89071

loss:0.29432
loss:0.27628
loss:0.26656
loss:0.29196
loss:0.28615
loss:0.27752
loss:0.28062
loss:0.27406
loss:0.25363
loss:0.27820
loss:0.29167
loss:0.27482
loss:0.24662
loss:0.23059
loss:0.29206
loss:0.26206
loss:0.26988
loss:0.24867
loss:0.27182
loss:0.26021
loss:0.27286
loss:0.28084
loss:0.29317
loss:0.29159
loss:0.26348
loss:0.28951
loss:0.27316
loss:0.25763
loss:0.27327
loss:0.23636
loss:0.25358
loss:0.25412
loss:0.26980
loss:0.24578
loss:0.29994
loss:0.26358
loss:0.26676
loss:0.28411
loss:0.26360
loss:0.26216
loss:0.26630
loss:0.27571
loss:0.26480
loss:0.31587
loss:0.24474
loss:0.24614
loss:0.28547
loss:0.25114
loss:0.24904
loss:0.26345
loss:0.29137
loss:0.25765
loss:0.25846
loss:0.28581
loss:0.26184
loss:0.27071
loss:0.26140
loss:0.26635
loss:0.26703
loss:0.25126
loss:0.22912
loss:0.26299
loss:0.27315
loss:0.25268
loss:0.29702
loss:0.27883
loss:0.26287
loss:0.30244
loss:0.26101
loss:0.26339
loss:0.26936
loss:0.26153
loss:0.28281
loss:0.26619
loss:0.24828
loss:0.26912
loss:0.29054
loss:0.27407
loss:0.29592
loss:0.28200
loss:0.27033
loss:0.26614
loss:0.25863
loss:0.27718
loss:0.26415
loss:0.30647
loss:0.28254
loss:0.28307
loss:0.26130
loss:0.27788
loss:0.26358
loss:0.27413
loss:0.28217
loss:0.27881
loss:0.24626
loss:0.24401
loss:0.29128
loss:0.29118
loss:0.27011
loss:0.26660
loss:0.27609
loss:0.24842
loss:0.25939
loss:0.23992
loss:0.26410
loss:0.29255
loss:0.26857
loss:0.24086
loss:0.28457
loss:0.26460
loss:0.29208
loss:0.27217
loss:0.26146
loss:0.26730
loss:0.25681
loss:0.30963
loss:0.27271
loss:0.27555
loss:0.26620
loss:0.27403
loss:0.26745
loss:0.26824
loss:0.26560
loss:0.28238
loss:0.28405
loss:0.27509
loss:0.27241
loss:0.29515
loss:0.28196
loss:0.24078
loss:0.25197
loss:0.26502
loss:0.27300
loss:0.24323
loss:0.26136
loss:0.25270
loss:0.28937
loss:0.27000
loss:0.25808
loss:0.29464
loss:0.27028
loss:0.25540
loss:0.28405
loss:0.26726
loss:0.29075
loss:0.27533
loss:0.26292
loss:0.28061
loss:0.24646
loss:0.27336
loss:0.27401
loss:0.24907
loss:0.26742
loss:0.29679
loss:0.28245
loss:0.28288
loss:0.26609
loss:0.27024
loss:0.25461
loss:0.26620
loss:0.24350
loss:0.25935
loss:0.26202
loss:0.26172
loss:0.27505
loss:0.29970
loss:0.26900
loss:0.27429
loss:0.27049
loss:0.24997
loss:0.30273
loss:0.28794
loss:0.25666
loss:0.24798
loss:0.26852
loss:0.29336
loss:0.26430
loss:0.23272
Evaluation at Epoch 4/20. Step:3568/17840. AccuracyMetric: acc=0.89071

loss:0.28203
loss:0.25611
loss:0.27656
loss:0.25818
loss:0.27704
loss:0.25838
loss:0.24122
loss:0.27090
loss:0.25708
loss:0.26362
loss:0.25091
loss:0.25784
loss:0.29120
loss:0.23470
loss:0.28272
loss:0.27288
loss:0.28649
loss:0.27885
loss:0.28911
loss:0.26082
loss:0.27820
loss:0.25635
loss:0.27272
loss:0.27422
loss:0.27462
loss:0.26241
loss:0.27936
loss:0.24827
loss:0.28601
loss:0.26880
loss:0.25032
loss:0.26367
loss:0.27045
loss:0.26636
loss:0.29158
loss:0.26573
loss:0.28727
loss:0.24985
loss:0.28222
loss:0.26175
loss:0.26835
loss:0.30153
loss:0.29380
loss:0.27056
loss:0.27887
loss:0.22747
loss:0.29243
loss:0.25590
loss:0.25998
loss:0.25171
loss:0.26826
loss:0.26077
loss:0.25728
loss:0.25955
loss:0.27037
loss:0.25072
loss:0.26540
loss:0.25933
loss:0.28204
loss:0.26605
loss:0.26366
loss:0.26912
loss:0.25213
loss:0.21136
loss:0.31435
loss:0.25940
loss:0.25128
loss:0.27229
loss:0.26683
loss:0.27990
loss:0.23190
loss:0.28845
loss:0.25850
loss:0.31662
loss:0.26224
loss:0.27374
loss:0.25846
loss:0.24613
loss:0.26123
loss:0.27045
loss:0.25287
loss:0.25259
loss:0.26049
loss:0.24479
loss:0.28256
loss:0.24730
loss:0.24498
loss:0.26742
loss:0.27744
loss:0.23005
loss:0.26053
loss:0.25793
loss:0.26685
loss:0.25031
loss:0.29944
loss:0.25965
loss:0.27535
loss:0.26426
loss:0.28456
loss:0.27650
loss:0.24698
loss:0.25826
loss:0.26374
loss:0.26645
loss:0.26426
loss:0.25660
loss:0.29174
loss:0.29395
loss:0.29207
loss:0.28976
loss:0.26293
loss:0.26097
loss:0.30012
loss:0.20901
loss:0.25638
loss:0.26436
loss:0.25822
loss:0.24314
loss:0.25945
loss:0.26803
loss:0.27967
loss:0.28065
loss:0.24231
loss:0.26187
loss:0.26720
loss:0.27866
loss:0.28865
loss:0.27610
loss:0.24362
loss:0.25530
loss:0.24409
loss:0.26962
loss:0.26735
loss:0.26032
loss:0.26874
loss:0.29012
loss:0.26471
loss:0.25985
loss:0.25081
loss:0.22998
loss:0.29304
loss:0.26572
loss:0.28923
loss:0.25929
loss:0.26771
loss:0.27901
loss:0.27874
loss:0.29221
loss:0.25010
loss:0.27604
loss:0.24581
loss:0.27101
loss:0.27015
loss:0.25498
loss:0.28668
loss:0.26489
loss:0.25966
loss:0.27687
loss:0.26763
loss:0.26956
loss:0.29298
loss:0.28849
loss:0.25400
loss:0.27163
loss:0.23254
loss:0.26238
loss:0.24518
loss:0.25460
loss:0.26025
loss:0.28743
loss:0.24601
loss:0.28495
loss:0.27080
loss:0.29147
loss:0.24594
loss:0.26311
loss:0.22895
loss:0.27108
loss:0.24832
Evaluation at Epoch 5/20. Step:4460/17840. AccuracyMetric: acc=0.89071

loss:0.27123
loss:0.24987
loss:0.28868
loss:0.25309
loss:0.26801
loss:0.25821
loss:0.27004
loss:0.25848
loss:0.27808
loss:0.26169
loss:0.26754
loss:0.25963
loss:0.24877
loss:0.27164
loss:0.25783
loss:0.26071
loss:0.24645
loss:0.24625
loss:0.23428
loss:0.26925
loss:0.29341
loss:0.25410
loss:0.30037
loss:0.26806
loss:0.28175
loss:0.23706
loss:0.26256
loss:0.28706
loss:0.27006
loss:0.27630
loss:0.28254
loss:0.24465
loss:0.25912
loss:0.27847
loss:0.23822
loss:0.25975
loss:0.28408
loss:0.26353
loss:0.25727
loss:0.24846
loss:0.28912
loss:0.26331
loss:0.27250
loss:0.27008
loss:0.28610
loss:0.25627
loss:0.24617
loss:0.24701
loss:0.27287
loss:0.24501
loss:0.21648
loss:0.27706
loss:0.23980
loss:0.25464
loss:0.27815
loss:0.23551
loss:0.28056
loss:0.27472
loss:0.25858
loss:0.27134
loss:0.30218
loss:0.25288
loss:0.27102
loss:0.27450
loss:0.25454
loss:0.24674
loss:0.25931
loss:0.22666
loss:0.24666
loss:0.25754
loss:0.26659
loss:0.24783
loss:0.24963
loss:0.26107
loss:0.27615
loss:0.25759
loss:0.27277
loss:0.25090
loss:0.27339
loss:0.28873
loss:0.28175
loss:0.24034
loss:0.25284
loss:0.22674
loss:0.27590
loss:0.25283
loss:0.23439
loss:0.27344
loss:0.27505
loss:0.27316
loss:0.24806
loss:0.26594
loss:0.28985
loss:0.26186
loss:0.26121
loss:0.26599
loss:0.27452
loss:0.25650
loss:0.24518
loss:0.27906
loss:0.22980
loss:0.24116
loss:0.26191
loss:0.24829
loss:0.27431
loss:0.28026
loss:0.25528
loss:0.26176
loss:0.23367
loss:0.26150
loss:0.26679
loss:0.29636
loss:0.26733
loss:0.27240
loss:0.27285
loss:0.22549
loss:0.27428
loss:0.28882
loss:0.27284
loss:0.24951
loss:0.26647
loss:0.26367
loss:0.29244
loss:0.26815
loss:0.24432
loss:0.23957
loss:0.25970
loss:0.25719
loss:0.24706
loss:0.23219
loss:0.24476
loss:0.28858
loss:0.27083
loss:0.23446
loss:0.25661
loss:0.27684
loss:0.21819
loss:0.24902
loss:0.24350
loss:0.29040
loss:0.25829
loss:0.26626
loss:0.27072
loss:0.25581
loss:0.25273
loss:0.27438
loss:0.27387
loss:0.25619
loss:0.26172
loss:0.24841
loss:0.26246
loss:0.24565
loss:0.25836
loss:0.26783
loss:0.28910
loss:0.27793
loss:0.25559
loss:0.26055
loss:0.24405
loss:0.29916
loss:0.25273
loss:0.23995
loss:0.27292
loss:0.27058
loss:0.22449
loss:0.28220
loss:0.28835
loss:0.25556
loss:0.27700
loss:0.25481
loss:0.25477
loss:0.27051
loss:0.24033
loss:0.23262
loss:0.26579
loss:0.27427
loss:0.27257
loss:0.26101
Evaluation at Epoch 6/20. Step:5352/17840. AccuracyMetric: acc=0.890717

loss:0.23792
loss:0.25320
loss:0.25838
loss:0.24311
loss:0.25208
loss:0.26061
loss:0.23585
loss:0.25238
loss:0.27970
loss:0.28328
loss:0.25624
loss:0.27091
loss:0.23836
loss:0.23577
loss:0.24065
loss:0.28977
loss:0.25296
loss:0.24222
loss:0.26931
loss:0.22596
loss:0.23930
loss:0.25236
loss:0.25242
loss:0.25864
loss:0.26237
loss:0.26630
loss:0.26492
loss:0.25197
loss:0.26262
loss:0.24622
loss:0.25647
loss:0.23606
loss:0.24914
loss:0.26311
loss:0.25868
loss:0.26739
loss:0.25868
loss:0.26684
loss:0.24845
loss:0.25528
loss:0.25250
loss:0.26256
loss:0.28580
loss:0.25658
loss:0.24979
loss:0.24020
loss:0.24822
loss:0.29224
loss:0.26326
loss:0.24760
loss:0.25258
loss:0.26143
loss:0.24477
loss:0.27542
loss:0.26776
loss:0.26936
loss:0.24957
loss:0.26554
loss:0.25245
loss:0.24652
loss:0.27184
loss:0.26528
loss:0.22788
loss:0.25484
loss:0.22234
loss:0.21867
loss:0.26074
loss:0.22444
loss:0.25259
loss:0.24770
loss:0.26445
loss:0.26733
loss:0.24721
loss:0.25362
loss:0.25801
loss:0.26316
loss:0.24826
loss:0.27383
loss:0.24683
loss:0.23265
loss:0.27825
loss:0.24217
loss:0.25457
loss:0.28907
loss:0.26910
loss:0.25808
loss:0.25210
loss:0.23919
loss:0.24221
loss:0.25615
loss:0.25022
loss:0.24344
loss:0.29792
loss:0.24779
loss:0.27608
loss:0.24460
loss:0.23773
loss:0.24549
loss:0.27251
loss:0.28107
loss:0.26280
loss:0.24642
loss:0.24335
loss:0.25311
loss:0.24807
loss:0.25043
loss:0.27188
loss:0.24735
loss:0.25036
loss:0.24288
loss:0.23749
loss:0.26600
loss:0.29055
loss:0.28193
loss:0.27272
loss:0.25973
loss:0.23873
loss:0.25822
loss:0.27426
loss:0.27752
loss:0.22462
loss:0.23851
loss:0.26752
loss:0.25719
loss:0.24538
loss:0.24719
loss:0.25412
loss:0.25074
loss:0.23010
loss:0.26157
loss:0.23683
loss:0.26076
loss:0.25058
loss:0.22281
loss:0.27190
loss:0.26396
loss:0.28282
loss:0.26498
loss:0.24487
loss:0.24897
loss:0.27338
loss:0.27793
loss:0.25434
loss:0.26158
loss:0.27719
loss:0.25830
loss:0.26455
loss:0.26848
loss:0.25334
loss:0.25420
loss:0.24733
loss:0.25997
loss:0.24455
loss:0.26875
loss:0.28207
loss:0.25485
loss:0.24660
loss:0.26772
loss:0.26125
loss:0.25696
loss:0.25911
loss:0.23889
loss:0.26825
loss:0.24984
loss:0.27979
loss:0.26018
loss:0.27588
loss:0.26077
loss:0.25511
loss:0.23889
loss:0.31126
loss:0.25081
loss:0.25564
loss:0.25284
loss:0.25538
loss:0.24458
loss:0.27445
loss:0.24736
Evaluation at Epoch 7/20. Step:6244/17840. AccuracyMetric: acc=0.890561

loss:0.25094
loss:0.24328
loss:0.26771
loss:0.26697
loss:0.25000
loss:0.23285
loss:0.23968
loss:0.23224
loss:0.25584
loss:0.26089
loss:0.28346
loss:0.25427
loss:0.24583
loss:0.25391
loss:0.23695
loss:0.25411
loss:0.24194
loss:0.23182
loss:0.24239
loss:0.24537
loss:0.24795
loss:0.23674
loss:0.24172
loss:0.25953
loss:0.25882
loss:0.22756
loss:0.25149
loss:0.20985
loss:0.24539
loss:0.24857
loss:0.22044
loss:0.28290
loss:0.24575
loss:0.25812
loss:0.25154
loss:0.24838
loss:0.25350
loss:0.26331
loss:0.23696
loss:0.25207
loss:0.25079
loss:0.26434
loss:0.24985
loss:0.23655
loss:0.25861
loss:0.23203
loss:0.23159
loss:0.23647
loss:0.26623
loss:0.25733
loss:0.27421
loss:0.24125
loss:0.26827
loss:0.24669
loss:0.26247
loss:0.24234
loss:0.27212
loss:0.23296
loss:0.26707
loss:0.23003
loss:0.24001
loss:0.22567
loss:0.25940
loss:0.23676
loss:0.26928
loss:0.23508
loss:0.26481
loss:0.22968
loss:0.27866
loss:0.26602
loss:0.25293
loss:0.27099
loss:0.24314
loss:0.21910
loss:0.25714
loss:0.22865
loss:0.22470
loss:0.26266
loss:0.23443
loss:0.29699
loss:0.25049
loss:0.25242
loss:0.28511
loss:0.23847
loss:0.26908
loss:0.24444
loss:0.24615
loss:0.28317
loss:0.23203
loss:0.25697
loss:0.24387
loss:0.23231
loss:0.28716
loss:0.24820
loss:0.23038
loss:0.23824
loss:0.23095
loss:0.24541
loss:0.24945
loss:0.24663
loss:0.23490
loss:0.23851
loss:0.24462
loss:0.27515
loss:0.26347
loss:0.26779
loss:0.26058
loss:0.25389
loss:0.21238
loss:0.25879
loss:0.25256
loss:0.23491
loss:0.23815
loss:0.24083
loss:0.25390
loss:0.25697
loss:0.26130
loss:0.28201
loss:0.27013
loss:0.22435
loss:0.26686
loss:0.24822
loss:0.26216
loss:0.25096
loss:0.21053
loss:0.25321
loss:0.27240
loss:0.23125
loss:0.26891
loss:0.25893
loss:0.26364
loss:0.25808
loss:0.25232
loss:0.22243
loss:0.25010
loss:0.25456
loss:0.22933
loss:0.24895
loss:0.24491
loss:0.22715
loss:0.24392
loss:0.23673
loss:0.23202
loss:0.25303
loss:0.26899
loss:0.25640
loss:0.26158
loss:0.26815
loss:0.23346
loss:0.24423
loss:0.25669
loss:0.26389
loss:0.27600
loss:0.26147
loss:0.24317
loss:0.26630
loss:0.23313
loss:0.27009
loss:0.25293
loss:0.24869
loss:0.27518
loss:0.26350
loss:0.27711
loss:0.25229
loss:0.23684
loss:0.25245
loss:0.25167
loss:0.25306
loss:0.24693
loss:0.23581
loss:0.26605
loss:0.22904
loss:0.22743
loss:0.23237
loss:0.22176
loss:0.23787
loss:0.24854
loss:0.24331
loss:0.27109
Evaluation at Epoch 8/20. Step:7136/17840. AccuracyMetric: acc=0.890008

loss:0.27379
loss:0.26104
loss:0.21697
loss:0.22710
loss:0.22064
loss:0.22265
loss:0.21270
loss:0.24081
loss:0.26249
loss:0.24131
loss:0.24401
loss:0.24619
loss:0.25759
loss:0.22732
loss:0.23751
loss:0.21959
loss:0.23560
loss:0.29883
loss:0.24112
loss:0.24215
loss:0.24789
loss:0.24063
loss:0.23968
loss:0.25957
loss:0.22385
loss:0.24066
loss:0.24605
loss:0.23538
loss:0.22013
loss:0.21631
loss:0.23377
loss:0.24316
loss:0.24126
loss:0.26779
loss:0.21524
loss:0.22067
loss:0.21070
loss:0.21644
loss:0.27000
loss:0.23265
loss:0.23626
loss:0.22436
loss:0.24657
loss:0.24735
loss:0.25517
loss:0.23053
loss:0.24701
loss:0.23103
loss:0.23329
loss:0.24827
loss:0.25007
loss:0.24730
loss:0.21394
loss:0.25830
loss:0.24377
loss:0.26015
loss:0.24794
loss:0.23902
loss:0.23281
loss:0.22530
loss:0.23200
loss:0.23002
loss:0.25705
loss:0.23818
loss:0.22626
loss:0.22995
loss:0.25428
loss:0.27654
loss:0.24611
loss:0.22263
loss:0.24804
loss:0.22241
loss:0.27076
loss:0.25214
loss:0.27139
loss:0.23334
loss:0.24805
loss:0.25098
loss:0.27031
loss:0.25166
loss:0.25375
loss:0.27061
loss:0.24835
loss:0.23356
loss:0.24251
loss:0.25151
loss:0.27725
loss:0.24778
loss:0.23676
loss:0.23309
loss:0.21454
loss:0.27292
loss:0.26195
loss:0.26535
loss:0.25333
loss:0.26775
loss:0.26062
loss:0.23369
loss:0.24387
loss:0.22920
loss:0.24712
loss:0.24599
loss:0.27304
loss:0.25830
loss:0.20797
loss:0.22007
loss:0.24395
loss:0.25002
loss:0.24224
loss:0.23599
loss:0.24566
loss:0.24394
loss:0.24511
loss:0.22575
loss:0.23065
loss:0.29161
loss:0.23651
loss:0.25231
loss:0.23159
loss:0.23619
loss:0.22449
loss:0.22047
loss:0.24439
loss:0.25799
loss:0.21382
loss:0.27593
loss:0.25327
loss:0.25692
loss:0.23004
loss:0.26721
loss:0.23428
loss:0.25149
loss:0.26020
loss:0.21503
loss:0.24246
loss:0.24794
loss:0.25554
loss:0.20731
loss:0.25392
loss:0.26629
loss:0.24147
loss:0.26806
loss:0.23734
loss:0.22825
loss:0.26092
loss:0.23457
loss:0.27699
loss:0.23761
loss:0.24659
loss:0.27325
loss:0.22600
loss:0.24170
loss:0.23457
loss:0.23483
loss:0.23488
loss:0.25370
loss:0.23929
loss:0.22889
loss:0.27874
loss:0.24344
loss:0.24810
loss:0.25527
loss:0.23665
loss:0.25211
loss:0.25424
loss:0.25499
loss:0.23572
loss:0.25581
loss:0.25511
loss:0.22214
loss:0.25895
loss:0.22631
loss:0.24887
loss:0.25387
loss:0.23713
loss:0.23878
loss:0.23960
loss:0.24200
Evaluation at Epoch 9/20. Step:8028/17840. AccuracyMetric: acc=0.889649

loss:0.22620
loss:0.22697
loss:0.22192
loss:0.21653
loss:0.23747
loss:0.26579
loss:0.25394
loss:0.21707
loss:0.25435
loss:0.21569
loss:0.20908
loss:0.25102
loss:0.20844
loss:0.25209
loss:0.25154
loss:0.22508
loss:0.24182
loss:0.25329
loss:0.22544
loss:0.22811
loss:0.24071
loss:0.23098
loss:0.22141
loss:0.25195
loss:0.22051
loss:0.21207
loss:0.20855
loss:0.24518
loss:0.24592
loss:0.22542
loss:0.19549
loss:0.20340
loss:0.22547
loss:0.20211
loss:0.24746
loss:0.22896
loss:0.23909
loss:0.24331
loss:0.21826
loss:0.21450
loss:0.24416
loss:0.22880
loss:0.21632
loss:0.22346
loss:0.24041
loss:0.21044
loss:0.22437
loss:0.22888
loss:0.25670
loss:0.25438
loss:0.22615
loss:0.22539
loss:0.24977
loss:0.24279
loss:0.25673
loss:0.23796
loss:0.24747
loss:0.23149
loss:0.23712
loss:0.22835
loss:0.21476
loss:0.23322
loss:0.22533
loss:0.25903
loss:0.22511
loss:0.21509
loss:0.24671
loss:0.25432
loss:0.24351
loss:0.24238
loss:0.22736
loss:0.22429
loss:0.23220
loss:0.23923
loss:0.24239
loss:0.25769
loss:0.27575
loss:0.24013
loss:0.24626
loss:0.23796
loss:0.22199
loss:0.23029
loss:0.23848
loss:0.23538
loss:0.24427
loss:0.25436
loss:0.24440
loss:0.22346
loss:0.24180
loss:0.23644
loss:0.23403
loss:0.24013
loss:0.23968
loss:0.23794
loss:0.25720
loss:0.24572
loss:0.23098
loss:0.26755
loss:0.22382
loss:0.22266
loss:0.22164
loss:0.22160
loss:0.24622
loss:0.24670
loss:0.25159
loss:0.21840
loss:0.27074
loss:0.24288
loss:0.24164
loss:0.24602
loss:0.23540
loss:0.25615
loss:0.22956
loss:0.23742
loss:0.27545
loss:0.23312
loss:0.22377
loss:0.19729
loss:0.24511
loss:0.22851
loss:0.19514
loss:0.22759
loss:0.22742
loss:0.23149
loss:0.21277
loss:0.22076
loss:0.22259
loss:0.24861
loss:0.24506
loss:0.23030
loss:0.19783
loss:0.23391
loss:0.24059
loss:0.23311
loss:0.25351
loss:0.24063
loss:0.23567
loss:0.25627
loss:0.22256
loss:0.24134
loss:0.22505
loss:0.24399
loss:0.23696
loss:0.23698
loss:0.21988
loss:0.23886
loss:0.22468
loss:0.25203
loss:0.23656
loss:0.25948
loss:0.23938
loss:0.23096
loss:0.21802
loss:0.24070
loss:0.25806
loss:0.26169
loss:0.22042
loss:0.20641
loss:0.24002
loss:0.23062
loss:0.23075
loss:0.26353
loss:0.23702
loss:0.23451
loss:0.22500
loss:0.25480
loss:0.23504
loss:0.23803
loss:0.24918
loss:0.25622
loss:0.23156
loss:0.24367
loss:0.23821
loss:0.26027
loss:0.24075
loss:0.22089
loss:0.22403
loss:0.25026
loss:0.22874
Evaluation at Epoch 10/20. Step:8920/17840. AccuracyMetric: acc=0.888542

loss:0.21445
loss:0.22753
loss:0.24162
loss:0.21750
loss:0.21209
loss:0.22542
loss:0.25856
loss:0.21816
loss:0.20840
loss:0.20898
loss:0.23784
loss:0.20676
loss:0.21704
loss:0.25053
loss:0.20449
loss:0.20911
loss:0.23165
loss:0.23218
loss:0.22785
loss:0.24573
loss:0.23257
loss:0.21964
loss:0.21646
loss:0.22863
loss:0.23256
loss:0.22874
loss:0.23279
loss:0.20843
loss:0.21257
loss:0.21479
loss:0.24275
loss:0.20567
loss:0.25184
loss:0.24412
loss:0.19491
loss:0.24106
loss:0.18540
loss:0.21370
loss:0.20746
loss:0.21700
loss:0.22113
loss:0.23870
loss:0.22057
loss:0.24484
loss:0.24658
loss:0.22079
loss:0.19479
loss:0.22176
loss:0.23900
loss:0.23901
loss:0.21566
loss:0.26205
loss:0.25416
loss:0.23100
loss:0.21564
loss:0.24982
loss:0.22406
loss:0.22970
loss:0.23404
loss:0.20368
loss:0.23524
loss:0.24488
loss:0.23623
loss:0.24068
loss:0.21713
loss:0.24962
loss:0.23122
loss:0.24917
loss:0.23627
loss:0.21693
loss:0.22648
loss:0.23981
loss:0.22452
loss:0.23190
loss:0.23045
loss:0.20303
loss:0.23025
loss:0.24655
loss:0.21999
loss:0.22289
loss:0.20952
loss:0.21949
loss:0.20159
loss:0.23245
loss:0.23106
loss:0.25238
loss:0.24661
loss:0.26149
loss:0.20571
loss:0.22443
loss:0.22465
loss:0.22717
loss:0.21061
loss:0.22081
loss:0.21908
loss:0.23973
loss:0.23742
loss:0.22375
loss:0.20945
loss:0.23759
loss:0.23569
loss:0.24156
loss:0.20803
loss:0.26568
loss:0.21738
loss:0.21501
loss:0.24581
loss:0.26498
loss:0.22687
loss:0.22452
loss:0.23291
loss:0.20544
loss:0.23465
loss:0.21907
loss:0.23350
loss:0.20367
loss:0.22550
loss:0.22366
loss:0.24104
loss:0.21881
loss:0.24572
loss:0.24933
loss:0.23157
loss:0.24174
loss:0.21208
loss:0.23255
loss:0.21741
loss:0.23821
loss:0.23168
loss:0.24847
loss:0.24293
loss:0.20797
loss:0.25268
loss:0.20598
loss:0.24386
loss:0.20439
loss:0.22219
loss:0.20732
loss:0.20615
loss:0.21346
loss:0.21745
loss:0.18515
loss:0.26196
loss:0.22819
loss:0.22120
loss:0.23016
loss:0.21668
loss:0.19298
loss:0.23882
loss:0.22479
loss:0.23859
loss:0.23031
loss:0.22107
loss:0.22746
loss:0.23786
loss:0.22369
loss:0.22855
loss:0.23664
loss:0.21771
loss:0.25659
loss:0.20206
loss:0.20913
loss:0.22734
loss:0.19462
loss:0.23858
loss:0.20112
loss:0.24196
loss:0.22720
loss:0.22721
loss:0.24709
loss:0.24347
loss:0.23227
loss:0.25331
loss:0.23902
loss:0.21265
loss:0.25124
loss:0.22935
loss:0.21553
Evaluation at Epoch 11/20. Step:9812/17840. AccuracyMetric: acc=0.887701

loss:0.22141
loss:0.20948
loss:0.21899
loss:0.21058
loss:0.21505
loss:0.24767
loss:0.22147
loss:0.21135
loss:0.21563
loss:0.21829
loss:0.21583
loss:0.20104
loss:0.23553
loss:0.24601
loss:0.22451
loss:0.21443
loss:0.22573
loss:0.21177
loss:0.20873
loss:0.20203
loss:0.21581
loss:0.20874
loss:0.21198
loss:0.23368
loss:0.20178
loss:0.24123
loss:0.20916
loss:0.21134
loss:0.22184
loss:0.20849
loss:0.23972
loss:0.20870
loss:0.21628
loss:0.20230
loss:0.21313
loss:0.22400
loss:0.21144
loss:0.23468
loss:0.21356
loss:0.22087
loss:0.21617
loss:0.21994
loss:0.20080
loss:0.21474
loss:0.22439
loss:0.22290
loss:0.19890
loss:0.22207
loss:0.20791
loss:0.21199
loss:0.22992
loss:0.18482
loss:0.24490
loss:0.23008
loss:0.21884
loss:0.21743
loss:0.22282
loss:0.23810
loss:0.23422
loss:0.18864
loss:0.23489
loss:0.20847
loss:0.22061
loss:0.22010
loss:0.19752
loss:0.23747
loss:0.23045
loss:0.22997
loss:0.21914
loss:0.19964
loss:0.23153
loss:0.20773
loss:0.19983
loss:0.22240
loss:0.22229
loss:0.20021
loss:0.19435
loss:0.21223
loss:0.22059
loss:0.20632
loss:0.18661
loss:0.19882
loss:0.24592
loss:0.25553
loss:0.21047
loss:0.22021
loss:0.23084
loss:0.23844
loss:0.23589
loss:0.23997
loss:0.25168
loss:0.21105
loss:0.18853
loss:0.24038
loss:0.22933
loss:0.21130
loss:0.23409
loss:0.22279
loss:0.21938
loss:0.21969
loss:0.22060
loss:0.22158
loss:0.20916
loss:0.22254
loss:0.22013
loss:0.23161
loss:0.20199
loss:0.22804
loss:0.23083
loss:0.20631
loss:0.21639
loss:0.21834
loss:0.21460
loss:0.21309
loss:0.21804
loss:0.23050
loss:0.23951
loss:0.23731
loss:0.20729
loss:0.21300
loss:0.23641
loss:0.20724
loss:0.22310
loss:0.22563
loss:0.24017
loss:0.23027
loss:0.23129
loss:0.24356
loss:0.23283
loss:0.19535
loss:0.19977
loss:0.20884
loss:0.20414
loss:0.23548
loss:0.23010
loss:0.19812
loss:0.23153
loss:0.19317
loss:0.20793
loss:0.20172
loss:0.22897
loss:0.22165
loss:0.22058
loss:0.20668
loss:0.24114
loss:0.22939
loss:0.24646
loss:0.23105
loss:0.22308
loss:0.20784
loss:0.21678
loss:0.21433
loss:0.19630
loss:0.23672
loss:0.20860
loss:0.20132
loss:0.21833
loss:0.24826
loss:0.19332
loss:0.25068
loss:0.21646
loss:0.20975
loss:0.22283
loss:0.22401
loss:0.21990
loss:0.23121
loss:0.22472
loss:0.21380
loss:0.22727
loss:0.20418
loss:0.23332
loss:0.22939
loss:0.21911
loss:0.26755
loss:0.21828
loss:0.24301
loss:0.22515
loss:0.23157
Evaluation at Epoch 12/20. Step:10704/17840. AccuracyMetric: acc=0.886492

loss:0.21879
loss:0.19986
loss:0.20161
loss:0.19234
loss:0.20777
loss:0.19967
loss:0.21240
loss:0.18744
loss:0.22182
loss:0.19668
loss:0.22814
loss:0.19753
loss:0.20942
loss:0.21589
loss:0.22935
loss:0.19734
loss:0.20348
loss:0.21033
loss:0.21045
loss:0.23151
loss:0.22037
loss:0.21126
loss:0.18017
loss:0.21180
loss:0.18925
loss:0.18395
loss:0.20505
loss:0.22503
loss:0.23326
loss:0.22428
loss:0.23074
loss:0.19583
loss:0.22235
loss:0.21151
loss:0.25052
loss:0.17799
loss:0.19042
loss:0.19192
loss:0.19184
loss:0.23118
loss:0.20634
loss:0.21038
loss:0.24215
loss:0.21625
loss:0.20227
loss:0.19753
loss:0.18893
loss:0.19120
loss:0.22834
loss:0.22185
loss:0.19897
loss:0.22774
loss:0.21090
loss:0.22111
loss:0.19474
loss:0.21176
loss:0.21610
loss:0.20566
loss:0.20516
loss:0.23221
loss:0.23644
loss:0.20260
loss:0.21261
loss:0.19288
loss:0.21754
loss:0.23284
loss:0.21336
loss:0.20161
loss:0.19328
loss:0.22227
loss:0.20940
loss:0.19905
loss:0.22899
loss:0.19809
loss:0.22367
loss:0.22087
loss:0.18589
loss:0.21227
loss:0.20943
loss:0.21820
loss:0.18307
loss:0.22839
loss:0.23944
loss:0.22273
loss:0.19977
loss:0.22627
loss:0.21993
loss:0.18385
loss:0.20820
loss:0.20178
loss:0.20943
loss:0.19504
loss:0.20937
loss:0.22026
loss:0.21143
loss:0.19549
loss:0.22337
loss:0.23772
loss:0.21889
loss:0.19770
loss:0.22097
loss:0.19859
loss:0.22571
loss:0.22317
loss:0.23176
loss:0.22121
loss:0.22427
loss:0.21494
loss:0.21247
loss:0.23282
loss:0.22483
loss:0.21030
loss:0.20958
loss:0.20663
loss:0.20781
loss:0.20371
loss:0.21326
loss:0.20636
loss:0.20272
loss:0.21365
loss:0.18868
loss:0.20934
loss:0.20827
loss:0.22650
loss:0.20099
loss:0.20274
loss:0.23985
loss:0.23424
loss:0.21439
loss:0.21354
loss:0.20758
loss:0.19454
loss:0.20965
loss:0.21585
loss:0.19174
loss:0.20626
loss:0.23832
loss:0.21245
loss:0.21609
loss:0.20561
loss:0.20345
loss:0.20424
loss:0.23808
loss:0.22396
loss:0.23323
loss:0.21037
loss:0.19161
loss:0.24733
loss:0.22534
loss:0.20820
loss:0.21322
loss:0.24034
loss:0.21893
loss:0.21470
loss:0.23145
loss:0.17500
loss:0.23084
loss:0.24248
loss:0.22562
loss:0.23496
loss:0.20787
loss:0.21809
loss:0.23762
loss:0.21836
loss:0.22393
loss:0.21794
loss:0.20779
loss:0.20886
loss:0.23276
loss:0.22474
loss:0.23829
loss:0.18383
loss:0.20386
loss:0.22967
loss:0.21709
loss:0.21264
loss:0.23131
loss:0.18830
loss:0.20525
Evaluation at Epoch 13/20. Step:11596/17840. AccuracyMetric: acc=0.88583

loss:0.19369
loss:0.20070
loss:0.17914
loss:0.21264
loss:0.19668
loss:0.20283
loss:0.22738
loss:0.18092
loss:0.18449
loss:0.18150
loss:0.21050
loss:0.21510
loss:0.20596
loss:0.19614
loss:0.21798
loss:0.21504
loss:0.19927
loss:0.18350
loss:0.19042
loss:0.22622
loss:0.20898
loss:0.20339
loss:0.22121
loss:0.21935
loss:0.19310
loss:0.20898
loss:0.22974
loss:0.20824
loss:0.21702
loss:0.20272
loss:0.18848
loss:0.20720
loss:0.24192
loss:0.19614
loss:0.19280
loss:0.21100
loss:0.17619
loss:0.20944
loss:0.20482
loss:0.21868
loss:0.20214
loss:0.19943
loss:0.20722
loss:0.19592
loss:0.17798
loss:0.22473
loss:0.20837
loss:0.18380
loss:0.20334
loss:0.19784
loss:0.19123
loss:0.19449
loss:0.21038
loss:0.18346
loss:0.20199
loss:0.19741
loss:0.20316
loss:0.18392
loss:0.20398
loss:0.24482
loss:0.21462
loss:0.21977
loss:0.20722
loss:0.20011
loss:0.20267
loss:0.21856
loss:0.22506
loss:0.21278
loss:0.20517
loss:0.19648
loss:0.20776
loss:0.19215
loss:0.21135
loss:0.21258
loss:0.18517
loss:0.20858
loss:0.20572
loss:0.21030
loss:0.20578
loss:0.20175
loss:0.20548
loss:0.19257
loss:0.19788
loss:0.18362
loss:0.21113
loss:0.19154
loss:0.20663
loss:0.18802
loss:0.23058
loss:0.20373
loss:0.19505
loss:0.19956
loss:0.21867
loss:0.19949
loss:0.20912
loss:0.22157
loss:0.22355
loss:0.19232
loss:0.23895
loss:0.20066
loss:0.18097
loss:0.21136
loss:0.21430
loss:0.21614
loss:0.21084
loss:0.21280
loss:0.21994
loss:0.20694
loss:0.19701
loss:0.19917
loss:0.21181
loss:0.17499
loss:0.21559
loss:0.20205
loss:0.21859
loss:0.20650
loss:0.20684
loss:0.20214
loss:0.20336
loss:0.19468
loss:0.23847
loss:0.21850
loss:0.20645
loss:0.22906
loss:0.20775
loss:0.20511
loss:0.20375
loss:0.17584
loss:0.22378
loss:0.18664
loss:0.19201
loss:0.22521
loss:0.21484
loss:0.19240
loss:0.21121
loss:0.19768
loss:0.20397
loss:0.20038
loss:0.22245
loss:0.19873
loss:0.18656
loss:0.19555
loss:0.21174
loss:0.19904
loss:0.17730
loss:0.21480
loss:0.22725
loss:0.20336
loss:0.22276
loss:0.19968
loss:0.21965
loss:0.20973
loss:0.20570
loss:0.20534
loss:0.22553
loss:0.21162
loss:0.20845
loss:0.20581
loss:0.22533
loss:0.20303
loss:0.19371
loss:0.20355
loss:0.22372
loss:0.21641
loss:0.20820
loss:0.21708
loss:0.20827
loss:0.20586
loss:0.19654
loss:0.21348
loss:0.20947
loss:0.20758
loss:0.21928
loss:0.18866
loss:0.20418
loss:0.24188
loss:0.22759
loss:0.22443
Evaluation at Epoch 14/20. Step:12488/17840. AccuracyMetric: acc=0.882033

loss:0.22983
loss:0.20989
loss:0.17683
loss:0.19960
loss:0.19890
loss:0.21591
loss:0.20812
loss:0.19781
loss:0.17860
loss:0.19734
loss:0.19724
loss:0.18208
loss:0.19484
loss:0.20904
loss:0.20101
loss:0.18351
loss:0.19235
loss:0.21610
loss:0.19871
loss:0.19906
loss:0.20155
loss:0.21819
loss:0.22342
loss:0.18684
loss:0.19239
loss:0.21332
loss:0.19054
loss:0.19109
loss:0.16727
loss:0.18221
loss:0.21991
loss:0.20124
loss:0.21325
loss:0.18491
loss:0.20255
loss:0.18885
loss:0.20783
loss:0.19680
loss:0.17904
loss:0.17536
loss:0.18917
loss:0.19327
loss:0.20687
loss:0.20226
loss:0.21828
loss:0.19924
loss:0.18667
loss:0.19627
loss:0.21008
loss:0.19158
loss:0.20017
loss:0.19764
loss:0.18874
loss:0.17912
loss:0.18498
loss:0.20719
loss:0.18784
loss:0.18725
loss:0.17898
loss:0.17016
loss:0.22108
loss:0.19356
loss:0.19643
loss:0.22229
loss:0.20821
loss:0.18676
loss:0.19348
loss:0.17725
loss:0.20153
loss:0.21104
loss:0.19599
loss:0.22362
loss:0.21267
loss:0.19778
loss:0.21296
loss:0.20919
loss:0.17455
loss:0.21127
loss:0.22193
loss:0.18886
loss:0.20024
loss:0.20293
loss:0.18978
loss:0.18402
loss:0.20018
loss:0.20232
loss:0.18588
loss:0.19622
loss:0.19284
loss:0.15364
loss:0.18360
loss:0.21316
loss:0.20281
loss:0.22128
loss:0.20802
loss:0.20890
loss:0.19945
loss:0.19203
loss:0.20702
loss:0.18871
loss:0.21704
loss:0.18591
loss:0.19017
loss:0.19323
loss:0.20449
loss:0.21484
loss:0.22111
loss:0.21527
loss:0.20377
loss:0.22144
loss:0.19015
loss:0.19323
loss:0.23741
loss:0.18470
loss:0.20977
loss:0.21214
loss:0.19446
loss:0.18535
loss:0.22421
loss:0.20435
loss:0.20652
loss:0.21645
loss:0.19322
loss:0.17028
loss:0.18863
loss:0.19103
loss:0.19925
loss:0.19871
loss:0.19527
loss:0.21556
loss:0.19847
loss:0.20138
loss:0.20527
loss:0.18206
loss:0.20177
loss:0.20112
loss:0.17725
loss:0.21692
loss:0.19259
loss:0.22548
loss:0.21134
loss:0.18358
loss:0.19910
loss:0.20184
loss:0.19406
loss:0.19069
loss:0.23431
loss:0.19594
loss:0.20812
loss:0.17665
loss:0.20120
loss:0.20090
loss:0.19016
loss:0.21366
loss:0.20037
loss:0.19243
loss:0.21572
loss:0.20644
loss:0.18739
loss:0.22145
loss:0.19587
loss:0.19944
loss:0.18985
loss:0.18839
loss:0.21981
loss:0.21456
loss:0.19182
loss:0.22843
loss:0.20236
loss:0.19674
loss:0.21467
loss:0.19119
loss:0.19672
loss:0.19197
loss:0.20342
loss:0.22359
loss:0.21713
loss:0.20125
loss:0.20069
Evaluation at Epoch 15/20. Step:13380/17840. AccuracyMetric: acc=0.884091

loss:0.18468
loss:0.18620
loss:0.16281
loss:0.18758
loss:0.19699
loss:0.17771
loss:0.16547
loss:0.17555
loss:0.18302
loss:0.19481
loss:0.18566
loss:0.16213
loss:0.16598
loss:0.18447
loss:0.19369
loss:0.16326
loss:0.19092
loss:0.19544
loss:0.18658
loss:0.15933
loss:0.21374
loss:0.18387
loss:0.19106
loss:0.16584
loss:0.21628
loss:0.18888
loss:0.20572
loss:0.19184
loss:0.17732
loss:0.20401
loss:0.19178
loss:0.18313
loss:0.19984
loss:0.16350
loss:0.20070
loss:0.19342
loss:0.18343
loss:0.18972
loss:0.19415
loss:0.19370
loss:0.18573
loss:0.19341
loss:0.20164
loss:0.17837
loss:0.19664
loss:0.18376
loss:0.19401
loss:0.19594
loss:0.18041
loss:0.19369
loss:0.16228
loss:0.17673
loss:0.22020
loss:0.17728
loss:0.19710
loss:0.17888
loss:0.17869
loss:0.17214
loss:0.17421
loss:0.19530
loss:0.20264
loss:0.20765
loss:0.18069
loss:0.18784
loss:0.19903
loss:0.18353
loss:0.19570
loss:0.17934
loss:0.16252
loss:0.19303
loss:0.20104
loss:0.20703
loss:0.17815
loss:0.18757
loss:0.17431
loss:0.20193
loss:0.18591
loss:0.19695
loss:0.19312
loss:0.16793
loss:0.19968
loss:0.18964
loss:0.20451
loss:0.21119
loss:0.18987
loss:0.21672
loss:0.20323
loss:0.20621
loss:0.19447
loss:0.19188
loss:0.20517
loss:0.19589
loss:0.18642
loss:0.21099
loss:0.18625
loss:0.17196
loss:0.19880
loss:0.19572
loss:0.17935
loss:0.19895
loss:0.19669
loss:0.20199
loss:0.21164
loss:0.19544
loss:0.20782
loss:0.18554
loss:0.20034
loss:0.18857
loss:0.20263
loss:0.19196
loss:0.18484
loss:0.20605
loss:0.17022
loss:0.18847
loss:0.21606
loss:0.20077
loss:0.20779
loss:0.20564
loss:0.17038
loss:0.20453
loss:0.16726
loss:0.21182
loss:0.19750
loss:0.21717
loss:0.19188
loss:0.17288
loss:0.19917
loss:0.19148
loss:0.21276
loss:0.21845
loss:0.18724
loss:0.22341
loss:0.19600
loss:0.22184
loss:0.18534
loss:0.19242
loss:0.18581
loss:0.20827
loss:0.15368
loss:0.22274
loss:0.18003
loss:0.19726
loss:0.21934
loss:0.19503
loss:0.19796
loss:0.20890
loss:0.18300
loss:0.21047
loss:0.22210
loss:0.22059
loss:0.19781
loss:0.21569
loss:0.21367
loss:0.20255
loss:0.19452
loss:0.20748
loss:0.19606
loss:0.19199
loss:0.19094
loss:0.17952
loss:0.17383
loss:0.22110
loss:0.22120
loss:0.19943
loss:0.22147
loss:0.18907
loss:0.18994
loss:0.18376
loss:0.19310
loss:0.18471
loss:0.18581
loss:0.20489
loss:0.19556
loss:0.20241
loss:0.17223
loss:0.18454
loss:0.17856
loss:0.19591
Evaluation at Epoch 16/20. Step:14272/17840. AccuracyMetric: acc=0.883436

loss:0.21515
loss:0.20615
loss:0.16727
loss:0.17613
loss:0.18738
loss:0.17970
loss:0.15303
loss:0.16873
loss:0.18406
loss:0.16529
loss:0.20650
loss:0.16391
loss:0.18119
loss:0.20192
loss:0.18337
loss:0.18023
loss:0.19181
loss:0.19128
loss:0.17953
loss:0.19559
loss:0.18162
loss:0.16161
loss:0.16256
loss:0.19839
loss:0.19006
loss:0.17370
loss:0.16198
loss:0.17504
loss:0.15004
loss:0.17690
loss:0.20675
loss:0.20192
loss:0.20149
loss:0.17336
loss:0.19658
loss:0.18432
loss:0.18209
loss:0.18854
loss:0.20589
loss:0.16939
loss:0.18516
loss:0.16749
loss:0.19029
loss:0.20627
loss:0.17874
loss:0.16417
loss:0.17030
loss:0.19682
loss:0.19835
loss:0.18280
loss:0.16928
loss:0.22556
loss:0.19300
loss:0.15792
loss:0.20808
loss:0.18807
loss:0.17993
loss:0.20077
loss:0.18748
loss:0.15886
loss:0.19045
loss:0.17930
loss:0.17705
loss:0.19324
loss:0.20403
loss:0.21407
loss:0.17339
loss:0.19792
loss:0.19661
loss:0.18007
loss:0.20656
loss:0.19905
loss:0.19381
loss:0.19233
loss:0.19722
loss:0.17821
loss:0.18281
loss:0.17696
loss:0.17507
loss:0.19366
loss:0.17038
loss:0.17045
loss:0.22657
loss:0.19113
loss:0.20299
loss:0.17131
loss:0.16881
loss:0.17536
loss:0.18381
loss:0.19462
loss:0.19568
loss:0.15422
loss:0.21850
loss:0.15233
loss:0.20599
loss:0.20165
loss:0.16848
loss:0.16644
loss:0.19936
loss:0.20007
loss:0.19981
loss:0.18111
loss:0.19968
loss:0.19085
loss:0.16730
loss:0.17649
loss:0.20442
loss:0.17178
loss:0.19195
loss:0.17748
loss:0.17408
loss:0.21863
loss:0.18268
loss:0.17089
loss:0.16669
loss:0.19633
loss:0.19400
loss:0.18619
loss:0.18146
loss:0.17307
loss:0.19945
loss:0.19526
loss:0.21252
loss:0.18999
loss:0.18909
loss:0.21117
loss:0.17967
loss:0.19795
loss:0.18549
loss:0.19008
loss:0.20975
loss:0.20416
loss:0.18077
loss:0.20457
loss:0.18212
loss:0.19391
loss:0.18931
loss:0.20029
loss:0.17917
loss:0.19720
loss:0.18685
loss:0.22157
loss:0.15081
loss:0.18806
loss:0.20125
loss:0.21221
loss:0.18373
loss:0.16280
loss:0.17997
loss:0.21021
loss:0.17892
loss:0.19917
loss:0.20920
loss:0.18924
loss:0.17519
loss:0.20089
loss:0.19833
loss:0.17514
loss:0.17717
loss:0.19296
loss:0.19295
loss:0.18990
loss:0.19203
loss:0.18757
loss:0.22732
loss:0.18432
loss:0.20151
loss:0.16853
loss:0.19811
loss:0.17821
loss:0.18306
loss:0.21397
loss:0.19178
loss:0.17585
loss:0.19512
loss:0.19776
loss:0.16751
loss:0.19178
Evaluation at Epoch 17/20. Step:15164/17840. AccuracyMetric: acc=0.881838

loss:0.19325
loss:0.19050
loss:0.16916
loss:0.18310
loss:0.18299
loss:0.19199
loss:0.16306
loss:0.18481
loss:0.16139
loss:0.17505
loss:0.18876
loss:0.18715
loss:0.18595
loss:0.17049
loss:0.16594
loss:0.16019
loss:0.18025
loss:0.15283
loss:0.18172
loss:0.18519
loss:0.17833
loss:0.20253
loss:0.18440
loss:0.17508
loss:0.14863
loss:0.21175
loss:0.18202
loss:0.18065
loss:0.19219
loss:0.16098
loss:0.16553
loss:0.18522
loss:0.19701
loss:0.14406
loss:0.16369
loss:0.16184
loss:0.17489
loss:0.18527
loss:0.15834
loss:0.17810
loss:0.16779
loss:0.16175
loss:0.16385
loss:0.18320
loss:0.18743
loss:0.18787
loss:0.17431
loss:0.19313
loss:0.19249
loss:0.17872
loss:0.16736
loss:0.17151
loss:0.19434
loss:0.17145
loss:0.19503
loss:0.17041
loss:0.15776
loss:0.17129
loss:0.17804
loss:0.16800
loss:0.16692
loss:0.16385
loss:0.18244
loss:0.16098
loss:0.18685
loss:0.17136
loss:0.17201
loss:0.16742
loss:0.16662
loss:0.15687
loss:0.16618
loss:0.15934
loss:0.17104
loss:0.19825
loss:0.16840
loss:0.20398
loss:0.20646
loss:0.18011
loss:0.17306
loss:0.17202
loss:0.17001
loss:0.15882
loss:0.18132
loss:0.18789
loss:0.15991
loss:0.19869
loss:0.18304
loss:0.17222
loss:0.17002
loss:0.17785
loss:0.17714
loss:0.17628
loss:0.15467
loss:0.16826
loss:0.16430
loss:0.18661
loss:0.19279
loss:0.18844
loss:0.19372
loss:0.18283
loss:0.18447
loss:0.18683
loss:0.18864
loss:0.21886
loss:0.19596
loss:0.16446
loss:0.15607
loss:0.19056
loss:0.16832
loss:0.17767
loss:0.19399
loss:0.17656
loss:0.19666
loss:0.18677
loss:0.18919
loss:0.18152
loss:0.20485
loss:0.17993
loss:0.16171
loss:0.17726
loss:0.17204
loss:0.17434
loss:0.20766
loss:0.16283
loss:0.16761
loss:0.18750
loss:0.18589
loss:0.19252
loss:0.18042
loss:0.20960
loss:0.17009
loss:0.19909
loss:0.19134
loss:0.19828
loss:0.19112
loss:0.17787
loss:0.18308
loss:0.18164
loss:0.19219
loss:0.18548
loss:0.18788
loss:0.17654
loss:0.18051
loss:0.19086
loss:0.19256
loss:0.19969
loss:0.17057
loss:0.19218
loss:0.18114
loss:0.17405
loss:0.18428
loss:0.19258
loss:0.18925
loss:0.19791
loss:0.17161
loss:0.18889
loss:0.19246
loss:0.20598
loss:0.20752
loss:0.19462
loss:0.17984
loss:0.17260
loss:0.20762
loss:0.17747
loss:0.20647
loss:0.17464
loss:0.23342
loss:0.19696
loss:0.19613
loss:0.19295
loss:0.17251
loss:0.19485
loss:0.21368
loss:0.16737
loss:0.18646
loss:0.18534
loss:0.19964
loss:0.21860
loss:0.17983
Evaluation at Epoch 18/20. Step:16056/17840. AccuracyMetric: acc=0.877341

loss:0.15623
loss:0.17849
loss:0.18907
loss:0.17178
loss:0.16486
loss:0.18460
loss:0.16912
loss:0.16177
loss:0.18169
loss:0.16766
loss:0.16335
loss:0.16907
loss:0.17671
loss:0.15746
loss:0.15955
loss:0.16320
loss:0.17557
loss:0.16190
loss:0.17862
loss:0.19323
loss:0.17926
loss:0.15809
loss:0.14541
loss:0.16291
loss:0.16938
loss:0.15182
loss:0.15979
loss:0.16350
loss:0.17987
loss:0.17188
loss:0.17301
loss:0.14509
loss:0.18725
loss:0.18943
loss:0.17251
loss:0.15646
loss:0.17195
loss:0.17561
loss:0.15853
loss:0.17135
loss:0.17309
loss:0.18297
loss:0.16936
loss:0.16017
loss:0.16320
loss:0.17547
loss:0.14604
loss:0.17347
loss:0.16949
loss:0.17135
loss:0.15986
loss:0.20212
loss:0.16022
loss:0.18960
loss:0.18425
loss:0.15513
loss:0.16868
loss:0.16907
loss:0.18669
loss:0.20874
loss:0.19120
loss:0.17000
loss:0.17633
loss:0.18102
loss:0.18266
loss:0.17982
loss:0.18859
loss:0.15170
loss:0.15614
loss:0.15692
loss:0.18892
loss:0.16324
loss:0.18806
loss:0.16864
loss:0.15792
loss:0.16894
loss:0.15187
loss:0.20134
loss:0.16240
loss:0.18622
loss:0.16733
loss:0.17821
loss:0.16082
loss:0.16270
loss:0.19072
loss:0.16757
loss:0.16553
loss:0.16794
loss:0.15854
loss:0.14557
loss:0.16715
loss:0.16795
loss:0.15287
loss:0.20064
loss:0.18661
loss:0.17748
loss:0.17376
loss:0.17030
loss:0.18800
loss:0.16761
loss:0.17746
loss:0.18126
loss:0.17423
loss:0.17585
loss:0.17585
loss:0.17038
loss:0.19434
loss:0.17803
loss:0.18962
loss:0.16963
loss:0.17360
loss:0.19011
loss:0.18074
loss:0.19373
loss:0.18763
loss:0.18728
loss:0.15939
loss:0.17095
loss:0.16290
loss:0.19981
loss:0.19165
loss:0.18629
loss:0.16699
loss:0.18505
loss:0.18482
loss:0.17965
loss:0.17563
loss:0.19213
loss:0.18214
loss:0.19636
loss:0.16287
loss:0.18175
loss:0.19267
loss:0.16041
loss:0.18340
loss:0.16630
loss:0.21310
loss:0.20321
loss:0.17652
loss:0.16197
loss:0.20211
loss:0.16253
loss:0.18726
loss:0.17010
loss:0.18359
loss:0.19911
loss:0.15519
loss:0.19474
loss:0.18906
loss:0.17333
loss:0.18661
loss:0.16135
loss:0.18128
loss:0.16767
loss:0.20114
loss:0.19070
loss:0.16592
loss:0.18591
loss:0.15801
loss:0.16437
loss:0.17697
loss:0.17304
loss:0.20049
loss:0.18916
loss:0.20083
loss:0.17492
loss:0.19259
loss:0.18702
loss:0.17092
loss:0.17707
loss:0.18550
loss:0.17197
loss:0.17943
loss:0.18186
loss:0.18780
loss:0.20229
loss:0.17661
loss:0.17914
Evaluation at Epoch 19/20. Step:16948/17840. AccuracyMetric: acc=0.877231

loss:0.15635
loss:0.15715
loss:0.16602
loss:0.14723
loss:0.16063
loss:0.17105
loss:0.16427
loss:0.18928
loss:0.17505
loss:0.17143
loss:0.15117
loss:0.16119
loss:0.15707
loss:0.15625
loss:0.16639
loss:0.15010
loss:0.16880
loss:0.17704
loss:0.16258
loss:0.15558
loss:0.18675
loss:0.17147
loss:0.17331
loss:0.16035
loss:0.18851
loss:0.17788
loss:0.17120
loss:0.19136
loss:0.18821
loss:0.17121
loss:0.16063
loss:0.18628
loss:0.16124
loss:0.17202
loss:0.16937
loss:0.15070
loss:0.17269
loss:0.17431
loss:0.16973
loss:0.15985
loss:0.17614
loss:0.17749
loss:0.16521
loss:0.15341
loss:0.16322
loss:0.16744
loss:0.19235
loss:0.15373
loss:0.15043
loss:0.17935
loss:0.16253
loss:0.18701
loss:0.17434
loss:0.18204
loss:0.18545
loss:0.16064
loss:0.16482
loss:0.16979
loss:0.16831
loss:0.18023
loss:0.17717
loss:0.16880
loss:0.16406
loss:0.18925
loss:0.17304
loss:0.18964
loss:0.16119
loss:0.16872
loss:0.17835
loss:0.17346
loss:0.15035
loss:0.15657
loss:0.17598
loss:0.16386
loss:0.17447
loss:0.17093
loss:0.18564
loss:0.16506
loss:0.14935
loss:0.17812
loss:0.17996
loss:0.14505
loss:0.17454
loss:0.17351
loss:0.17985
loss:0.15798
loss:0.17143
loss:0.17575
loss:0.14215
loss:0.16304
loss:0.16133
loss:0.18342
loss:0.17446
loss:0.15882
loss:0.16762
loss:0.18620
loss:0.17367
loss:0.18237
loss:0.16317
loss:0.16408
loss:0.20229
loss:0.17810
loss:0.17843
loss:0.18373
loss:0.17244
loss:0.17346
loss:0.16783
loss:0.16146
loss:0.16038
loss:0.16361
loss:0.17177
loss:0.18097
loss:0.19244
loss:0.18330
loss:0.18785
loss:0.16606
loss:0.17466
loss:0.16492
loss:0.16775
loss:0.17186
loss:0.19828
loss:0.18080
loss:0.17445
loss:0.16002
loss:0.17185
loss:0.15761
loss:0.16279
loss:0.17148
loss:0.18034
loss:0.16008
loss:0.16152
loss:0.15374
loss:0.17238
loss:0.15904
loss:0.19430
loss:0.17509
loss:0.18342
loss:0.15218
loss:0.15907
loss:0.17082
loss:0.15234
loss:0.15636
loss:0.17236
loss:0.18226
loss:0.16766
loss:0.18902
loss:0.19179
loss:0.18550
loss:0.18624
loss:0.15709
loss:0.16441
loss:0.15773
loss:0.14736
loss:0.16636
loss:0.17329
loss:0.17222
loss:0.17683
loss:0.17140
loss:0.18680
loss:0.19329
loss:0.15868
loss:0.17924
loss:0.17453
loss:0.17133
loss:0.19865
loss:0.18414
loss:0.20495
loss:0.17741
loss:0.17795
loss:0.17976
loss:0.17816
loss:0.18310
loss:0.15899
loss:0.17303
loss:0.16075
loss:0.17896
loss:0.15367
loss:0.17637
loss:0.18988
Evaluation at Epoch 20/20. Step:17840/17840. AccuracyMetric: acc=0.877894


In Epoch:6/Step:5352, got best dev performance:AccuracyMetric: acc=0.890717
Reloaded the best model.
Train Done.
[tester] 
AccuracyMetric: acc=0.890717
Test Done.
