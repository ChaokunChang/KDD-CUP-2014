[34mAuto commit by fitlog[0m
Running with args:Namespace(batch_size=128, cuda=True, data_dir='../../data', data_src='all_data', dropout=0.3, embed_size=128, epochs=4, gpu='0', hidden_size=128, learning_rate=0.001, log_path='./run_records.log', max_seq_len=500, min_count=10, model='CNNText', model_dir='../../data/models', model_suffix='essay_is_exciting', num_layers=2, optim='Adam', patience=2, predict=False, prepare=True, prepare_dir='../../data/prepare/', pretrain=False, pretrain_model='None', reload_model_name='best_LSTMText_accuracy_2019-06-14-04-01-48', result_dir='../../data/results/', show_data=False, target_var='is_exciting', text_var='essay', train=True, vocab_data='vocab_essay_is_exciting.data', vocab_dir='../../data/vocab', weight_decay=0.0)
Checking the data files...
Checking the data files...
Generateing essay - is_exciting
Loading data from ../../data/essays_all_outcome.csv ...
nums:(291519,131329,44772)
Over Sample mode
subset nums:(291519,131329,44772)
0
tokenize train set
Tokenizing data , total num:291519
Tokenized:291519/291519
ATTENTION TYPE:<class 'float'> * 1038
tokenize val set
Tokenizing data , total num:131329
Tokenized:131329/131329
ATTENTION TYPE:<class 'float'> * 811
tokenize test set
Tokenizing data , total num:44772
Tokenized:44772/44772
ATTENTION TYPE:<class 'float'> * 688
Building Fastnlp dataset.
Over Sampling...
Building Fastnlp vocabulary.
Building id-presentation for train_set and test_set.
1070
Building target-vector for train_set and test_set.
Data Sizes (538736, 234618, 44772)
Saving vocab(TextData)...
Done with preparing!
(vocab_size,class_num,seq_len):(37380,2,1070)
No pretrained model with be used.
vocabsize:37380
Using CNN Model.
CNNText(
  (embed): Embedding(
    37380, 128
    (dropout): Dropout(p=0.0)
  )
  (conv_pool): ConvMaxpool(
    (convs): ModuleList(
      (0): Conv1d(128, 3, kernel_size=(3,), stride=(1,), padding=(2,))
      (1): Conv1d(128, 4, kernel_size=(4,), stride=(1,), padding=(2,))
      (2): Conv1d(128, 5, kernel_size=(5,), stride=(1,), padding=(2,))
    )
  )
  (dropout): Dropout(p=0.3)
  (fc): Linear(in_features=12, out_features=2, bias=True)
)
train_size:538736 ; val_size:234618 ; test_size:44772
Using Adam as optimizer.
input fields after batch(if batch size is 2):
	words: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2, 1070]) 
	seq_len: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2]) 
target fields after batch(if batch size is 2):
	target: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2]) 

training epochs started 2019-06-15-12-07-08
loss:2.36573
loss:1.76633
loss:1.46314
loss:1.25962
loss:1.15840
loss:1.11515
loss:1.06088
loss:1.01054
loss:0.97390
loss:0.95422
loss:0.97047
loss:0.91129
loss:0.98191
loss:0.95219
loss:0.90433
loss:0.92890
loss:0.93266
loss:0.83685
loss:0.81926
loss:0.86033
loss:0.84352
loss:0.91019
loss:0.84069
loss:0.85889
loss:0.79880
loss:0.80429
loss:0.79660
loss:0.75229
loss:0.82677
loss:0.79706
loss:0.77067
loss:0.77390
loss:0.76070
loss:0.76066
loss:0.75556
loss:0.74754
loss:0.74049
loss:0.73624
loss:0.74479
loss:0.74422
loss:0.72136
loss:0.74320
loss:0.72960
loss:0.74279
loss:0.74037
loss:0.73353
loss:0.75333
loss:0.73493
loss:0.73904
loss:0.74131
loss:0.71482
loss:0.71409
loss:0.72613
loss:0.72500
loss:0.71131
loss:0.71206
loss:0.71211
loss:0.70003
loss:0.72513
loss:0.71203
loss:0.71846
loss:0.72115
loss:0.71747
loss:0.70943
loss:0.69103
loss:0.70789
loss:0.70830
loss:0.70323
loss:0.70644
loss:0.69909
loss:0.70926
loss:0.70836
loss:0.69434
loss:0.68450
loss:0.69514
loss:0.68237
loss:0.69692
loss:0.68221
loss:0.67273
loss:0.70421
loss:0.68115
loss:0.69775
loss:0.67500
loss:0.68731
loss:0.66441
loss:0.66522
loss:0.67576
loss:0.67584
loss:0.68356
loss:0.68264
loss:0.68193
loss:0.68743
loss:0.68126
loss:0.67122
loss:0.68329
loss:0.67871
loss:0.67238
loss:0.66457
loss:0.65688
loss:0.67509
loss:0.66989
loss:0.66263
loss:0.68721
loss:0.66924
loss:0.66974
loss:0.66112
loss:0.66481
loss:0.67317
loss:0.66551
loss:0.64574
loss:0.67286
loss:0.65705
loss:0.67878
loss:0.66343
loss:0.66182
loss:0.66117
loss:0.65342
loss:0.67152
loss:0.66853
loss:0.67155
loss:0.67125
loss:0.66769
loss:0.66343
loss:0.66074
loss:0.65848
loss:0.65839
loss:0.67031
loss:0.67144
loss:0.66310
loss:0.66315
loss:0.68462
loss:0.65943
loss:0.66403
loss:0.65492
loss:0.65268
loss:0.64081
loss:0.65633
loss:0.64935
loss:0.65733
loss:0.66221
loss:0.65759
loss:0.65328
loss:0.65606
loss:0.65394
loss:0.66246
loss:0.65446
loss:0.65705
loss:0.63935
loss:0.64391
loss:0.66379
loss:0.65827
loss:0.65085
loss:0.66497
loss:0.65768
loss:0.65117
loss:0.67045
loss:0.63830
loss:0.64574
loss:0.64386
loss:0.66718
loss:0.65829
loss:0.65975
loss:0.63112
loss:0.62908
loss:0.63778
loss:0.63011
loss:0.63720
loss:0.64667
loss:0.64881
loss:0.66738
loss:0.62743
loss:0.65673
loss:0.64337
loss:0.64489
loss:0.63716
loss:0.67943
loss:0.64766
loss:0.64276
loss:0.65195
loss:0.64521
loss:0.64849
loss:0.65805
loss:0.63016
loss:0.63780
loss:0.63865
loss:0.64364
loss:0.63672
loss:0.65191
loss:0.64337
loss:0.62080
loss:0.64895
loss:0.63406
loss:0.64273
loss:0.66756
loss:0.65262
loss:0.63778
loss:0.63430
loss:0.62642
loss:0.66511
loss:0.63562
loss:0.63811
loss:0.65209
loss:0.63039
loss:0.64161
loss:0.63010
loss:0.65196
loss:0.65318
loss:0.63719
loss:0.64691
loss:0.64138
loss:0.62652
loss:0.63301
loss:0.64943
loss:0.64292
loss:0.64256
loss:0.64372
loss:0.61987
loss:0.64353
loss:0.63900
loss:0.65914
loss:0.64011
loss:0.65094
loss:0.61658
loss:0.65519
loss:0.62174
loss:0.63670
loss:0.62358
loss:0.63198
loss:0.62532
loss:0.62238
loss:0.61559
loss:0.63054
loss:0.63608
loss:0.62874
loss:0.66191
loss:0.62756
loss:0.64443
loss:0.62809
loss:0.64319
loss:0.65264
loss:0.64269
loss:0.64409
loss:0.61943
loss:0.61730
loss:0.63581
loss:0.63609
loss:0.64819
loss:0.63478
loss:0.62796
loss:0.62720
loss:0.64212
loss:0.65507
loss:0.64589
loss:0.62751
loss:0.62706
loss:0.64479
loss:0.64222
loss:0.65062
loss:0.63481
loss:0.62296
loss:0.63784
loss:0.64093
loss:0.63787
loss:0.65719
loss:0.63418
loss:0.62359
loss:0.64763
loss:0.64441
loss:0.63799
loss:0.63516
loss:0.65883
loss:0.64071
loss:0.63554
loss:0.66046
loss:0.61134
loss:0.63389
loss:0.63647
loss:0.64457
loss:0.63347
loss:0.63439
loss:0.63898
loss:0.62781
loss:0.62792
loss:0.62216
loss:0.64312
loss:0.61339
loss:0.62768
loss:0.65119
loss:0.63466
loss:0.62615
loss:0.64839
loss:0.62698
loss:0.62336
loss:0.62448
loss:0.62181
loss:0.63416
loss:0.63616
loss:0.62279
loss:0.63476
loss:0.63541
loss:0.64140
loss:0.64132
loss:0.64295
loss:0.62787
loss:0.63021
loss:0.63276
loss:0.64239
loss:0.62883
loss:0.63308
loss:0.63404
loss:0.63260
loss:0.64539
loss:0.63223
loss:0.62906
loss:0.63408
loss:0.63143
loss:0.62791
loss:0.61624
loss:0.64255
loss:0.58677
loss:0.63626
loss:0.64188
loss:0.63684
loss:0.62370
loss:0.59329
loss:0.63709
loss:0.63870
loss:0.63589
loss:0.61959
loss:0.63003
loss:0.62487
loss:0.63270
loss:0.63953
loss:0.63263
loss:0.63555
loss:0.61369
loss:0.63474
loss:0.64590
loss:0.62310
loss:0.62388
loss:0.63985
loss:0.61907
loss:0.62503
loss:0.62210
loss:0.62329
loss:0.62957
loss:0.62769
loss:0.59448
loss:0.65141
loss:0.65370
loss:0.62750
loss:0.61377
loss:0.65416
loss:0.62518
loss:0.63018
loss:0.62673
loss:0.62264
loss:0.61895
loss:0.64321
loss:0.61755
loss:0.65016
loss:0.63672
loss:0.63018
loss:0.62330
loss:0.60040
loss:0.61078
loss:0.60475
loss:0.62619
loss:0.61795
loss:0.61606
loss:0.62133
loss:0.60322
loss:0.61998
loss:0.61239
loss:0.63787
loss:0.60322
loss:0.65555
loss:0.63450
loss:0.61777
loss:0.63420
loss:0.61028
loss:0.61176
loss:0.60791
loss:0.61230
loss:0.63464
loss:0.63552
loss:0.62998
loss:0.64075
loss:0.62250
loss:0.60647
loss:0.60053
loss:0.63510
loss:0.61599
loss:0.60891
loss:0.60760
loss:0.60558
loss:0.63673
loss:0.60697
loss:0.64226
loss:0.62772
loss:0.61916
loss:0.59146
loss:0.61524
loss:0.59974
loss:0.63563
loss:0.62694
loss:0.61239
loss:0.63904
loss:0.60637
loss:0.61462
loss:0.60268
loss:0.63541
loss:0.60577
loss:0.62207
loss:0.62399
loss:0.62332
loss:0.62424
loss:0.63463
loss:0.61229
loss:0.61022
loss:0.60306
loss:0.61337
loss:0.59614
loss:0.63499
loss:0.63747
loss:0.62590
loss:0.60311
loss:0.62148
loss:0.61695
loss:0.63507
loss:0.63482
loss:0.61364
loss:0.61555
loss:0.62219
loss:0.61141
loss:0.61980
loss:0.63646
loss:0.59169
loss:0.60392
loss:0.61412
loss:0.61118
loss:0.59416
loss:0.60491
loss:0.59213
loss:0.62144
loss:0.62248
loss:0.59486
loss:0.60724
loss:0.61400
loss:0.62061
loss:0.60689
loss:0.62556
loss:0.61847
loss:0.59285
loss:0.58979
loss:0.63664
loss:0.60987
loss:0.60525
loss:0.61492
loss:0.64015
loss:0.63733
loss:0.60305
loss:0.60016
loss:0.58322
loss:0.57138
loss:0.61586
loss:0.59245
loss:0.60689
loss:0.61449
loss:0.60562
loss:0.63721
loss:0.60418
loss:0.63037
loss:0.60802
loss:0.60752
loss:0.63121
loss:0.61961
loss:0.60690
loss:0.59672
loss:0.62927
loss:0.61447
loss:0.58917
loss:0.60556
loss:0.61223
loss:0.59174
loss:0.60697
loss:0.58136
loss:0.59976
loss:0.60684
loss:0.62586
loss:0.57516
loss:0.59532
loss:0.62369
loss:0.61083
loss:0.61237
loss:0.60269
loss:0.61593
loss:0.60641
loss:0.59568
loss:0.57953
loss:0.59884
loss:0.58651
loss:0.58765
loss:0.60728
loss:0.63169
loss:0.58721
loss:0.60631
loss:0.61957
loss:0.62205
loss:0.63526
loss:0.59347
loss:0.60184
loss:0.61992
loss:0.60549
loss:0.59821
loss:0.62211
loss:0.63321
loss:0.59533
loss:0.61342
loss:0.61903
loss:0.60065
loss:0.60475
loss:0.64022
loss:0.62529
loss:0.61076
loss:0.61109
loss:0.59961
loss:0.61353
loss:0.60607
loss:0.61814
loss:0.61020
loss:0.60385
loss:0.61018
loss:0.60174
loss:0.59511
loss:0.58607
loss:0.60197
loss:0.59627
loss:0.59087
loss:0.60017
loss:0.62120
loss:0.58878
loss:0.59622
loss:0.59384
loss:0.61389
loss:0.61782
loss:0.60098
loss:0.59651
loss:0.59378
loss:0.59615
loss:0.61404
loss:0.61970
loss:0.61699
loss:0.59066
loss:0.58720
loss:0.60224
loss:0.62690
loss:0.58642
loss:0.57577
loss:0.61499
loss:0.62430
loss:0.56944
loss:0.62358
loss:0.59453
loss:0.55764
loss:0.61152
loss:0.60099
loss:0.59460
loss:0.59586
loss:0.63080
loss:0.60727
loss:0.58826
loss:0.60026
loss:0.57144
loss:0.60186
loss:0.59406
loss:0.57499
loss:0.60430
loss:0.61541
loss:0.59213
loss:0.59383
loss:0.59801
loss:0.61410
loss:0.59811
loss:0.59934
loss:0.57543
loss:0.58797
loss:0.59889
loss:0.61156
loss:0.57740
loss:0.62817
loss:0.59832
loss:0.60356
loss:0.58559
loss:0.58869
loss:0.59644
loss:0.61585
loss:0.60444
loss:0.57978
loss:0.62543
loss:0.60123
loss:0.59785
loss:0.61099
loss:0.60351
loss:0.58574
loss:0.60518
loss:0.60880
loss:0.59005
loss:0.60527
loss:0.58644
loss:0.57522
loss:0.61907
loss:0.57439
loss:0.59777
loss:0.59543
loss:0.58890
loss:0.58080
loss:0.61631
loss:0.61867
loss:0.58956
loss:0.60986
loss:0.60600
loss:0.59446
loss:0.60446
loss:0.60205
loss:0.60426
loss:0.61588
loss:0.57843
loss:0.62396
loss:0.61500
loss:0.58905
loss:0.57505
loss:0.57343
loss:0.58781
loss:0.60860
loss:0.57396
loss:0.60781
loss:0.56917
loss:0.61044
loss:0.59484
loss:0.59414
loss:0.61830
loss:0.59517
loss:0.59479
loss:0.61027
loss:0.60878
loss:0.57811
loss:0.57980
loss:0.60198
loss:0.59635
loss:0.60028
loss:0.59058
loss:0.60370
loss:0.58498
loss:0.57593
loss:0.58893
loss:0.58727
loss:0.63405
loss:0.59148
loss:0.59122
loss:0.60404
loss:0.62139
loss:0.58564
loss:0.61574
loss:0.58218
loss:0.57405
loss:0.56562
loss:0.60524
loss:0.59766
loss:0.60949
loss:0.58678
loss:0.55710
loss:0.57768
loss:0.58029
loss:0.60769
loss:0.60013
loss:0.61033
loss:0.62354
loss:0.58366
loss:0.61058
loss:0.59556
loss:0.57074
loss:0.58122
loss:0.58098
loss:0.60465
loss:0.58718
loss:0.56882
loss:0.60484
loss:0.60656
loss:0.60036
loss:0.63000
loss:0.58649
loss:0.58643
loss:0.60204
loss:0.59495
loss:0.56734
loss:0.60750
loss:0.58638
loss:0.59657
loss:0.57103
loss:0.57569
loss:0.58538
loss:0.57634
loss:0.59940
loss:0.57096
loss:0.60366
loss:0.57976
loss:0.57618
loss:0.56990
loss:0.55942
loss:0.59652
loss:0.61844
loss:0.57336
loss:0.60470
loss:0.59437
loss:0.58690
loss:0.55857
loss:0.58528
loss:0.61074
loss:0.55144
loss:0.57143
loss:0.58620
loss:0.58681
loss:0.58337
loss:0.59482
loss:0.56812
loss:0.59019
loss:0.57975
loss:0.56007
loss:0.59356
loss:0.57109
loss:0.57039
loss:0.58761
loss:0.56811
loss:0.59339
loss:0.57830
loss:0.61017
loss:0.59217
loss:0.58091
loss:0.59952
loss:0.57613
loss:0.57370
loss:0.60251
loss:0.58839
loss:0.57246
loss:0.60675
loss:0.57108
loss:0.59874
loss:0.58615
loss:0.58909
loss:0.55992
loss:0.58711
loss:0.57513
loss:0.58742
loss:0.60242
loss:0.58661
loss:0.61215
loss:0.56907
loss:0.59184
loss:0.58465
loss:0.58613
loss:0.58583
loss:0.59599
loss:0.56169
loss:0.58163
loss:0.57099
loss:0.59581
loss:0.57076
loss:0.58376
loss:0.60458
loss:0.55462
loss:0.57139
loss:0.57956
loss:0.58308
loss:0.57430
loss:0.58642
loss:0.57274
loss:0.57349
loss:0.58991
loss:0.57266
loss:0.57883
loss:0.59995
loss:0.57990
loss:0.54551
loss:0.58923
loss:0.55727
loss:0.59177
loss:0.59394
loss:0.59187
loss:0.59523
loss:0.56963
loss:0.58146
loss:0.56574
loss:0.55125
loss:0.56012
loss:0.57600
loss:0.58212
loss:0.60860
loss:0.53625
loss:0.54246
loss:0.53585
loss:0.60057
loss:0.58154
loss:0.60196
loss:0.56070
loss:0.58922
loss:0.56790
loss:0.60236
loss:0.56406
loss:0.60567
loss:0.60184
loss:0.57833
loss:0.53385
loss:0.55753
loss:0.59318
loss:0.54800
loss:0.59042
loss:0.59659
loss:0.55636
loss:0.57558
loss:0.55033
loss:0.54605
loss:0.55651
loss:0.59177
loss:0.58063
loss:0.59817
loss:0.58161
loss:0.57831
loss:0.56034
loss:0.54582
loss:0.56609
loss:0.57676
loss:0.56766
loss:0.58514
loss:0.58197
loss:0.55581
loss:0.56490
loss:0.59031
loss:0.59320
loss:0.59357
loss:0.58191
Evaluation at Epoch 1/4. Step:4209/16836. AccuracyMetric: acc=0.529333

loss:0.59644
loss:0.57509
loss:0.56047
loss:0.54917
loss:0.55376
loss:0.56153
loss:0.56260
loss:0.55407
loss:0.57094
loss:0.53524
loss:0.57496
loss:0.53411
loss:0.54263
loss:0.54858
loss:0.55971
loss:0.57405
loss:0.54515
loss:0.56029
loss:0.53178
loss:0.56419
loss:0.54319
loss:0.55047
loss:0.56183
loss:0.56837
loss:0.52900
loss:0.55231
loss:0.55980
loss:0.56297
loss:0.55332
loss:0.55029
loss:0.55829
loss:0.55560
loss:0.57647
loss:0.56661
loss:0.56081
loss:0.57648
loss:0.56727
loss:0.55121
loss:0.55082
loss:0.54485
loss:0.56425
loss:0.54108
loss:0.52762
loss:0.52522
loss:0.59477
loss:0.54576
loss:0.53753
loss:0.56173
loss:0.53715
loss:0.55637
loss:0.55715
loss:0.57161
loss:0.52978
loss:0.53876
loss:0.55818
loss:0.54671
loss:0.57331
loss:0.52973
loss:0.55037
loss:0.57486
loss:0.60064
loss:0.53007
loss:0.55336
loss:0.58178
loss:0.55704
loss:0.54951
loss:0.56101
loss:0.55959
loss:0.55833
loss:0.54187
loss:0.58658
loss:0.54265
loss:0.57925
loss:0.57165
loss:0.54918
loss:0.57771
loss:0.57662
loss:0.56101
loss:0.54734
loss:0.59980
loss:0.55311
loss:0.57094
loss:0.53738
loss:0.54742
loss:0.56050
loss:0.56075
loss:0.56624
loss:0.55616
loss:0.56005
loss:0.55628
loss:0.55684
loss:0.55598
loss:0.55307
loss:0.57609
loss:0.56797
loss:0.55106
loss:0.55864
loss:0.56236
loss:0.56562
loss:0.53771
loss:0.54650
loss:0.55102
loss:0.54779
loss:0.54415
loss:0.58415
loss:0.56521
loss:0.56314
loss:0.55776
loss:0.53412
loss:0.54349
loss:0.55570
loss:0.56717
loss:0.52906
loss:0.55625
loss:0.56215
loss:0.57332
loss:0.52491
loss:0.54706
loss:0.54039
loss:0.54782
loss:0.54795
loss:0.54498
loss:0.52453
loss:0.55947
loss:0.54876
loss:0.53250
loss:0.55264
loss:0.53942
loss:0.53067
loss:0.56447
loss:0.51788
loss:0.53160
loss:0.54371
loss:0.54478
loss:0.53982
loss:0.54756
loss:0.55019
loss:0.56425
loss:0.55763
loss:0.50374
loss:0.53416
loss:0.52358
loss:0.57551
loss:0.54876
loss:0.57856
loss:0.55353
loss:0.52188
loss:0.52729
loss:0.55902
loss:0.53377
loss:0.51951
loss:0.55912
loss:0.54972
loss:0.56818
loss:0.54668
loss:0.54135
loss:0.54285
loss:0.50759
loss:0.54808
loss:0.57467
loss:0.53344
loss:0.55835
loss:0.53595
loss:0.53580
loss:0.55633
loss:0.54323
loss:0.54071
loss:0.53918
loss:0.54397
loss:0.53591
loss:0.55373
loss:0.54300
loss:0.55036
loss:0.54383
loss:0.54596
loss:0.52949
loss:0.56500
loss:0.55909
loss:0.55286
loss:0.51631
loss:0.50486
loss:0.53940
loss:0.55696
loss:0.53653
loss:0.53970
loss:0.52911
loss:0.54858
loss:0.55216
loss:0.55725
loss:0.53366
loss:0.53318
loss:0.55205
loss:0.52005
loss:0.54038
loss:0.53993
loss:0.53184
loss:0.57790
loss:0.55349
loss:0.53299
loss:0.53731
loss:0.56079
loss:0.54390
loss:0.56835
loss:0.52958
loss:0.55827
loss:0.55693
loss:0.53070
loss:0.53382
loss:0.54648
loss:0.55651
loss:0.52546
loss:0.54269
loss:0.50205
loss:0.52640
loss:0.53653
loss:0.52392
loss:0.56720
loss:0.53812
loss:0.54771
loss:0.55482
loss:0.52971
loss:0.54012
loss:0.55965
loss:0.54542
loss:0.54065
loss:0.52654
loss:0.56824
loss:0.52637
loss:0.57432
loss:0.56667
loss:0.53239
loss:0.52369
loss:0.55274
loss:0.57470
loss:0.54246
loss:0.53327
loss:0.57338
loss:0.53053
loss:0.52028
loss:0.53737
loss:0.56226
loss:0.54295
loss:0.54093
loss:0.54161
loss:0.50676
loss:0.57015
loss:0.53098
loss:0.52855
loss:0.54358
loss:0.54717
loss:0.53303
loss:0.54498
loss:0.52827
loss:0.56416
loss:0.52414
loss:0.56693
loss:0.51564
loss:0.56910
loss:0.51789
loss:0.51949
loss:0.56141
loss:0.52786
loss:0.55024
loss:0.55373
loss:0.52905
loss:0.53269
loss:0.53454
loss:0.54494
loss:0.55969
loss:0.53964
loss:0.53384
loss:0.53435
loss:0.53306
loss:0.53011
loss:0.50560
loss:0.55900
loss:0.56255
loss:0.56393
loss:0.53281
loss:0.52434
loss:0.54293
loss:0.51475
loss:0.59154
loss:0.53077
loss:0.54438
loss:0.54063
loss:0.50228
loss:0.53930
loss:0.52623
loss:0.59042
loss:0.53907
loss:0.54777
loss:0.56022
loss:0.53608
loss:0.53666
loss:0.51356
loss:0.52279
loss:0.52614
loss:0.54560
loss:0.53993
loss:0.52718
loss:0.52911
loss:0.53122
loss:0.53042
loss:0.54761
loss:0.56744
loss:0.54345
loss:0.56539
loss:0.54450
loss:0.52831
loss:0.53388
loss:0.58662
loss:0.57697
loss:0.52942
loss:0.55264
loss:0.53320
loss:0.54130
loss:0.53450
loss:0.56686
loss:0.51267
loss:0.53364
loss:0.52146
loss:0.52916
loss:0.55243
loss:0.50576
loss:0.55292
loss:0.52551
loss:0.51594
loss:0.47362
loss:0.51083
loss:0.53578
loss:0.52510
loss:0.48647
loss:0.55446
loss:0.52037
loss:0.53935
loss:0.53739
loss:0.54290
loss:0.51311
loss:0.49781
loss:0.57053
loss:0.50323
loss:0.55110
loss:0.55448
loss:0.54864
loss:0.53105
loss:0.57006
loss:0.53148
loss:0.54309
loss:0.54879
loss:0.53519
loss:0.51992
loss:0.53593
loss:0.54171
loss:0.53614
loss:0.55202
loss:0.53108
loss:0.49726
loss:0.57006
loss:0.53743
loss:0.54951
loss:0.51900
loss:0.53130
loss:0.52077
loss:0.51523
loss:0.53395
loss:0.56149
loss:0.55652
loss:0.52242
loss:0.55177
loss:0.51890
loss:0.54002
loss:0.53043
loss:0.51550
loss:0.58774
loss:0.52027
loss:0.55438
loss:0.54561
loss:0.54288
loss:0.53331
loss:0.54890
loss:0.54029
loss:0.54223
loss:0.50812
loss:0.55552
loss:0.55844
loss:0.53632
loss:0.53800
loss:0.53247
loss:0.51814
loss:0.50043
loss:0.50212
loss:0.48450
loss:0.53183
loss:0.52084
loss:0.51554
loss:0.54499
loss:0.53986
loss:0.51696
loss:0.53257
loss:0.53587
loss:0.53315
loss:0.57024
loss:0.49840
loss:0.53344
loss:0.52120
loss:0.54917
loss:0.51625
loss:0.54742
loss:0.54882
loss:0.49780
loss:0.52915
loss:0.52357
loss:0.52170
loss:0.53452
loss:0.53463
loss:0.55276
loss:0.53826
loss:0.52442
loss:0.54091
loss:0.55001
loss:0.51483
loss:0.52441
loss:0.53419
loss:0.51968
loss:0.57063
loss:0.53181
loss:0.53772
loss:0.49426
loss:0.51516
loss:0.55592
loss:0.52324
loss:0.55049
loss:0.53763
loss:0.53845
loss:0.52696
loss:0.51369
loss:0.51091
loss:0.53015
loss:0.53915
loss:0.52578
loss:0.48659
loss:0.52840
loss:0.50453
loss:0.57239
loss:0.50913
loss:0.54845
loss:0.51775
loss:0.53218
loss:0.49272
loss:0.54966
loss:0.53384
loss:0.53464
loss:0.53519
loss:0.51331
loss:0.55587
loss:0.53551
loss:0.50785
loss:0.54393
loss:0.50094
loss:0.55348
loss:0.50451
loss:0.52069
loss:0.51481
loss:0.53644
loss:0.55425
loss:0.48795
loss:0.53605
loss:0.49086
loss:0.53426
loss:0.50653
loss:0.51798
loss:0.50814
loss:0.52422
loss:0.52736
loss:0.53015
loss:0.53205
loss:0.51810
loss:0.53013
loss:0.50819
loss:0.52382
loss:0.50448
loss:0.50200
loss:0.49156
loss:0.52849
loss:0.54505
loss:0.48020
loss:0.51109
loss:0.51953
loss:0.53017
loss:0.53543
loss:0.50662
loss:0.53142
loss:0.53452
loss:0.50540
loss:0.54952
loss:0.47649
loss:0.52728
loss:0.51861
loss:0.51339
loss:0.52671
loss:0.56624
loss:0.49815
loss:0.49258
loss:0.54068
loss:0.51226
loss:0.51408
loss:0.49512
loss:0.53728
loss:0.49566
loss:0.54068
loss:0.48449
loss:0.48554
loss:0.53528
loss:0.53932
loss:0.50307
loss:0.52651
loss:0.48684
loss:0.53238
loss:0.51963
loss:0.52668
loss:0.53631
loss:0.53990
loss:0.55042
loss:0.49903
loss:0.49953
loss:0.51332
loss:0.50651
loss:0.53309
loss:0.52725
loss:0.51393
loss:0.52998
loss:0.51070
loss:0.52131
loss:0.48660
loss:0.49758
loss:0.52318
loss:0.52541
loss:0.51726
loss:0.50027
loss:0.50023
loss:0.51644
loss:0.52056
loss:0.50365
loss:0.54148
loss:0.51787
loss:0.48274
loss:0.52246
loss:0.50526
loss:0.51900
loss:0.50697
loss:0.53768
loss:0.51693
loss:0.51249
loss:0.53552
loss:0.51593
loss:0.49932
loss:0.53054
loss:0.51325
loss:0.53988
loss:0.49579
loss:0.51693
loss:0.53126
loss:0.50322
loss:0.50642
loss:0.49822
loss:0.51295
loss:0.52419
loss:0.49974
loss:0.50780
loss:0.52634
loss:0.50743
loss:0.53593
loss:0.49504
loss:0.53063
loss:0.54371
loss:0.49283
loss:0.55829
loss:0.49709
loss:0.52727
loss:0.50538
loss:0.47149
loss:0.48838
loss:0.53038
loss:0.51536
loss:0.52895
loss:0.52092
loss:0.47734
loss:0.49506
loss:0.52710
loss:0.49833
loss:0.50812
loss:0.48850
loss:0.48953
loss:0.52630
loss:0.48426
loss:0.51879
loss:0.52085
loss:0.47324
loss:0.51205
loss:0.48246
loss:0.52460
loss:0.52699
loss:0.49175
loss:0.51625
loss:0.51404
loss:0.52475
loss:0.53435
loss:0.49166
loss:0.50458
loss:0.48643
loss:0.53861
loss:0.52763
loss:0.52849
loss:0.55322
loss:0.51289
loss:0.51940
loss:0.48713
loss:0.53908
loss:0.50790
loss:0.47810
loss:0.49618
loss:0.50539
loss:0.51491
loss:0.50775
loss:0.52373
loss:0.50871
loss:0.49900
loss:0.51921
loss:0.50913
loss:0.50710
loss:0.50155
loss:0.52312
loss:0.46809
loss:0.50926
loss:0.53418
loss:0.51719
loss:0.53253
loss:0.51665
loss:0.52126
loss:0.48929
loss:0.51383
loss:0.51477
loss:0.50478
loss:0.50584
loss:0.48538
loss:0.51828
loss:0.51648
loss:0.50717
loss:0.51343
loss:0.50943
loss:0.52430
loss:0.50858
loss:0.50748
loss:0.48299
loss:0.51190
loss:0.53777
loss:0.51056
loss:0.53827
loss:0.50701
loss:0.52466
loss:0.51802
loss:0.51004
loss:0.50629
loss:0.52199
loss:0.49711
loss:0.49117
loss:0.49928
loss:0.53336
loss:0.48268
loss:0.51103
loss:0.51206
loss:0.49478
loss:0.48262
loss:0.48420
loss:0.51870
loss:0.50837
loss:0.51938
loss:0.54403
loss:0.50383
loss:0.51106
loss:0.48579
loss:0.50870
loss:0.53206
loss:0.47674
loss:0.47799
loss:0.50968
loss:0.50282
loss:0.51183
loss:0.49497
loss:0.51562
loss:0.50733
loss:0.50796
loss:0.52934
loss:0.48293
loss:0.50752
loss:0.49254
loss:0.50092
loss:0.49506
loss:0.53190
loss:0.52815
loss:0.50561
loss:0.49502
loss:0.47005
loss:0.54042
loss:0.51697
loss:0.53359
loss:0.51935
loss:0.48484
loss:0.49118
loss:0.51026
loss:0.53198
loss:0.52404
loss:0.50058
loss:0.46998
loss:0.47925
loss:0.53825
loss:0.50037
loss:0.50052
loss:0.50093
loss:0.52410
loss:0.50861
loss:0.51978
loss:0.49441
loss:0.52894
loss:0.54004
loss:0.49741
loss:0.48789
loss:0.49364
loss:0.49702
loss:0.50228
loss:0.50646
loss:0.51604
loss:0.52828
loss:0.49887
loss:0.48460
loss:0.49408
loss:0.47766
loss:0.47460
loss:0.47368
loss:0.49489
loss:0.51646
loss:0.54747
loss:0.48061
loss:0.52068
loss:0.49918
loss:0.50063
loss:0.49627
loss:0.49334
loss:0.49959
loss:0.50406
loss:0.48220
loss:0.50926
loss:0.49992
loss:0.49270
loss:0.48297
loss:0.49880
loss:0.47742
loss:0.50378
loss:0.47198
loss:0.45477
loss:0.47914
loss:0.48949
loss:0.47453
loss:0.53264
loss:0.49785
loss:0.52051
loss:0.50813
loss:0.47925
loss:0.49339
loss:0.48855
loss:0.52713
loss:0.52336
loss:0.50939
loss:0.52953
loss:0.49850
loss:0.49775
loss:0.51273
loss:0.48255
loss:0.49181
loss:0.51106
loss:0.48545
loss:0.49885
loss:0.46139
loss:0.51333
loss:0.54988
loss:0.49353
loss:0.49113
loss:0.52335
loss:0.51626
loss:0.47202
loss:0.50954
loss:0.50461
loss:0.48895
loss:0.51209
loss:0.51533
loss:0.49097
loss:0.49076
loss:0.48818
loss:0.48706
loss:0.48102
loss:0.50932
loss:0.50343
loss:0.50253
loss:0.48345
loss:0.49610
loss:0.46865
loss:0.47202
loss:0.52815
loss:0.46665
loss:0.52040
loss:0.50564
loss:0.51162
loss:0.46233
loss:0.46557
loss:0.48380
loss:0.45825
loss:0.48501
loss:0.46446
loss:0.47692
loss:0.47872
loss:0.48447
loss:0.49048
loss:0.50583
loss:0.51001
loss:0.51213
loss:0.51248
loss:0.49084
loss:0.49685
loss:0.49071
loss:0.49265
loss:0.49405
loss:0.48339
loss:0.48672
loss:0.52531
loss:0.49494
loss:0.45544
loss:0.47798
loss:0.48243
loss:0.46971
loss:0.48080
loss:0.49363
Evaluation at Epoch 2/4. Step:8418/16836. AccuracyMetric: acc=0.523263

loss:0.45901
loss:0.48535
loss:0.43072
loss:0.47753
loss:0.45772
loss:0.48307
loss:0.47086
loss:0.46954
loss:0.48813
loss:0.46249
loss:0.48079
loss:0.46006
loss:0.48445
loss:0.46743
loss:0.48726
loss:0.49673
loss:0.45754
loss:0.45418
loss:0.52565
loss:0.49969
loss:0.47253
loss:0.48429
loss:0.45246
loss:0.47225
loss:0.49428
loss:0.45720
loss:0.49610
loss:0.44717
loss:0.49192
loss:0.48974
loss:0.46576
loss:0.47689
loss:0.48874
loss:0.45965
loss:0.45527
loss:0.45730
loss:0.49091
loss:0.48657
loss:0.42317
loss:0.45939
loss:0.48073
loss:0.44479
loss:0.47740
loss:0.42791
loss:0.51863
loss:0.44060
loss:0.47430
loss:0.47926
loss:0.46511
loss:0.46188
loss:0.49471
loss:0.48322
loss:0.49808
loss:0.45938
loss:0.46377
loss:0.49285
loss:0.48530
loss:0.49180
loss:0.45214
loss:0.46104
loss:0.48875
loss:0.46897
loss:0.47675
loss:0.48914
loss:0.46953
loss:0.49287
loss:0.49487
loss:0.44638
loss:0.47199
loss:0.46195
loss:0.46994
loss:0.47014
loss:0.46463
loss:0.45159
loss:0.47143
loss:0.51222
loss:0.45319
loss:0.42829
loss:0.46469
loss:0.47036
loss:0.52056
loss:0.46840
loss:0.43116
loss:0.46900
loss:0.46783
loss:0.46249
loss:0.45828
loss:0.46935
loss:0.47450
loss:0.44455
loss:0.44171
loss:0.44969
loss:0.43565
loss:0.45953
loss:0.47497
loss:0.42198
loss:0.42597
loss:0.43354
loss:0.44994
loss:0.47644
loss:0.44615
loss:0.46565
loss:0.47653
loss:0.49526
loss:0.47357
loss:0.48494
loss:0.48014
loss:0.43549
loss:0.44832
loss:0.44957
loss:0.48699
loss:0.44254
loss:0.46768
loss:0.47834
loss:0.42931
loss:0.47147
loss:0.47481
loss:0.45426
loss:0.49301
loss:0.49752
loss:0.43819
loss:0.43355
loss:0.41075
loss:0.46149
loss:0.46567
loss:0.43237
loss:0.46949
loss:0.45904
loss:0.45875
loss:0.43841
loss:0.46279
loss:0.44647
loss:0.45690
loss:0.49389
loss:0.48840
loss:0.45898
loss:0.47881
loss:0.49979
loss:0.47879
loss:0.43067
loss:0.45372
loss:0.47335
loss:0.47461
loss:0.45592
loss:0.50235
loss:0.43330
loss:0.48348
loss:0.47168
loss:0.45723
loss:0.50493
loss:0.45445
loss:0.45326
loss:0.47121
loss:0.45680
loss:0.46009
loss:0.42940
loss:0.46592
loss:0.43588
loss:0.46002
loss:0.46734
loss:0.48087
loss:0.46237
loss:0.44327
loss:0.47061
loss:0.48396
loss:0.46132
loss:0.46245
loss:0.46315
loss:0.43139
loss:0.50304
loss:0.49166
loss:0.46093
loss:0.45062
loss:0.44147
loss:0.44240
loss:0.45289
loss:0.44916
loss:0.44489
loss:0.46143
loss:0.46217
loss:0.47619
loss:0.49818
loss:0.45203
loss:0.44420
loss:0.46057
loss:0.44937
loss:0.48334
loss:0.45422
loss:0.43829
loss:0.46904
loss:0.43310
loss:0.44589
loss:0.44329
loss:0.45556
loss:0.49577
loss:0.48573
loss:0.43244
loss:0.48464
loss:0.44926
loss:0.44482
loss:0.46559
loss:0.44171
loss:0.48743
loss:0.44220
loss:0.48100
loss:0.49147
loss:0.46855
loss:0.48652
loss:0.43563
loss:0.49083
loss:0.45111
loss:0.46064
loss:0.45055
loss:0.44108
loss:0.42976
loss:0.43339
loss:0.47719
loss:0.44889
loss:0.45297
loss:0.45752
loss:0.48283
loss:0.45302
loss:0.46000
loss:0.42802
loss:0.44424
loss:0.46997
loss:0.46145
loss:0.46100
loss:0.44283
loss:0.47432
loss:0.43522
loss:0.44341
loss:0.42461
loss:0.43498
loss:0.44545
loss:0.41608
loss:0.45474
loss:0.42411
loss:0.48701
loss:0.44553
loss:0.42603
loss:0.47705
loss:0.46783
loss:0.46455
loss:0.44033
loss:0.49156
loss:0.43267
loss:0.48423
loss:0.45589
loss:0.44669
loss:0.45279
loss:0.44518
loss:0.45674
loss:0.45762
loss:0.47074
loss:0.45135
loss:0.45568
loss:0.45932
loss:0.46284
loss:0.43555
loss:0.44097
loss:0.45958
loss:0.45595
loss:0.48854
loss:0.45106
loss:0.44376
loss:0.44343
loss:0.44408
loss:0.48500
loss:0.45353
loss:0.45975
loss:0.43763
loss:0.42597
loss:0.45884
loss:0.43099
loss:0.40797
loss:0.48414
loss:0.44807
loss:0.43443
loss:0.42373
loss:0.46019
loss:0.44462
loss:0.48088
loss:0.44070
loss:0.46187
loss:0.45631
loss:0.47105
loss:0.45203
loss:0.48122
loss:0.46154
loss:0.44671
loss:0.46883
loss:0.46341
loss:0.44051
loss:0.47022
loss:0.42199
loss:0.47275
loss:0.45172
loss:0.43196
loss:0.43641
loss:0.45168
loss:0.47611
loss:0.45883
loss:0.47396
loss:0.45477
loss:0.39457
loss:0.43266
loss:0.49749
loss:0.45471
loss:0.45254
loss:0.45153
loss:0.48403
loss:0.41477
loss:0.42155
loss:0.47766
loss:0.47721
loss:0.47615
loss:0.44180
loss:0.44148
loss:0.44457
loss:0.45945
loss:0.43580
loss:0.42836
loss:0.46770
loss:0.48501
loss:0.48228
loss:0.42040
loss:0.42770
loss:0.41658
loss:0.46552
loss:0.46102
loss:0.47094
loss:0.47041
loss:0.43429
loss:0.46185
loss:0.46003
loss:0.41362
loss:0.43131
loss:0.45232
loss:0.46070
loss:0.42792
loss:0.43289
loss:0.44193
loss:0.44321
loss:0.42490
loss:0.43598
loss:0.46588
loss:0.43047
loss:0.43491
loss:0.40514
loss:0.44650
loss:0.46101
loss:0.46731
loss:0.45430
loss:0.44716
loss:0.44943
loss:0.46986
loss:0.43023
loss:0.45308
loss:0.46530
loss:0.45963
loss:0.43830
loss:0.44154
loss:0.47001
loss:0.44089
loss:0.45814
loss:0.45968
loss:0.45802
loss:0.43797
loss:0.45691
loss:0.45557
loss:0.44233
loss:0.46491
loss:0.39874
loss:0.45967
loss:0.45630
loss:0.43335
loss:0.48054
loss:0.44209
loss:0.45806
loss:0.43817
loss:0.44944
loss:0.43115
loss:0.45495
loss:0.45052
loss:0.44875
loss:0.47180
loss:0.46206
loss:0.44692
loss:0.41350
loss:0.45767
loss:0.43303
loss:0.44550
loss:0.45675
loss:0.44381
loss:0.41431
loss:0.43624
loss:0.44497
loss:0.45474
loss:0.43952
loss:0.46329
loss:0.47290
loss:0.39819
loss:0.41335
loss:0.46185
loss:0.44769
loss:0.41017
loss:0.44041
loss:0.43615
loss:0.45040
loss:0.43956
loss:0.41431
loss:0.49945
loss:0.46580
loss:0.43246
loss:0.43400
loss:0.44341
loss:0.45252
loss:0.41926
loss:0.44748
loss:0.45362
loss:0.45321
loss:0.48648
loss:0.44569
loss:0.44463
loss:0.41905
loss:0.43906
loss:0.41749
loss:0.42595
loss:0.46574
loss:0.43565
loss:0.45215
loss:0.45483
loss:0.48866
loss:0.45014
loss:0.43608
loss:0.42910
loss:0.42428
loss:0.45236
loss:0.45791
loss:0.45863
loss:0.42774
loss:0.48256
loss:0.44631
loss:0.43755
loss:0.46184
loss:0.41139
loss:0.47758
loss:0.44094
loss:0.44466
loss:0.44455
loss:0.45220
loss:0.46338
loss:0.43180
loss:0.43532
loss:0.45793
loss:0.44215
loss:0.44201
loss:0.47123
loss:0.44366
loss:0.44545
loss:0.47788
loss:0.42722
loss:0.45941
loss:0.45629
loss:0.46175
loss:0.46704
loss:0.46128
loss:0.46101
loss:0.43645
loss:0.40560
loss:0.41082
loss:0.42373
loss:0.44768
loss:0.41184
loss:0.40993
loss:0.43375
loss:0.43580
loss:0.44078
loss:0.45650
loss:0.47321
loss:0.39892
loss:0.48592
loss:0.44447
loss:0.46309
loss:0.44499
loss:0.44998
loss:0.46889
loss:0.44298
loss:0.45458
loss:0.42162
loss:0.42068
loss:0.45039
loss:0.45557
loss:0.40246
loss:0.43893
loss:0.48035
loss:0.46341
loss:0.39041
loss:0.45949
loss:0.46705
loss:0.45560
loss:0.40539
loss:0.44245
loss:0.43476
loss:0.39522
loss:0.44080
loss:0.42431
loss:0.44023
loss:0.48811
loss:0.43648
loss:0.39657
loss:0.45033
loss:0.43794
loss:0.40298
loss:0.41876
loss:0.42472
loss:0.43502
loss:0.46586
loss:0.44816
loss:0.39018
loss:0.41112
loss:0.46342
loss:0.41730
loss:0.43981
loss:0.44627
loss:0.45782
loss:0.41835
loss:0.43372
loss:0.43458
loss:0.41207
loss:0.40601
loss:0.41023
loss:0.43095
loss:0.42376
loss:0.41751
loss:0.42860
loss:0.45939
loss:0.45538
loss:0.44165
loss:0.43758
loss:0.46833
loss:0.41307
loss:0.45113
loss:0.41635
loss:0.44386
loss:0.47368
loss:0.50168
loss:0.45715
loss:0.40688
loss:0.41766
loss:0.43761
loss:0.40882
loss:0.41968
loss:0.45176
loss:0.43065
loss:0.46157
loss:0.44105
loss:0.46912
loss:0.43035
loss:0.42113
loss:0.41764
loss:0.47971
loss:0.41823
loss:0.46095
loss:0.43114
loss:0.45563
loss:0.40152
loss:0.40840
loss:0.47006
loss:0.45961
loss:0.44843
loss:0.45118
loss:0.45359
loss:0.43382
loss:0.38711
loss:0.40193
loss:0.43360
loss:0.47800
loss:0.40477
loss:0.42712
loss:0.47065
loss:0.44653
loss:0.44367
loss:0.41605
loss:0.41469
loss:0.41025
loss:0.43827
loss:0.43716
loss:0.43019
loss:0.41661
loss:0.43340
loss:0.41022
loss:0.44723
loss:0.45231
loss:0.42135
loss:0.41867
loss:0.38774
loss:0.41122
loss:0.44259
loss:0.44059
loss:0.42690
loss:0.41397
loss:0.42354
loss:0.42706
loss:0.44694
loss:0.40746
loss:0.40185
loss:0.41515
loss:0.44866
loss:0.44922
loss:0.41513
loss:0.46160
loss:0.44828
loss:0.41919
loss:0.41295
loss:0.44525
loss:0.42480
loss:0.46178
loss:0.43685
loss:0.43775
loss:0.40214
loss:0.43826
loss:0.41993
loss:0.41293
loss:0.43827
loss:0.40551
loss:0.40505
loss:0.43389
loss:0.41689
loss:0.39335
loss:0.42911
loss:0.41628
loss:0.42743
loss:0.42260
loss:0.45227
loss:0.42889
loss:0.43083
loss:0.44201
loss:0.44091
loss:0.44893
loss:0.39794
loss:0.39430
loss:0.43473
loss:0.41113
loss:0.41095
loss:0.40110
loss:0.39936
loss:0.41364
loss:0.42424
loss:0.42001
loss:0.46958
loss:0.44387
loss:0.38291
loss:0.43627
loss:0.42003
loss:0.42841
loss:0.42737
loss:0.46909
loss:0.42894
loss:0.44272
loss:0.43310
loss:0.41920
loss:0.40970
loss:0.43216
loss:0.42102
loss:0.45868
loss:0.43674
loss:0.42409
loss:0.45292
loss:0.43689
loss:0.41702
loss:0.40438
loss:0.46927
loss:0.42929
loss:0.41206
loss:0.40861
loss:0.41219
loss:0.41448
loss:0.42098
loss:0.42338
loss:0.42269
loss:0.40411
loss:0.42424
loss:0.40776
loss:0.44371
loss:0.42992
loss:0.37589
loss:0.41654
loss:0.41578
loss:0.42101
loss:0.44838
loss:0.40845
loss:0.49817
loss:0.41606
loss:0.39209
loss:0.42442
loss:0.41703
loss:0.39580
loss:0.45340
loss:0.43790
loss:0.41981
loss:0.41433
loss:0.41007
loss:0.41753
loss:0.42633
loss:0.41257
loss:0.42159
loss:0.42536
loss:0.39157
loss:0.39423
loss:0.38816
loss:0.41259
loss:0.39559
loss:0.41975
loss:0.43180
loss:0.41189
loss:0.44291
loss:0.43051
loss:0.42700
loss:0.42658
loss:0.42611
loss:0.43656
loss:0.42630
loss:0.42271
loss:0.43007
loss:0.42637
loss:0.45043
loss:0.43672
loss:0.38812
loss:0.43448
loss:0.43427
loss:0.42477
loss:0.40047
loss:0.41794
loss:0.44745
loss:0.43632
loss:0.40773
loss:0.46771
loss:0.44146
loss:0.46493
loss:0.43478
loss:0.43288
loss:0.42702
loss:0.42643
loss:0.43996
loss:0.43090
loss:0.39267
loss:0.42365
loss:0.42248
loss:0.43210
loss:0.40262
loss:0.46059
loss:0.43365
loss:0.42828
loss:0.47122
loss:0.39675
loss:0.43481
loss:0.42420
loss:0.37951
loss:0.39306
loss:0.43271
loss:0.43018
loss:0.41986
loss:0.42533
loss:0.42982
loss:0.45054
loss:0.42117
loss:0.43798
loss:0.47526
loss:0.38550
loss:0.45284
loss:0.46500
loss:0.43946
loss:0.43283
loss:0.42366
loss:0.43777
loss:0.43644
loss:0.43841
loss:0.39624
loss:0.44482
loss:0.44410
loss:0.41167
loss:0.43604
loss:0.44552
loss:0.42969
loss:0.43315
loss:0.38570
loss:0.42650
loss:0.42993
loss:0.39294
loss:0.40949
loss:0.43754
loss:0.43696
loss:0.42694
loss:0.36358
loss:0.41520
loss:0.38085
loss:0.45072
loss:0.42762
loss:0.39116
loss:0.43778
loss:0.38464
loss:0.41272
loss:0.41957
loss:0.42747
loss:0.43123
loss:0.42199
loss:0.44704
loss:0.44120
loss:0.39604
loss:0.41747
loss:0.44016
loss:0.44271
loss:0.39448
loss:0.40567
loss:0.40293
loss:0.42400
loss:0.36437
loss:0.42954
loss:0.39782
loss:0.42934
loss:0.44516
loss:0.41428
loss:0.40259
loss:0.42330
loss:0.43173
loss:0.44034
loss:0.40435
loss:0.40439
loss:0.37752
loss:0.42748
loss:0.41345
loss:0.43727
loss:0.39831
loss:0.40671
loss:0.39229
loss:0.43267
loss:0.39238
loss:0.43450
loss:0.39373
loss:0.39402
Evaluation at Epoch 3/4. Step:12627/16836. AccuracyMetric: acc=0.51924

loss:0.40421
loss:0.40885
loss:0.39012
loss:0.38059
loss:0.37026
loss:0.37601
loss:0.39973
loss:0.38801
loss:0.40815
loss:0.38812
loss:0.37222
loss:0.39576
loss:0.38732
loss:0.39226
loss:0.37172
loss:0.40222
loss:0.39558
loss:0.45065
loss:0.41940
loss:0.39028
loss:0.40216
loss:0.38911
loss:0.38014
loss:0.38153
loss:0.39904
loss:0.42661
loss:0.37677
loss:0.39373
loss:0.40890
loss:0.40535
loss:0.43035
loss:0.36273
loss:0.37987
loss:0.40426
loss:0.39450
loss:0.42099
loss:0.39362
loss:0.39109
loss:0.40536
loss:0.44158
loss:0.37505
loss:0.38962
loss:0.38065
loss:0.36812
loss:0.39095
loss:0.37314
loss:0.38161
loss:0.36544
loss:0.41204
loss:0.38114
loss:0.39155
loss:0.39022
loss:0.36700
loss:0.38068
loss:0.38365
loss:0.39398
loss:0.41548
loss:0.35455
loss:0.41692
loss:0.43238
loss:0.35039
loss:0.40190
loss:0.41145
loss:0.39271
loss:0.40747
loss:0.40548
loss:0.38752
loss:0.36762
loss:0.39688
loss:0.38611
loss:0.35838
loss:0.37576
loss:0.36392
loss:0.42088
loss:0.41764
loss:0.39480
loss:0.42706
loss:0.37875
loss:0.36547
loss:0.39301
loss:0.39906
loss:0.37685
loss:0.40980
loss:0.35823
loss:0.35934
loss:0.39155
loss:0.38193
loss:0.41991
loss:0.39142
loss:0.39789
loss:0.37701
loss:0.38610
loss:0.37129
loss:0.42669
loss:0.37327
loss:0.36570
loss:0.39609
loss:0.36936
loss:0.37709
loss:0.42821
loss:0.36529
loss:0.39801
loss:0.35830
loss:0.37367
loss:0.39426
loss:0.42005
loss:0.36846
loss:0.35643
loss:0.42071
loss:0.40111
loss:0.39586
loss:0.35840
loss:0.41444
loss:0.37574
loss:0.40423
loss:0.37505
loss:0.36856
loss:0.42482
loss:0.36962
loss:0.33388
loss:0.36409
loss:0.40599
loss:0.38793
loss:0.39125
loss:0.39461
loss:0.36641
loss:0.38898
loss:0.35668
loss:0.38287
loss:0.36688
loss:0.35716
loss:0.40806
loss:0.40316
loss:0.38962
loss:0.38298
loss:0.41259
loss:0.46156
loss:0.39310
loss:0.34340
loss:0.37293
loss:0.35050
loss:0.40605
loss:0.39759
loss:0.39961
loss:0.36816
loss:0.40376
loss:0.39653
loss:0.38443
loss:0.38357
loss:0.40874
loss:0.40241
loss:0.37949
loss:0.42872
loss:0.40637
loss:0.37569
loss:0.37032
loss:0.40401
loss:0.37938
loss:0.42375
loss:0.38867
loss:0.39111
loss:0.37813
loss:0.36959
loss:0.36143
loss:0.40339
loss:0.37745
loss:0.39401
loss:0.36254
loss:0.38451
loss:0.36511
loss:0.36819
loss:0.38217
loss:0.38896
loss:0.39810
loss:0.38468
loss:0.39481
loss:0.41920
loss:0.42358
loss:0.41266
loss:0.39952
loss:0.38610
loss:0.41164
loss:0.39941
loss:0.37109
loss:0.35683
loss:0.42768
loss:0.38559
loss:0.37250
loss:0.43117
loss:0.37483
loss:0.40746
loss:0.36811
loss:0.35288
loss:0.40551
loss:0.42854
loss:0.37585
loss:0.40631
loss:0.35231
loss:0.37130
loss:0.35485
loss:0.38474
loss:0.40909
loss:0.39827
loss:0.38206
loss:0.36750
loss:0.34259
loss:0.36190
loss:0.37991
loss:0.35930
loss:0.40002
loss:0.38942
loss:0.44198
loss:0.38560
loss:0.37538
loss:0.35684
loss:0.35164
loss:0.38071
loss:0.41645
loss:0.36767
loss:0.38257
loss:0.44361
loss:0.39975
loss:0.38785
loss:0.40084
loss:0.37317
loss:0.36152
loss:0.39793
loss:0.41490
loss:0.33835
loss:0.36347
loss:0.40689
loss:0.37066
loss:0.37265
loss:0.42738
loss:0.37462
loss:0.34920
loss:0.40661
loss:0.37842
loss:0.36465
loss:0.37845
loss:0.37431
loss:0.37869
loss:0.36624
loss:0.36363
loss:0.36244
loss:0.39096
loss:0.40368
loss:0.34992
loss:0.44828
loss:0.41862
loss:0.38151
loss:0.39993
loss:0.34662
loss:0.34678
loss:0.37595
loss:0.40915
loss:0.41784
loss:0.39649
loss:0.37452
loss:0.37483
loss:0.38047
loss:0.39859
loss:0.36180
loss:0.34119
loss:0.39369
loss:0.39461
loss:0.37369
loss:0.38367
loss:0.43206
loss:0.37160
loss:0.42017
loss:0.40079
loss:0.39215
loss:0.40242
loss:0.35574
loss:0.37190
loss:0.39821
loss:0.34859
loss:0.35795
loss:0.40792
loss:0.35717
loss:0.39690
loss:0.41224
loss:0.37656
loss:0.41198
loss:0.38035
loss:0.40102
loss:0.37336
loss:0.37218
loss:0.41396
loss:0.36878
loss:0.34994
loss:0.35295
loss:0.35178
loss:0.36899
loss:0.40623
loss:0.38528
loss:0.39532
loss:0.38094
loss:0.42433
loss:0.41656
loss:0.35668
loss:0.37470
loss:0.41882
loss:0.37409
loss:0.36551
loss:0.39969
loss:0.35727
loss:0.40248
loss:0.36397
loss:0.36443
loss:0.42036
loss:0.35742
loss:0.34557
loss:0.37687
loss:0.41626
loss:0.39272
loss:0.43228
loss:0.35725
loss:0.36117
loss:0.37935
loss:0.37224
loss:0.40878
loss:0.36932
loss:0.33533
loss:0.41891
loss:0.36924
loss:0.42400
loss:0.37324
loss:0.35888
loss:0.37631
loss:0.33757
loss:0.44616
loss:0.41312
loss:0.40414
loss:0.34914
loss:0.35620
loss:0.42297
loss:0.38661
loss:0.39548
loss:0.40131
loss:0.41390
loss:0.40309
loss:0.40168
loss:0.42280
loss:0.34821
loss:0.39030
loss:0.38580
loss:0.40776
loss:0.38447
loss:0.38807
loss:0.38447
loss:0.36697
loss:0.39324
loss:0.34874
loss:0.42557
loss:0.39612
loss:0.37235
loss:0.41710
loss:0.37861
loss:0.36596
loss:0.37830
loss:0.35007
loss:0.36926
loss:0.35039
loss:0.36258
loss:0.40067
loss:0.35533
loss:0.37218
loss:0.41652
loss:0.40456
loss:0.39470
loss:0.38966
loss:0.37369
loss:0.37190
loss:0.38079
loss:0.37795
loss:0.34629
loss:0.37111
loss:0.37368
loss:0.38451
loss:0.33997
loss:0.35625
loss:0.36912
loss:0.41506
loss:0.41832
loss:0.36917
loss:0.39183
loss:0.35923
loss:0.37720
loss:0.38725
loss:0.40824
loss:0.40803
loss:0.34216
loss:0.38402
loss:0.37101
loss:0.39802
loss:0.42290
loss:0.34756
loss:0.37874
loss:0.36204
loss:0.39726
loss:0.38961
loss:0.37990
loss:0.35265
loss:0.43253
loss:0.35330
loss:0.34264
loss:0.36809
loss:0.37266
loss:0.36022
loss:0.33904
loss:0.35767
loss:0.35847
loss:0.40339
loss:0.37902
loss:0.40805
loss:0.38662
loss:0.38490
loss:0.36268
loss:0.39067
loss:0.41031
loss:0.37364
loss:0.35468
loss:0.39533
loss:0.35933
loss:0.37886
loss:0.44645
loss:0.38588
loss:0.40234
loss:0.34843
loss:0.37222
loss:0.35852
loss:0.40169
loss:0.35929
loss:0.38464
loss:0.38513
loss:0.35933
loss:0.39521
loss:0.42398
loss:0.35108
loss:0.42871
loss:0.36719
loss:0.38448
loss:0.36721
loss:0.36697
loss:0.39082
loss:0.34473
loss:0.39470
loss:0.37561
loss:0.36125
loss:0.32340
loss:0.37804
loss:0.39377
loss:0.34186
loss:0.35591
loss:0.40324
loss:0.34759
loss:0.36353
loss:0.41592
loss:0.35531
loss:0.37661
loss:0.37202
loss:0.40562
loss:0.38911
loss:0.37144
loss:0.34001
loss:0.36120
loss:0.35640
loss:0.36445
loss:0.40205
loss:0.38110
loss:0.37878
loss:0.42110
loss:0.33237
loss:0.38335
loss:0.39881
loss:0.36435
loss:0.37454
loss:0.40424
loss:0.39268
loss:0.38712
loss:0.37490
loss:0.37402
loss:0.34851
loss:0.35052
loss:0.38303
loss:0.34577
loss:0.36885
loss:0.35234
loss:0.39886
loss:0.34923
loss:0.36186
loss:0.37021
loss:0.38185
loss:0.37299
loss:0.33277
loss:0.40269
loss:0.35747
loss:0.40586
loss:0.35395
loss:0.38052
loss:0.37916
loss:0.37058
loss:0.35447
loss:0.39254
loss:0.43760
loss:0.34094
loss:0.37616
loss:0.35732
loss:0.35085
loss:0.35671
loss:0.39934
loss:0.37773
loss:0.34544
loss:0.35274
loss:0.40965
loss:0.34751
loss:0.33800
loss:0.39773
loss:0.39918
loss:0.41333
loss:0.34264
loss:0.40399
loss:0.37119
loss:0.39620
loss:0.34179
loss:0.35676
loss:0.33634
loss:0.37110
loss:0.35140
loss:0.36490
loss:0.36279
loss:0.35567
loss:0.36972
loss:0.35572
loss:0.33376
loss:0.36027
loss:0.37339
loss:0.36118
loss:0.35847
loss:0.34232
loss:0.37072
loss:0.39414
loss:0.35066
loss:0.36177
loss:0.35908
loss:0.35385
loss:0.37779
loss:0.37133
loss:0.38628
loss:0.39720
loss:0.37720
loss:0.35465
loss:0.37766
loss:0.36680
loss:0.35463
loss:0.36393
loss:0.40936
loss:0.36769
loss:0.39851
loss:0.38846
loss:0.36182
loss:0.36799
loss:0.40187
loss:0.36677
loss:0.35843
loss:0.36316
loss:0.37290
loss:0.36771
loss:0.40721
loss:0.37367
loss:0.39050
loss:0.34499
loss:0.36261
loss:0.38310
loss:0.38059
loss:0.33277
loss:0.38026
loss:0.38530
loss:0.36322
loss:0.38339
loss:0.34179
loss:0.32733
loss:0.35285
loss:0.38949
loss:0.38785
loss:0.40804
loss:0.39438
loss:0.30966
loss:0.34762
loss:0.32820
loss:0.37299
loss:0.36719
loss:0.34554
loss:0.34043
loss:0.40117
loss:0.39447
loss:0.37452
loss:0.35884
loss:0.38094
loss:0.42322
loss:0.35853
loss:0.36201
loss:0.36420
loss:0.36459
loss:0.38584
loss:0.37452
loss:0.36662
loss:0.35599
loss:0.35012
loss:0.40280
loss:0.37146
loss:0.40970
loss:0.39739
loss:0.37479
loss:0.36494
loss:0.39406
loss:0.38139
loss:0.33951
loss:0.33830
loss:0.32174
loss:0.35854
loss:0.36258
loss:0.38428
loss:0.38369
loss:0.38386
loss:0.34205
loss:0.35405
loss:0.35718
loss:0.35939
loss:0.41976
loss:0.38633
loss:0.35247
loss:0.34993
loss:0.35467
loss:0.36447
loss:0.37207
loss:0.35345
loss:0.36541
loss:0.34440
loss:0.34783
loss:0.35923
loss:0.36134
loss:0.39158
loss:0.39846
loss:0.38638
loss:0.37231
loss:0.37598
loss:0.35648
loss:0.34915
loss:0.38307
loss:0.35897
loss:0.35788
loss:0.35898
loss:0.39645
loss:0.35569
loss:0.34278
loss:0.36676
loss:0.34436
loss:0.36719
loss:0.36208
loss:0.39719
loss:0.35459
loss:0.38237
loss:0.34315
loss:0.34907
loss:0.33674
loss:0.35790
loss:0.34252
loss:0.37817
loss:0.34365
loss:0.38796
loss:0.36522
loss:0.33548
loss:0.35256
loss:0.32207
loss:0.33484
loss:0.39559
loss:0.35580
loss:0.36447
loss:0.39008
loss:0.37782
loss:0.40269
loss:0.33405
loss:0.38258
loss:0.34628
loss:0.32023
loss:0.33262
loss:0.36628
loss:0.33422
loss:0.39702
loss:0.37053
loss:0.37151
loss:0.32792
loss:0.34893
loss:0.35027
loss:0.35171
loss:0.35912
loss:0.39481
loss:0.35329
loss:0.35792
loss:0.32906
loss:0.36236
loss:0.40791
loss:0.38668
loss:0.34223
loss:0.39333
loss:0.36254
loss:0.37162
loss:0.39116
loss:0.37519
loss:0.34200
loss:0.40252
loss:0.39841
loss:0.37975
loss:0.33925
loss:0.35631
loss:0.36604
loss:0.34467
loss:0.33997
loss:0.39845
loss:0.37906
loss:0.39331
loss:0.41531
loss:0.34334
loss:0.35761
loss:0.36682
loss:0.33560
loss:0.38825
loss:0.36656
loss:0.40689
loss:0.39373
loss:0.32677
loss:0.39233
loss:0.35208
loss:0.35014
loss:0.37067
loss:0.34840
loss:0.31336
loss:0.35871
loss:0.31551
loss:0.39635
loss:0.36536
loss:0.37713
loss:0.41076
loss:0.35155
loss:0.36084
loss:0.33938
loss:0.33409
loss:0.34270
loss:0.36226
loss:0.37208
loss:0.35486
loss:0.34312
loss:0.34735
loss:0.35102
loss:0.39128
loss:0.37118
loss:0.37676
loss:0.35550
loss:0.37981
loss:0.34355
loss:0.36314
loss:0.32428
loss:0.36627
loss:0.34858
loss:0.35856
loss:0.35665
loss:0.32476
loss:0.36062
loss:0.37008
loss:0.36146
loss:0.36515
loss:0.37204
loss:0.33309
loss:0.38677
loss:0.38200
loss:0.35500
loss:0.38657
loss:0.29819
loss:0.35459
loss:0.37716
loss:0.36708
loss:0.33693
loss:0.35387
loss:0.40424
loss:0.35475
loss:0.34586
loss:0.36062
loss:0.37570
loss:0.34741
loss:0.37248
loss:0.37355
loss:0.33397
loss:0.36269
loss:0.37587
loss:0.38403
loss:0.36542
loss:0.31538
loss:0.33468
loss:0.36041
loss:0.41309
loss:0.30236
loss:0.34268
loss:0.33815
loss:0.34594
loss:0.35084
loss:0.37497
loss:0.34214
loss:0.37919
loss:0.42466
loss:0.33013
loss:0.39394
loss:0.36997
loss:0.34561
loss:0.33372
loss:0.36600
loss:0.32423
loss:0.37136
loss:0.33993
loss:0.38720
loss:0.32834
loss:0.38134
loss:0.37181
loss:0.33660
loss:0.37608
loss:0.38063
loss:0.36122
loss:0.39744
loss:0.35867
loss:0.30877
loss:0.34504
loss:0.34919
loss:0.33139
loss:0.36058
loss:0.36529
loss:0.33498
loss:0.32423
loss:0.34033
loss:0.33486
loss:0.38338
Early Stopping triggered in epoch 4!

In Epoch:1/Step:4209, got best dev performance:AccuracyMetric: acc=0.529333
Reloaded the best model.
Train Done.
[tester] 
AccuracyMetric: acc=0.529333
Test Done.
Predict the answer with best model...
(44772, 2)
Predict Done. 5730816 records
true sample count:23418
Add 0 default result in predict.
Predict Done, results saved to ../../data/results/CNNText_essay_is_exciting
[34mAuto commit by fitlog[0m
Running with args:Namespace(batch_size=128, cuda=True, data_dir='../../data', data_src='all_data', dropout=0.3, embed_size=128, epochs=4, gpu='0', hidden_size=128, learning_rate=0.001, log_path='./run_records.log', max_seq_len=500, min_count=10, model='CNNText', model_dir='../../data/models', model_suffix='essay_one_non_teacher_referred_donor_giving_100_plus', num_layers=2, optim='Adam', patience=2, predict=False, prepare=True, prepare_dir='../../data/prepare/', pretrain=False, pretrain_model='None', reload_model_name='best_LSTMText_accuracy_2019-06-14-04-01-48', result_dir='../../data/results/', show_data=False, target_var='one_non_teacher_referred_donor_giving_100_plus', text_var='essay', train=True, vocab_data='vocab_essay_one_non_teacher_referred_donor_giving_100_plus.data', vocab_dir='../../data/vocab', weight_decay=0.0)
Checking the data files...
Checking the data files...
Generateing essay - one_non_teacher_referred_donor_giving_100_plus
Loading data from ../../data/essays_all_outcome.csv ...
nums:(291519,131329,44772)
Over Sample mode
subset nums:(291519,131329,44772)
0
tokenize train set
Tokenizing data , total num:291519
Tokenized:291519/291519
ATTENTION TYPE:<class 'float'> * 1038
tokenize val set
Tokenizing data , total num:131329
Tokenized:131329/131329
ATTENTION TYPE:<class 'float'> * 811
tokenize test set
Tokenizing data , total num:44772
Tokenized:44772/44772
ATTENTION TYPE:<class 'float'> * 688
Building Fastnlp dataset.
Over Sampling...
Building Fastnlp vocabulary.
Building id-presentation for train_set and test_set.
1070
Building target-vector for train_set and test_set.
Data Sizes (343614, 148842, 44772)
Saving vocab(TextData)...
Done with preparing!
(vocab_size,class_num,seq_len):(29297,2,1070)
No pretrained model with be used.
vocabsize:29297
Using CNN Model.
CNNText(
  (embed): Embedding(
    29297, 128
    (dropout): Dropout(p=0.0)
  )
  (conv_pool): ConvMaxpool(
    (convs): ModuleList(
      (0): Conv1d(128, 3, kernel_size=(3,), stride=(1,), padding=(2,))
      (1): Conv1d(128, 4, kernel_size=(4,), stride=(1,), padding=(2,))
      (2): Conv1d(128, 5, kernel_size=(5,), stride=(1,), padding=(2,))
    )
  )
  (dropout): Dropout(p=0.3)
  (fc): Linear(in_features=12, out_features=2, bias=True)
)
train_size:343614 ; val_size:148842 ; test_size:44772
Using Adam as optimizer.
input fields after batch(if batch size is 2):
	words: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2, 1070]) 
	seq_len: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2]) 
target fields after batch(if batch size is 2):
	target: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2]) 

training epochs started 2019-06-15-13-01-35
loss:1.01222
loss:0.91796
loss:0.91648
loss:0.90368
loss:0.97058
loss:0.90709
loss:0.90963
loss:0.91494
loss:0.88244
loss:0.86221
loss:0.78883
loss:0.80886
loss:0.81213
loss:0.79254
loss:0.76783
loss:0.77669
loss:0.76557
loss:0.77642
loss:0.77782
loss:0.71618
loss:0.76027
loss:0.75705
loss:0.75858
loss:0.77455
loss:0.74923
loss:0.73448
loss:0.73062
loss:0.71901
loss:0.72324
loss:0.73308
loss:0.73260
loss:0.71759
loss:0.73930
loss:0.72787
loss:0.70907
loss:0.71668
loss:0.71414
loss:0.71824
loss:0.72938
loss:0.72445
loss:0.71272
loss:0.71327
loss:0.70302
loss:0.70304
loss:0.70712
loss:0.71718
loss:0.69236
loss:0.70703
loss:0.71662
loss:0.70717
loss:0.69875
loss:0.69371
loss:0.69915
loss:0.71200
loss:0.71322
loss:0.70528
loss:0.69967
loss:0.70050
loss:0.69781
loss:0.70232
loss:0.70567
loss:0.70898
loss:0.70135
loss:0.70064
loss:0.70451
loss:0.69998
loss:0.69695
loss:0.70226
loss:0.69840
loss:0.69882
loss:0.69365
loss:0.69627
loss:0.69378
loss:0.69555
loss:0.69429
loss:0.69245
loss:0.69007
loss:0.70287
loss:0.69354
loss:0.69759
loss:0.69119
loss:0.69238
loss:0.69258
loss:0.69838
loss:0.68512
loss:0.69088
loss:0.69911
loss:0.69806
loss:0.69944
loss:0.69452
loss:0.69880
loss:0.70377
loss:0.69898
loss:0.69872
loss:0.69870
loss:0.69451
loss:0.69473
loss:0.68854
loss:0.69281
loss:0.69465
loss:0.69592
loss:0.69827
loss:0.69493
loss:0.68785
loss:0.69068
loss:0.69037
loss:0.69010
loss:0.68991
loss:0.69369
loss:0.69667
loss:0.68618
loss:0.69195
loss:0.69281
loss:0.69199
loss:0.69204
loss:0.69061
loss:0.69685
loss:0.69047
loss:0.69437
loss:0.69320
loss:0.69433
loss:0.69355
loss:0.69199
loss:0.69131
loss:0.69088
loss:0.69095
loss:0.69786
loss:0.68990
loss:0.68697
loss:0.69330
loss:0.69398
loss:0.69059
loss:0.69542
loss:0.69028
loss:0.69302
loss:0.69338
loss:0.69184
loss:0.68854
loss:0.68131
loss:0.69533
loss:0.68767
loss:0.69457
loss:0.68885
loss:0.69346
loss:0.69151
loss:0.69227
loss:0.68685
loss:0.68964
loss:0.69008
loss:0.69705
loss:0.69133
loss:0.69283
loss:0.68954
loss:0.69076
loss:0.69444
loss:0.69397
loss:0.69207
loss:0.68923
loss:0.69260
loss:0.68521
loss:0.69110
loss:0.69204
loss:0.69104
loss:0.69724
loss:0.69177
loss:0.69219
loss:0.68729
loss:0.69311
loss:0.68889
loss:0.68899
loss:0.69427
loss:0.69298
loss:0.69329
loss:0.69126
loss:0.69209
loss:0.69027
loss:0.68821
loss:0.69249
loss:0.69130
loss:0.68963
loss:0.69623
loss:0.68873
loss:0.69995
loss:0.68830
loss:0.68864
loss:0.68951
loss:0.68513
loss:0.68825
loss:0.69188
loss:0.69483
loss:0.68916
loss:0.70361
loss:0.69153
loss:0.68737
loss:0.68770
loss:0.68774
loss:0.69331
loss:0.69239
loss:0.69294
loss:0.69512
loss:0.69329
loss:0.68956
loss:0.68552
loss:0.68804
loss:0.70052
loss:0.69155
loss:0.69079
loss:0.69266
loss:0.69041
loss:0.69193
loss:0.69022
loss:0.69575
loss:0.68840
loss:0.69207
loss:0.69149
loss:0.68989
loss:0.69011
loss:0.69432
loss:0.68779
loss:0.69222
loss:0.68895
loss:0.69215
loss:0.69145
loss:0.69119
loss:0.68822
loss:0.68809
loss:0.69875
loss:0.69028
loss:0.69305
loss:0.69447
loss:0.68839
loss:0.69244
loss:0.69397
loss:0.69160
loss:0.68838
loss:0.69447
loss:0.68888
loss:0.68656
loss:0.69427
loss:0.69096
loss:0.69093
loss:0.68825
loss:0.69283
loss:0.69014
loss:0.68593
loss:0.69750
loss:0.69869
loss:0.69690
loss:0.69282
loss:0.69003
loss:0.68895
loss:0.69476
loss:0.69140
loss:0.68749
loss:0.68892
loss:0.68632
loss:0.69353
loss:0.69129
loss:0.68878
loss:0.68926
loss:0.68967
loss:0.68982
loss:0.69047
loss:0.69181
loss:0.68820
loss:0.69352
loss:0.68930
loss:0.68950
loss:0.69190
loss:0.69253
loss:0.68857
loss:0.69336
loss:0.69347
loss:0.69650
loss:0.68953
loss:0.69126
loss:0.69016
loss:0.68902
loss:0.69069
loss:0.69244
loss:0.69030
loss:0.68891
loss:0.68888
loss:0.68696
loss:0.68531
loss:0.68531
loss:0.68700
loss:0.68585
loss:0.68611
loss:0.68321
loss:0.69052
loss:0.69639
loss:0.68863
loss:0.68622
loss:0.68051
loss:0.69462
loss:0.69022
loss:0.68757
loss:0.68954
loss:0.69425
loss:0.68986
loss:0.68380
loss:0.69064
loss:0.68496
loss:0.68708
loss:0.68540
loss:0.69333
loss:0.68924
loss:0.68778
loss:0.69034
loss:0.68534
loss:0.68297
loss:0.69181
loss:0.68450
loss:0.69138
loss:0.69051
loss:0.68014
loss:0.69125
loss:0.68272
loss:0.68943
loss:0.68934
loss:0.69451
loss:0.68651
loss:0.68820
loss:0.68560
loss:0.69252
loss:0.68838
loss:0.69432
loss:0.68944
loss:0.68284
loss:0.69040
loss:0.69411
loss:0.69399
loss:0.69214
loss:0.68100
loss:0.68623
loss:0.68825
loss:0.69338
loss:0.68533
loss:0.68830
loss:0.68548
loss:0.68067
loss:0.68169
loss:0.69226
loss:0.68403
loss:0.68306
loss:0.68455
loss:0.69018
loss:0.68664
loss:0.68970
loss:0.68563
loss:0.68856
loss:0.68817
loss:0.68526
loss:0.69073
loss:0.68690
loss:0.68683
loss:0.68244
loss:0.68403
loss:0.69035
loss:0.68692
loss:0.70175
loss:0.69012
loss:0.68927
loss:0.68645
loss:0.68800
loss:0.68196
loss:0.68492
loss:0.68903
loss:0.68620
loss:0.68092
loss:0.68584
loss:0.68971
loss:0.68861
loss:0.68004
loss:0.69447
loss:0.68092
loss:0.68896
loss:0.68663
loss:0.69269
loss:0.68533
loss:0.68666
loss:0.68451
loss:0.68474
loss:0.68519
loss:0.68665
loss:0.67731
loss:0.67797
loss:0.68592
loss:0.69766
loss:0.69169
loss:0.68301
loss:0.68886
loss:0.68150
loss:0.68018
loss:0.68460
loss:0.68127
loss:0.68536
loss:0.68819
loss:0.69234
loss:0.68172
loss:0.68851
loss:0.69260
loss:0.68769
loss:0.69595
loss:0.68936
loss:0.68308
loss:0.67883
loss:0.68376
loss:0.67991
loss:0.68235
loss:0.69023
loss:0.68451
loss:0.68716
loss:0.68082
loss:0.68208
loss:0.68922
loss:0.68961
loss:0.69351
loss:0.69085
loss:0.68291
loss:0.68457
loss:0.68440
loss:0.68272
loss:0.68349
loss:0.68713
loss:0.68609
loss:0.67957
loss:0.69140
loss:0.68705
loss:0.68444
loss:0.68286
loss:0.67920
loss:0.68873
loss:0.68276
loss:0.68322
loss:0.68195
loss:0.67612
loss:0.68908
loss:0.68916
loss:0.69076
loss:0.68785
loss:0.69003
loss:0.68385
loss:0.68443
loss:0.68567
loss:0.67603
loss:0.68617
loss:0.68325
loss:0.68346
loss:0.68801
loss:0.67783
loss:0.67984
loss:0.67942
loss:0.68239
loss:0.67770
loss:0.67768
loss:0.68850
loss:0.68424
loss:0.68225
loss:0.68553
loss:0.68377
loss:0.67620
loss:0.68096
loss:0.68390
loss:0.67609
loss:0.68236
loss:0.67999
loss:0.68534
loss:0.68564
loss:0.68609
loss:0.68126
loss:0.68676
loss:0.68030
loss:0.68343
loss:0.69010
loss:0.68452
loss:0.68455
loss:0.69372
loss:0.68813
loss:0.68840
loss:0.69491
loss:0.68499
loss:0.67860
loss:0.68691
loss:0.68479
loss:0.69778
loss:0.68701
loss:0.67742
loss:0.68761
loss:0.68394
loss:0.68554
loss:0.68299
loss:0.68438
loss:0.69078
loss:0.68584
loss:0.68356
loss:0.68280
loss:0.69110
loss:0.67467
loss:0.69146
loss:0.68988
loss:0.69486
loss:0.68599
loss:0.68669
loss:0.68490
loss:0.68359
loss:0.68196
loss:0.68159
loss:0.68629
loss:0.68685
loss:0.68409
loss:0.67914
loss:0.68396
loss:0.68926
loss:0.67592
loss:0.68357
loss:0.67543
loss:0.68678
loss:0.68379
loss:0.69737
loss:0.68241
loss:0.67725
loss:0.68522
loss:0.68562
loss:0.67774
loss:0.68529
loss:0.68575
loss:0.67347
loss:0.67696
loss:0.67316
loss:0.69063
loss:0.67914
loss:0.68201
loss:0.68354
loss:0.67723
loss:0.68744
Evaluation at Epoch 1/4. Step:2685/10740. AccuracyMetric: acc=0.548736

loss:0.67869
loss:0.67536
loss:0.68194
loss:0.68034
loss:0.67542
loss:0.67853
loss:0.69215
loss:0.68407
loss:0.68129
loss:0.68436
loss:0.67654
loss:0.68133
loss:0.68074
loss:0.66879
loss:0.69431
loss:0.68436
loss:0.68220
loss:0.68108
loss:0.67796
loss:0.68518
loss:0.68001
loss:0.68920
loss:0.67719
loss:0.68210
loss:0.68701
loss:0.68763
loss:0.66820
loss:0.68799
loss:0.68388
loss:0.67256
loss:0.67556
loss:0.68935
loss:0.67878
loss:0.67606
loss:0.68297
loss:0.66605
loss:0.66891
loss:0.68099
loss:0.68083
loss:0.68157
loss:0.68093
loss:0.67284
loss:0.67964
loss:0.68705
loss:0.68757
loss:0.68903
loss:0.67757
loss:0.68217
loss:0.68348
loss:0.68043
loss:0.67990
loss:0.68918
loss:0.67495
loss:0.68339
loss:0.67727
loss:0.67735
loss:0.68216
loss:0.67543
loss:0.67898
loss:0.67965
loss:0.67434
loss:0.68182
loss:0.67701
loss:0.66819
loss:0.67606
loss:0.66929
loss:0.68187
loss:0.67266
loss:0.68409
loss:0.67677
loss:0.67864
loss:0.69178
loss:0.67326
loss:0.67279
loss:0.68065
loss:0.69080
loss:0.68965
loss:0.68200
loss:0.68620
loss:0.66988
loss:0.68595
loss:0.68352
loss:0.66929
loss:0.68543
loss:0.68443
loss:0.67249
loss:0.68005
loss:0.69147
loss:0.68324
loss:0.68551
loss:0.67721
loss:0.67430
loss:0.67152
loss:0.67916
loss:0.67232
loss:0.67927
loss:0.68593
loss:0.67657
loss:0.66868
loss:0.69118
loss:0.68121
loss:0.67660
loss:0.69256
loss:0.67661
loss:0.68617
loss:0.67380
loss:0.67967
loss:0.66777
loss:0.69769
loss:0.67594
loss:0.67666
loss:0.67619
loss:0.68336
loss:0.67376
loss:0.67023
loss:0.68031
loss:0.66362
loss:0.68694
loss:0.67168
loss:0.67094
loss:0.66714
loss:0.68409
loss:0.67715
loss:0.67701
loss:0.68621
loss:0.67755
loss:0.68065
loss:0.67204
loss:0.68632
loss:0.68028
loss:0.67439
loss:0.68086
loss:0.68570
loss:0.68225
loss:0.68649
loss:0.69242
loss:0.68163
loss:0.68647
loss:0.67419
loss:0.67206
loss:0.68033
loss:0.68883
loss:0.67687
loss:0.66983
loss:0.67839
loss:0.69685
loss:0.68260
loss:0.68851
loss:0.68095
loss:0.67723
loss:0.67557
loss:0.68336
loss:0.67843
loss:0.68730
loss:0.68680
loss:0.66649
loss:0.68818
loss:0.66688
loss:0.69015
loss:0.68062
loss:0.67882
loss:0.68121
loss:0.68267
loss:0.68225
loss:0.66745
loss:0.68144
loss:0.67894
loss:0.67247
loss:0.67434
loss:0.67533
loss:0.67936
loss:0.68712
loss:0.67949
loss:0.67911
loss:0.69171
loss:0.68546
loss:0.68079
loss:0.68360
loss:0.67289
loss:0.68258
loss:0.68098
loss:0.67027
loss:0.67999
loss:0.68565
loss:0.68128
loss:0.67570
loss:0.69158
loss:0.67886
loss:0.68132
loss:0.68702
loss:0.67169
loss:0.68630
loss:0.68175
loss:0.67881
loss:0.67155
loss:0.67065
loss:0.67580
loss:0.68646
loss:0.67048
loss:0.67903
loss:0.67957
loss:0.68464
loss:0.67748
loss:0.67731
loss:0.68910
loss:0.67194
loss:0.66549
loss:0.67814
loss:0.67547
loss:0.68754
loss:0.67713
loss:0.67378
loss:0.67088
loss:0.67357
loss:0.66754
loss:0.68656
loss:0.67182
loss:0.68703
loss:0.67895
loss:0.66970
loss:0.68633
loss:0.67289
loss:0.69303
loss:0.67353
loss:0.68105
loss:0.66276
loss:0.67497
loss:0.67570
loss:0.68004
loss:0.67965
loss:0.68277
loss:0.66602
loss:0.67994
loss:0.67007
loss:0.67591
loss:0.66768
loss:0.68733
loss:0.69104
loss:0.67676
loss:0.67762
loss:0.67850
loss:0.67170
loss:0.68763
loss:0.68530
loss:0.68678
loss:0.68787
loss:0.67587
loss:0.68062
loss:0.67988
loss:0.67185
loss:0.67659
loss:0.67561
loss:0.67684
loss:0.67656
loss:0.68313
loss:0.68231
loss:0.67033
loss:0.68275
loss:0.67405
loss:0.68360
loss:0.67332
loss:0.67947
loss:0.67296
loss:0.68122
loss:0.68185
loss:0.68494
loss:0.68091
loss:0.67688
loss:0.68687
loss:0.67479
loss:0.67805
loss:0.67135
loss:0.66325
loss:0.68870
loss:0.67288
loss:0.68749
loss:0.66760
loss:0.67043
loss:0.67247
loss:0.67629
loss:0.67909
loss:0.68855
loss:0.68953
loss:0.67633
loss:0.67771
loss:0.67417
loss:0.68513
loss:0.67059
loss:0.68096
loss:0.68164
loss:0.67466
loss:0.67728
loss:0.66654
loss:0.68156
loss:0.68130
loss:0.68794
loss:0.67972
loss:0.67177
loss:0.68123
loss:0.67636
loss:0.68569
loss:0.68585
loss:0.67484
loss:0.67320
loss:0.67377
loss:0.68361
loss:0.68423
loss:0.68337
loss:0.67301
loss:0.67163
loss:0.67186
loss:0.69315
loss:0.68892
loss:0.66941
loss:0.66779
loss:0.68446
loss:0.68306
loss:0.67684
loss:0.67891
loss:0.68056
loss:0.67352
loss:0.67754
loss:0.67494
loss:0.67726
loss:0.67786
loss:0.68100
loss:0.67945
loss:0.67488
loss:0.67088
loss:0.66391
loss:0.67054
loss:0.67794
loss:0.68224
loss:0.67427
loss:0.68394
loss:0.68572
loss:0.68664
loss:0.68322
loss:0.67902
loss:0.67497
loss:0.68159
loss:0.68044
loss:0.67564
loss:0.68353
loss:0.68260
loss:0.66589
loss:0.68739
loss:0.67716
loss:0.67484
loss:0.67073
loss:0.67073
loss:0.67139
loss:0.68786
loss:0.67022
loss:0.68364
loss:0.68176
loss:0.67446
loss:0.68084
loss:0.67627
loss:0.69319
loss:0.68391
loss:0.67120
loss:0.68635
loss:0.67753
loss:0.69047
loss:0.67177
loss:0.68240
loss:0.66803
loss:0.68630
loss:0.68409
loss:0.67152
loss:0.67812
loss:0.68279
loss:0.67186
loss:0.67131
loss:0.67672
loss:0.68188
loss:0.67177
loss:0.66820
loss:0.68058
loss:0.68011
loss:0.68912
loss:0.68772
loss:0.67437
loss:0.65424
loss:0.67308
loss:0.68614
loss:0.68099
loss:0.67860
loss:0.68973
loss:0.67712
loss:0.66624
loss:0.67259
loss:0.67974
loss:0.68172
loss:0.67389
loss:0.67919
loss:0.68220
loss:0.68159
loss:0.68257
loss:0.67886
loss:0.67396
loss:0.68179
loss:0.67645
loss:0.67606
loss:0.66955
loss:0.66670
loss:0.67563
loss:0.66972
loss:0.66544
loss:0.68097
loss:0.68038
loss:0.67910
loss:0.67100
loss:0.67349
loss:0.68457
loss:0.67251
loss:0.67943
loss:0.67908
loss:0.67535
loss:0.68774
loss:0.67369
loss:0.66872
loss:0.67170
loss:0.68197
loss:0.67708
loss:0.68195
loss:0.67604
loss:0.67804
loss:0.67655
loss:0.68563
loss:0.68864
loss:0.69253
loss:0.66893
loss:0.67865
loss:0.68836
loss:0.67973
loss:0.67190
loss:0.67289
loss:0.67900
loss:0.68060
loss:0.67566
loss:0.68709
loss:0.67908
loss:0.67825
loss:0.66506
loss:0.68012
loss:0.68266
loss:0.67145
loss:0.67464
loss:0.67560
loss:0.68630
loss:0.68350
loss:0.67813
loss:0.68290
loss:0.67723
loss:0.67313
loss:0.66483
loss:0.68531
loss:0.66944
loss:0.68032
loss:0.67315
loss:0.67022
loss:0.68690
loss:0.68465
loss:0.68014
loss:0.67557
loss:0.67879
loss:0.67236
loss:0.67487
loss:0.66955
loss:0.67295
loss:0.68027
loss:0.67617
loss:0.65767
loss:0.69331
loss:0.68411
loss:0.67122
loss:0.67090
loss:0.68933
loss:0.68066
loss:0.67039
loss:0.68376
loss:0.67314
loss:0.68514
loss:0.68760
loss:0.68478
loss:0.68107
loss:0.67209
loss:0.67650
loss:0.68344
loss:0.67797
loss:0.67980
loss:0.67899
loss:0.68256
loss:0.66953
loss:0.66810
loss:0.68148
loss:0.68590
loss:0.68513
loss:0.66949
loss:0.68002
loss:0.66839
loss:0.68204
loss:0.67495
loss:0.67916
loss:0.67958
loss:0.67914
loss:0.68010
loss:0.67979
loss:0.68338
loss:0.66320
loss:0.67285
loss:0.67392
loss:0.67188
loss:0.66027
loss:0.67533
loss:0.68547
loss:0.67330
loss:0.68519
loss:0.69221
loss:0.68621
loss:0.68418
loss:0.67753
loss:0.67951
loss:0.66753
loss:0.68281
loss:0.67525
loss:0.68186
loss:0.67244
loss:0.67032
loss:0.66352
loss:0.70326
loss:0.67847
loss:0.66230
loss:0.66669
loss:0.68410
Evaluation at Epoch 2/4. Step:5370/10740. AccuracyMetric: acc=0.553607

loss:0.67463
loss:0.66568
loss:0.66979
loss:0.68219
loss:0.67157
loss:0.68663
loss:0.67879
loss:0.67770
loss:0.67287
loss:0.67121
loss:0.66730
loss:0.67133
loss:0.67199
loss:0.66628
loss:0.66914
loss:0.69108
loss:0.67623
loss:0.67180
loss:0.67558
loss:0.66929
loss:0.66101
loss:0.68100
loss:0.67553
loss:0.66687
loss:0.67400
loss:0.67391
loss:0.66390
loss:0.67310
loss:0.67630
loss:0.66906
loss:0.66458
loss:0.66888
loss:0.67869
loss:0.67441
loss:0.67650
loss:0.67704
loss:0.67108
loss:0.68056
loss:0.66526
loss:0.65897
loss:0.67502
loss:0.67371
loss:0.66706
loss:0.66916
loss:0.67946
loss:0.68039
loss:0.66654
loss:0.66528
loss:0.67988
loss:0.65938
loss:0.66081
loss:0.67922
loss:0.67350
loss:0.65990
loss:0.67056
loss:0.66204
loss:0.67999
loss:0.66647
loss:0.66953
loss:0.68206
loss:0.66609
loss:0.66291
loss:0.67169
loss:0.66174
loss:0.68040
loss:0.67739
loss:0.66771
loss:0.66690
loss:0.66377
loss:0.68332
loss:0.67352
loss:0.66981
loss:0.66878
loss:0.66055
loss:0.67080
loss:0.67105
loss:0.65636
loss:0.68287
loss:0.66658
loss:0.66568
loss:0.67862
loss:0.66742
loss:0.65212
loss:0.67203
loss:0.65730
loss:0.66698
loss:0.66488
loss:0.67019
loss:0.67591
loss:0.69071
loss:0.67568
loss:0.67368
loss:0.67467
loss:0.67456
loss:0.66739
loss:0.67393
loss:0.65492
loss:0.65868
loss:0.67923
loss:0.66035
loss:0.66051
loss:0.67149
loss:0.67715
loss:0.66318
loss:0.66686
loss:0.69406
loss:0.68155
loss:0.67036
loss:0.66951
loss:0.67226
loss:0.67952
loss:0.69104
loss:0.67048
loss:0.66416
loss:0.67120
loss:0.66696
loss:0.67593
loss:0.67407
loss:0.67061
loss:0.66774
loss:0.66665
loss:0.66858
loss:0.67931
loss:0.65838
loss:0.66718
loss:0.68150
loss:0.66149
loss:0.66666
loss:0.67584
loss:0.68443
loss:0.66580
loss:0.67684
loss:0.68026
loss:0.68302
loss:0.66810
loss:0.67305
loss:0.66474
loss:0.67833
loss:0.67696
loss:0.67014
loss:0.65889
loss:0.67800
loss:0.67960
loss:0.66521
loss:0.68468
loss:0.66732
loss:0.65320
loss:0.67303
loss:0.67322
loss:0.66523
loss:0.66042
loss:0.67027
loss:0.66997
loss:0.67860
loss:0.68065
loss:0.67648
loss:0.65670
loss:0.66898
loss:0.66433
loss:0.67948
loss:0.67349
loss:0.68222
loss:0.66252
loss:0.67198
loss:0.67198
loss:0.67579
loss:0.67453
loss:0.65528
loss:0.67307
loss:0.67545
loss:0.67100
loss:0.67643
loss:0.66933
loss:0.67032
loss:0.67276
loss:0.67936
loss:0.67266
loss:0.66549
loss:0.66830
loss:0.67018
loss:0.67793
loss:0.66995
loss:0.67117
loss:0.66297
loss:0.66840
loss:0.67632
loss:0.66879
loss:0.66732
loss:0.68220
loss:0.67820
loss:0.66085
loss:0.67398
loss:0.66891
loss:0.65816
loss:0.66582
loss:0.66399
loss:0.67287
loss:0.66429
loss:0.68475
loss:0.68619
loss:0.67124
loss:0.65719
loss:0.68480
loss:0.65476
loss:0.66593
loss:0.66456
loss:0.67150
loss:0.66705
loss:0.67599
loss:0.67366
loss:0.65081
loss:0.65640
loss:0.67630
loss:0.67792
loss:0.67095
loss:0.66535
loss:0.66399
loss:0.67189
loss:0.68511
loss:0.68238
loss:0.66026
loss:0.67592
loss:0.67129
loss:0.65534
loss:0.65686
loss:0.67465
loss:0.67841
loss:0.66198
loss:0.67411
loss:0.66506
loss:0.68132
loss:0.67342
loss:0.66960
loss:0.66599
loss:0.66739
loss:0.66957
loss:0.66501
loss:0.67405
loss:0.66500
loss:0.66488
loss:0.66755
loss:0.66613
loss:0.66234
loss:0.66974
loss:0.66467
loss:0.68964
loss:0.67880
loss:0.68081
loss:0.68421
loss:0.69383
loss:0.68504
loss:0.68560
loss:0.67229
loss:0.67050
loss:0.68130
loss:0.67859
loss:0.66629
loss:0.67713
loss:0.68102
loss:0.67584
loss:0.67897
loss:0.67358
loss:0.66439
loss:0.68038
loss:0.67785
loss:0.66257
loss:0.68537
loss:0.66509
loss:0.66298
loss:0.67061
loss:0.66658
loss:0.67291
loss:0.67217
loss:0.66780
loss:0.67871
loss:0.65938
loss:0.65040
loss:0.66850
loss:0.67209
loss:0.66852
loss:0.67583
loss:0.67168
loss:0.66848
loss:0.66978
loss:0.68695
loss:0.65359
loss:0.67000
loss:0.67730
loss:0.67369
loss:0.67338
loss:0.66150
loss:0.68680
loss:0.66215
loss:0.66325
loss:0.67480
loss:0.66317
loss:0.67005
loss:0.68765
loss:0.68642
loss:0.66525
loss:0.67900
loss:0.66570
loss:0.66191
loss:0.67325
loss:0.66061
loss:0.65924
loss:0.67803
loss:0.66643
loss:0.65475
loss:0.66990
loss:0.67592
loss:0.67998
loss:0.66852
loss:0.67232
loss:0.66413
loss:0.66043
loss:0.66326
loss:0.67714
loss:0.66052
loss:0.67921
loss:0.68199
loss:0.67412
loss:0.66637
loss:0.65536
loss:0.68324
loss:0.67621
loss:0.67571
loss:0.68500
loss:0.65381
loss:0.68320
loss:0.65017
loss:0.65983
loss:0.68118
loss:0.67276
loss:0.66487
loss:0.67168
loss:0.67208
loss:0.65632
loss:0.66254
loss:0.67120
loss:0.68605
loss:0.66836
loss:0.68036
loss:0.68820
loss:0.67223
loss:0.66580
loss:0.67069
loss:0.66194
loss:0.67130
loss:0.67419
loss:0.68342
loss:0.65809
loss:0.66234
loss:0.65328
loss:0.66729
loss:0.69378
loss:0.67659
loss:0.66876
loss:0.66266
loss:0.67648
loss:0.65744
loss:0.66189
loss:0.67518
loss:0.65809
loss:0.67331
loss:0.65804
loss:0.66528
loss:0.66864
loss:0.68567
loss:0.67006
loss:0.65806
loss:0.67198
loss:0.66704
loss:0.67546
loss:0.67570
loss:0.66623
loss:0.66790
loss:0.67450
loss:0.68563
loss:0.67001
loss:0.67138
loss:0.67441
loss:0.67549
loss:0.67541
loss:0.65053
loss:0.66897
loss:0.66777
loss:0.66805
loss:0.65902
loss:0.66360
loss:0.65169
loss:0.68956
loss:0.68276
loss:0.66151
loss:0.67401
loss:0.66245
loss:0.66994
loss:0.64831
loss:0.66555
loss:0.65677
loss:0.66033
loss:0.66399
loss:0.68468
loss:0.66802
loss:0.66042
loss:0.66516
loss:0.67842
loss:0.66960
loss:0.66901
loss:0.66646
loss:0.67198
loss:0.67005
loss:0.67553
loss:0.66124
loss:0.66948
loss:0.66375
loss:0.67115
loss:0.67371
loss:0.67185
loss:0.66747
loss:0.66743
loss:0.67164
loss:0.67353
loss:0.67189
loss:0.66517
loss:0.68147
loss:0.65979
loss:0.67084
loss:0.66986
loss:0.67068
loss:0.66845
loss:0.66434
loss:0.67118
loss:0.66152
loss:0.66758
loss:0.67551
loss:0.67918
loss:0.67113
loss:0.65062
loss:0.66142
loss:0.65897
loss:0.67159
loss:0.65788
loss:0.66498
loss:0.69124
loss:0.67358
loss:0.68012
loss:0.66787
loss:0.66276
loss:0.66594
loss:0.68362
loss:0.67872
loss:0.66489
loss:0.66885
loss:0.67802
loss:0.66596
loss:0.66210
loss:0.66660
loss:0.66891
loss:0.66144
loss:0.67578
loss:0.65904
loss:0.67046
loss:0.65362
loss:0.67832
loss:0.68081
loss:0.65655
loss:0.67776
loss:0.66395
loss:0.67587
loss:0.66989
loss:0.67305
loss:0.67496
loss:0.66699
loss:0.66950
loss:0.67410
loss:0.66073
loss:0.66573
loss:0.67837
loss:0.67581
loss:0.66318
loss:0.66234
loss:0.66105
loss:0.66956
loss:0.66809
loss:0.66072
loss:0.66649
loss:0.67066
loss:0.67990
loss:0.67673
loss:0.66703
loss:0.66677
loss:0.67891
loss:0.67648
loss:0.66864
loss:0.66222
loss:0.67444
loss:0.66820
loss:0.66762
loss:0.67159
loss:0.66120
loss:0.67102
loss:0.65041
loss:0.67077
loss:0.65509
loss:0.66047
loss:0.67343
loss:0.67394
loss:0.65210
loss:0.66884
loss:0.67904
loss:0.68029
loss:0.67557
loss:0.67358
loss:0.67385
loss:0.67432
loss:0.67877
loss:0.66163
loss:0.66338
loss:0.67960
loss:0.67919
loss:0.66574
loss:0.67629
loss:0.67635
loss:0.67205
loss:0.66475
loss:0.66639
loss:0.68477
loss:0.68537
loss:0.66778
loss:0.66558
loss:0.65632
loss:0.68048
loss:0.66965
loss:0.67953
loss:0.67428
loss:0.67412
Evaluation at Epoch 3/4. Step:8055/10740. AccuracyMetric: acc=0.555186

loss:0.67388
loss:0.67025
loss:0.66387
loss:0.65209
loss:0.66788
loss:0.65525
loss:0.66928
loss:0.66997
loss:0.66522
loss:0.67596
loss:0.65273
loss:0.67670
loss:0.66093
loss:0.65347
loss:0.66068
loss:0.65746
loss:0.67291
loss:0.68249
loss:0.64624
loss:0.66650
loss:0.66487
loss:0.65442
loss:0.67053
loss:0.66483
loss:0.66843
loss:0.65235
loss:0.66753
loss:0.65502
loss:0.67382
loss:0.66555
loss:0.65955
loss:0.66239
loss:0.65947
loss:0.65761
loss:0.65518
loss:0.66439
loss:0.67002
loss:0.65229
loss:0.66565
loss:0.66877
loss:0.66074
loss:0.66411
loss:0.65230
loss:0.66770
loss:0.66080
loss:0.66065
loss:0.66078
loss:0.64966
loss:0.66458
loss:0.66552
loss:0.66383
loss:0.65359
loss:0.65896
loss:0.65645
loss:0.65506
loss:0.67934
loss:0.66071
loss:0.65386
loss:0.66842
loss:0.65214
loss:0.65421
loss:0.66286
loss:0.65291
loss:0.65396
loss:0.67584
loss:0.65654
loss:0.65986
loss:0.66997
loss:0.66605
loss:0.67186
loss:0.65263
loss:0.65155
loss:0.64947
loss:0.67722
loss:0.66810
loss:0.67784
loss:0.66355
loss:0.65088
loss:0.67294
loss:0.66359
loss:0.65592
loss:0.64740
loss:0.65900
loss:0.66801
loss:0.66478
loss:0.66325
loss:0.64421
loss:0.66419
loss:0.66538
loss:0.65093
loss:0.66910
loss:0.66653
loss:0.65585
loss:0.67717
loss:0.66676
loss:0.67466
loss:0.66281
loss:0.66022
loss:0.66515
loss:0.64478
loss:0.66248
loss:0.66420
loss:0.64721
loss:0.64544
loss:0.64294
loss:0.67438
loss:0.64344
loss:0.66920
loss:0.64721
loss:0.67333
loss:0.68078
loss:0.65069
loss:0.66208
loss:0.66570
loss:0.66996
loss:0.65375
loss:0.67160
loss:0.66290
loss:0.66335
loss:0.66960
loss:0.65692
loss:0.65373
loss:0.66914
loss:0.66688
loss:0.66217
loss:0.67208
loss:0.65926
loss:0.64535
loss:0.65159
loss:0.67918
loss:0.66129
loss:0.66989
loss:0.66912
loss:0.66926
loss:0.67055
loss:0.68019
loss:0.64452
loss:0.66107
loss:0.66071
loss:0.67368
loss:0.64376
loss:0.65572
loss:0.66844
loss:0.67691
loss:0.65637
loss:0.65951
loss:0.65778
loss:0.66580
loss:0.65701
loss:0.67251
loss:0.64313
loss:0.66283
loss:0.65680
loss:0.65269
loss:0.66523
loss:0.65287
loss:0.64688
loss:0.64759
loss:0.65433
loss:0.66972
loss:0.66807
loss:0.66828
loss:0.67227
loss:0.66661
loss:0.66397
loss:0.65870
loss:0.65171
loss:0.65700
loss:0.65989
loss:0.65342
loss:0.67234
loss:0.65679
loss:0.65613
loss:0.68543
loss:0.67071
loss:0.66354
loss:0.65643
loss:0.65872
loss:0.67237
loss:0.64807
loss:0.65317
loss:0.65778
loss:0.67151
loss:0.65623
loss:0.66789
loss:0.66217
loss:0.66082
loss:0.65427
loss:0.67618
loss:0.65622
loss:0.65882
loss:0.66887
loss:0.65625
loss:0.65383
loss:0.66703
loss:0.65852
loss:0.66507
loss:0.67109
loss:0.64156
loss:0.66068
loss:0.65953
loss:0.65764
loss:0.66824
loss:0.64952
loss:0.64580
loss:0.65779
loss:0.66862
loss:0.64858
loss:0.66827
loss:0.67129
loss:0.66533
loss:0.67883
loss:0.66803
loss:0.66452
loss:0.66109
loss:0.66889
loss:0.66898
loss:0.66461
loss:0.67247
loss:0.65506
loss:0.66828
loss:0.65667
loss:0.66204
loss:0.65946
loss:0.66878
loss:0.65982
loss:0.67103
loss:0.65585
loss:0.66739
loss:0.66134
loss:0.66400
loss:0.67536
loss:0.65783
loss:0.64254
loss:0.65690
loss:0.65508
loss:0.67195
loss:0.66933
loss:0.67662
loss:0.66981
loss:0.66420
loss:0.65592
loss:0.66521
loss:0.65597
loss:0.66998
loss:0.63783
loss:0.66236
loss:0.65183
loss:0.65696
loss:0.66256
loss:0.65964
loss:0.65708
loss:0.66081
loss:0.66114
loss:0.67140
loss:0.65908
loss:0.65911
loss:0.66029
loss:0.66635
loss:0.65295
loss:0.66048
loss:0.66606
loss:0.66033
loss:0.68154
loss:0.66941
loss:0.67794
loss:0.66282
loss:0.66002
loss:0.65971
loss:0.65576
loss:0.66025
loss:0.64754
loss:0.64394
loss:0.66024
loss:0.66186
loss:0.66747
loss:0.67224
loss:0.64846
loss:0.66642
loss:0.65927
loss:0.64219
loss:0.64800
loss:0.64456
loss:0.66997
loss:0.66928
loss:0.65322
loss:0.67488
loss:0.65589
loss:0.64999
loss:0.67784
loss:0.66377
loss:0.68162
loss:0.66122
loss:0.66184
loss:0.65566
loss:0.65692
loss:0.67533
loss:0.65812
loss:0.67073
loss:0.65696
loss:0.64987
loss:0.65343
loss:0.66365
loss:0.65863
loss:0.66812
loss:0.67260
loss:0.65074
loss:0.65577
loss:0.66562
loss:0.66141
loss:0.66660
loss:0.66166
loss:0.67496
loss:0.65931
loss:0.66856
loss:0.63983
loss:0.64665
loss:0.67327
loss:0.66942
loss:0.64789
loss:0.67481
loss:0.65036
loss:0.64888
loss:0.67199
loss:0.65079
loss:0.67321
loss:0.65907
loss:0.66910
loss:0.66463
loss:0.66243
loss:0.66445
loss:0.66674
loss:0.67992
loss:0.65620
loss:0.64680
loss:0.65858
loss:0.66298
loss:0.67091
loss:0.64818
loss:0.66034
loss:0.66185
loss:0.66922
loss:0.65696
loss:0.66156
loss:0.63671
loss:0.66764
loss:0.65451
loss:0.65081
loss:0.67778
loss:0.65931
loss:0.66358
loss:0.63004
loss:0.66210
loss:0.64330
loss:0.67492
loss:0.66807
loss:0.66871
loss:0.66186
loss:0.65610
loss:0.66433
loss:0.67069
loss:0.65969
loss:0.64786
loss:0.66205
loss:0.66802
loss:0.68323
loss:0.64927
loss:0.65208
loss:0.65708
loss:0.66650
loss:0.64133
loss:0.67628
loss:0.66095
loss:0.66150
loss:0.65469
loss:0.66234
loss:0.64541
loss:0.66892
loss:0.66233
loss:0.65859
loss:0.66970
loss:0.67037
loss:0.66045
loss:0.65528
loss:0.65001
loss:0.65424
loss:0.66481
loss:0.67079
loss:0.65976
loss:0.66692
loss:0.66513
loss:0.67337
loss:0.67096
loss:0.65297
loss:0.66148
loss:0.67237
loss:0.66708
loss:0.65966
loss:0.65198
loss:0.67115
loss:0.65654
loss:0.66809
loss:0.65938
loss:0.65984
loss:0.65624
loss:0.67163
loss:0.65636
loss:0.66307
loss:0.67177
loss:0.65938
loss:0.66297
loss:0.64704
loss:0.65659
loss:0.67510
loss:0.65530
loss:0.66488
loss:0.65818
loss:0.66271
loss:0.65564
loss:0.66018
loss:0.66652
loss:0.64212
loss:0.67963
loss:0.66241
loss:0.65903
loss:0.66616
loss:0.66183
loss:0.66274
loss:0.63881
loss:0.65109
loss:0.66721
loss:0.65393
loss:0.64744
loss:0.64289
loss:0.65169
loss:0.64275
loss:0.66342
loss:0.65978
loss:0.65983
loss:0.66083
loss:0.65782
loss:0.64515
loss:0.66360
loss:0.67336
loss:0.67128
loss:0.64996
loss:0.65313
loss:0.65719
loss:0.67875
loss:0.68120
loss:0.65495
loss:0.66444
loss:0.65034
loss:0.65029
loss:0.66396
loss:0.66171
loss:0.68489
loss:0.65036
loss:0.65359
loss:0.65201
loss:0.67415
loss:0.65801
loss:0.66260
loss:0.65122
loss:0.65446
loss:0.65942
loss:0.66713
loss:0.67007
loss:0.63884
loss:0.67196
loss:0.67176
loss:0.65923
loss:0.65634
loss:0.66189
loss:0.65936
loss:0.65546
loss:0.64966
loss:0.66250
loss:0.65862
loss:0.64219
loss:0.67288
loss:0.66770
loss:0.66608
loss:0.65869
loss:0.66552
loss:0.65709
loss:0.67439
loss:0.65063
loss:0.67093
loss:0.64820
loss:0.67235
loss:0.65882
loss:0.66712
loss:0.64949
loss:0.65410
loss:0.66149
loss:0.66211
loss:0.65612
loss:0.67151
loss:0.65497
loss:0.63613
loss:0.66190
loss:0.67301
loss:0.67877
loss:0.67054
loss:0.68218
loss:0.67609
loss:0.65255
loss:0.66030
loss:0.67279
loss:0.64619
loss:0.66703
loss:0.64962
loss:0.66730
loss:0.66863
loss:0.65427
loss:0.66093
loss:0.65944
loss:0.65059
loss:0.66606
loss:0.65082
loss:0.65487
loss:0.68125
loss:0.64960
loss:0.66650
loss:0.65488
loss:0.64488
loss:0.66408
loss:0.66576
loss:0.65594
loss:0.66029
loss:0.66272
loss:0.66087
loss:0.66110
loss:0.66575
loss:0.65857
loss:0.65664
Evaluation at Epoch 4/4. Step:10740/10740. AccuracyMetric: acc=0.551457


In Epoch:3/Step:8055, got best dev performance:AccuracyMetric: acc=0.555186
Reloaded the best model.
Train Done.
[tester] 
AccuracyMetric: acc=0.555186
Test Done.
Predict the answer with best model...
(44772, 2)
Predict Done. 5730816 records
true sample count:6085
Add 0 default result in predict.
Predict Done, results saved to ../../data/results/CNNText_essay_one_non_teacher_referred_donor_giving_100_plus
